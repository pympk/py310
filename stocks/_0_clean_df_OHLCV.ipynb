{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve pickled dataframe with symbols' OHLCV\n",
    "source_filename = \"df_OHLCV_2025-03-03.pkl\"\n",
    "destination_filename = \"df_OHLCV_2025-03-03_clean.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_dir: c:\\Users\\ping\\Files_win10\\python\\py310\\stocks\\temp\n",
      "source_path: c:\\Users\\ping\\Files_win10\\python\\py310\\stocks\\temp\\df_OHLCV_2025-03-03.pkl\n",
      "dest_path: c:\\Users\\ping\\Files_win10\\python\\py310\\stocks\\temp\\df_OHLCV_2025-03-03_clean.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create temp directory if it doesn't exist\n",
    "temp_dir = os.path.join(os.getcwd(), 'temp')\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Get source and destination paths\n",
    "source_path = os.path.join(temp_dir, source_filename)\n",
    "dest_path = os.path.join(temp_dir, destination_filename)\n",
    "\n",
    "print(f'temp_dir: {temp_dir}')\n",
    "print(f'source_path: {source_path}')\n",
    "print(f'dest_path: {dest_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AAPL</th>\n",
       "      <th>2025-03-03</th>\n",
       "      <td>241.72</td>\n",
       "      <td>244.03</td>\n",
       "      <td>236.11</td>\n",
       "      <td>238.03</td>\n",
       "      <td>238.03</td>\n",
       "      <td>46788610</td>\n",
       "      <td>241.720000</td>\n",
       "      <td>244.030000</td>\n",
       "      <td>236.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-28</th>\n",
       "      <td>236.95</td>\n",
       "      <td>242.09</td>\n",
       "      <td>230.20</td>\n",
       "      <td>241.84</td>\n",
       "      <td>241.84</td>\n",
       "      <td>56796200</td>\n",
       "      <td>236.950000</td>\n",
       "      <td>242.090000</td>\n",
       "      <td>230.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-27</th>\n",
       "      <td>239.41</td>\n",
       "      <td>242.46</td>\n",
       "      <td>237.06</td>\n",
       "      <td>237.30</td>\n",
       "      <td>237.30</td>\n",
       "      <td>41153600</td>\n",
       "      <td>239.410000</td>\n",
       "      <td>242.460000</td>\n",
       "      <td>237.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26</th>\n",
       "      <td>244.33</td>\n",
       "      <td>244.98</td>\n",
       "      <td>239.13</td>\n",
       "      <td>240.36</td>\n",
       "      <td>240.36</td>\n",
       "      <td>44433600</td>\n",
       "      <td>244.330000</td>\n",
       "      <td>244.980000</td>\n",
       "      <td>239.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-25</th>\n",
       "      <td>248.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>244.91</td>\n",
       "      <td>247.04</td>\n",
       "      <td>247.04</td>\n",
       "      <td>48013300</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>244.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">IBTE</th>\n",
       "      <th>2024-03-08</th>\n",
       "      <td>23.89</td>\n",
       "      <td>23.89</td>\n",
       "      <td>23.88</td>\n",
       "      <td>23.89</td>\n",
       "      <td>22.99</td>\n",
       "      <td>446288</td>\n",
       "      <td>22.990000</td>\n",
       "      <td>22.990000</td>\n",
       "      <td>22.980377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-07</th>\n",
       "      <td>23.89</td>\n",
       "      <td>23.89</td>\n",
       "      <td>23.87</td>\n",
       "      <td>23.88</td>\n",
       "      <td>22.98</td>\n",
       "      <td>348642</td>\n",
       "      <td>22.989623</td>\n",
       "      <td>22.989623</td>\n",
       "      <td>22.970377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-06</th>\n",
       "      <td>23.86</td>\n",
       "      <td>23.87</td>\n",
       "      <td>23.86</td>\n",
       "      <td>23.87</td>\n",
       "      <td>22.97</td>\n",
       "      <td>399535</td>\n",
       "      <td>22.960377</td>\n",
       "      <td>22.970000</td>\n",
       "      <td>22.960377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-05</th>\n",
       "      <td>23.87</td>\n",
       "      <td>23.87</td>\n",
       "      <td>23.86</td>\n",
       "      <td>23.87</td>\n",
       "      <td>22.97</td>\n",
       "      <td>572369</td>\n",
       "      <td>22.970000</td>\n",
       "      <td>22.970000</td>\n",
       "      <td>22.960377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-04</th>\n",
       "      <td>23.86</td>\n",
       "      <td>23.87</td>\n",
       "      <td>23.86</td>\n",
       "      <td>23.87</td>\n",
       "      <td>22.97</td>\n",
       "      <td>316415</td>\n",
       "      <td>22.960377</td>\n",
       "      <td>22.970000</td>\n",
       "      <td>22.960377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630265 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Open    High     Low   Close  Adj Close    Volume  \\\n",
       "Symbol Date                                                              \n",
       "AAPL   2025-03-03  241.72  244.03  236.11  238.03     238.03  46788610   \n",
       "       2025-02-28  236.95  242.09  230.20  241.84     241.84  56796200   \n",
       "       2025-02-27  239.41  242.46  237.06  237.30     237.30  41153600   \n",
       "       2025-02-26  244.33  244.98  239.13  240.36     240.36  44433600   \n",
       "       2025-02-25  248.00  250.00  244.91  247.04     247.04  48013300   \n",
       "...                   ...     ...     ...     ...        ...       ...   \n",
       "IBTE   2024-03-08   23.89   23.89   23.88   23.89      22.99    446288   \n",
       "       2024-03-07   23.89   23.89   23.87   23.88      22.98    348642   \n",
       "       2024-03-06   23.86   23.87   23.86   23.87      22.97    399535   \n",
       "       2024-03-05   23.87   23.87   23.86   23.87      22.97    572369   \n",
       "       2024-03-04   23.86   23.87   23.86   23.87      22.97    316415   \n",
       "\n",
       "                     Adj Open    Adj High     Adj Low  \n",
       "Symbol Date                                            \n",
       "AAPL   2025-03-03  241.720000  244.030000  236.110000  \n",
       "       2025-02-28  236.950000  242.090000  230.200000  \n",
       "       2025-02-27  239.410000  242.460000  237.060000  \n",
       "       2025-02-26  244.330000  244.980000  239.130000  \n",
       "       2025-02-25  248.000000  250.000000  244.910000  \n",
       "...                       ...         ...         ...  \n",
       "IBTE   2024-03-08   22.990000   22.990000   22.980377  \n",
       "       2024-03-07   22.989623   22.989623   22.970377  \n",
       "       2024-03-06   22.960377   22.970000   22.960377  \n",
       "       2024-03-05   22.970000   22.970000   22.960377  \n",
       "       2024-03-04   22.960377   22.970000   22.960377  \n",
       "\n",
       "[630265 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df = pd.read_pickle(source_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 630265 entries, ('AAPL', Timestamp('2025-03-03 00:00:00')) to ('IBTE', Timestamp('2024-03-04 00:00:00'))\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   Open       630265 non-null  float64\n",
      " 1   High       630265 non-null  float64\n",
      " 2   Low        630265 non-null  float64\n",
      " 3   Close      630265 non-null  float64\n",
      " 4   Adj Close  630265 non-null  float64\n",
      " 5   Volume     630064 non-null  Int64  \n",
      " 6   Adj Open   630265 non-null  float64\n",
      " 7   Adj High   630265 non-null  float64\n",
      " 8   Adj Low    630265 non-null  float64\n",
      "dtypes: Int64(1), float64(8)\n",
      "memory usage: 46.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'filter_df_dates_to_reference_symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_df_dates_to_reference_symbol\u001b[49m(df)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'filter_df_dates_to_reference_symbol'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "filtered_df = utils.filter_df_dates_to_reference_symbol(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def filter_df_dates_to_reference_symbol(df, reference_symbol=\"AAPL\"):\n",
    "#     \"\"\"\n",
    "#     Filters symbols in a DataFrame based on date index matching a reference symbol (default AAPL)\n",
    "#     and provides analysis of the filtering results.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): DataFrame with a MultiIndex ('Symbol', 'Date').\n",
    "#         reference_symbol (str): The symbol to use as the reference for date comparison. Defaults to \"AAPL\".\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The filtered DataFrame.  Prints analysis to standard output.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Get the date index for the reference symbol.  Return empty DataFrame if symbol not found\n",
    "#     try:\n",
    "#         reference_dates = df.loc[reference_symbol].index\n",
    "#     except KeyError:\n",
    "#         print(f\"Error: Reference symbol '{reference_symbol}' not found in DataFrame.\")\n",
    "#         return pd.DataFrame()  # Return an empty DataFrame if reference_symbol is not found\n",
    "    \n",
    "#     original_symbols = df.index.get_level_values('Symbol').unique().tolist()\n",
    "\n",
    "#     # Filter symbols based on date index matching with the reference symbol\n",
    "#     filtered_symbols = []\n",
    "#     for symbol in original_symbols:\n",
    "#         try: # Handle the case where a symbol might be missing from the df\n",
    "#             symbol_dates = df.loc[symbol].index\n",
    "#         except KeyError:\n",
    "#             continue # Skip to the next symbol if this one is missing\n",
    "\n",
    "#         if (len(symbol_dates) == len(reference_dates) and symbol_dates.equals(reference_dates)):\n",
    "#             filtered_symbols.append(symbol)\n",
    "\n",
    "#     # Create the filtered DataFrame\n",
    "#     df_filtered = df.loc[filtered_symbols]\n",
    "\n",
    "\n",
    "#     # Analyze the filtering results\n",
    "#     print(f\"Original number of symbols: {len(original_symbols)}\")\n",
    "#     print(f\"Number of symbols after filtering: {len(filtered_symbols)}\")\n",
    "#     print(f\"Number of symbols filtered out: {len(original_symbols) - len(filtered_symbols)}\")\n",
    "\n",
    "#     filtered_out_symbols = list(set(original_symbols) - set(filtered_symbols))\n",
    "\n",
    "#     print(\"\\nFirst 10 symbols that were filtered out:\")\n",
    "#     print(filtered_out_symbols[:10])\n",
    "\n",
    "#     if filtered_out_symbols:\n",
    "#         print(\"\\nExample of dates for first filtered out symbol:\")\n",
    "#         first_filtered_symbol = filtered_out_symbols[0]\n",
    "#         try:  # Handle potential KeyError if the symbol doesn't exist (e.g., due to filtering earlier)\n",
    "#             print(f\"\\nDates for {first_filtered_symbol}:\")\n",
    "#             print(df.loc[first_filtered_symbol].index)\n",
    "#         except KeyError:\n",
    "#             print(f\"\\nSymbol '{first_filtered_symbol}' not found in the original DataFrame.\")\n",
    "\n",
    "\n",
    "#     print(\"\\nFiltered DataFrame info:\")\n",
    "#     print(df_filtered.info())\n",
    "\n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = df.index.get_level_values('Symbol').unique().tolist()\n",
    "print(f'Number of unique symbols: {len(symbols)}')\n",
    "print('\\nFirst 10 symbols:')\n",
    "print(symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter symbols based on date index matching with AAPL\n",
    "filtered_symbols = []\n",
    "\n",
    "for symbol in symbols:\n",
    "  symbol_dates = df.loc[symbol].index\n",
    "  \n",
    "  # Check if dates match exactly with AAPL's dates\n",
    "  if (len(symbol_dates) == len(aapl_dates) and \n",
    "    symbol_dates.equals(aapl_dates)):\n",
    "    filtered_symbols.append(symbol)\n",
    "\n",
    "# Create new filtered dataframe\n",
    "df_filtered = df.loc[filtered_symbols]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Original symbols count: {len(symbols)}\")\n",
    "print(f\"Filtered symbols count: {len(filtered_symbols)}\")\n",
    "print(f\"Removed symbols count: {len(symbols) - len(filtered_symbols)}\")\n",
    "\n",
    "# Verify the filtered dataframe\n",
    "print(\"\\nFiltered DataFrame info:\")\n",
    "print(df_filtered.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of original and filtered symbols\n",
    "original_symbols = df.index.get_level_values('Symbol').unique().tolist()\n",
    "filtered_symbols = filtered_df.index.get_level_values('Symbol').unique().tolist()\n",
    "\n",
    "# Find symbols that were filtered out\n",
    "filtered_out_symbols = list(set(original_symbols) - set(filtered_symbols))\n",
    "\n",
    "# Print summary\n",
    "print(f\"Original number of symbols: {len(original_symbols)}\")\n",
    "print(f\"Number of symbols after filtering: {len(filtered_symbols)}\")\n",
    "print(f\"Number of symbols filtered out: {len(filtered_out_symbols)}\")\n",
    "\n",
    "# Print first 10 filtered out symbols as sample\n",
    "print(\"\\nFirst 10 symbols that were filtered out:\")\n",
    "print(filtered_out_symbols[:10])\n",
    "\n",
    "# Optional: Save filtered out symbols to list for further analysis\n",
    "if filtered_out_symbols:\n",
    "  print(\"\\nExample of dates for first filtered out symbol:\")\n",
    "  first_filtered_symbol = filtered_out_symbols[0]\n",
    "  print(f\"\\nDates for {first_filtered_symbol}:\")\n",
    "  print(df.loc[first_filtered_symbol].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AAPL's date index as reference\n",
    "aapl_dates = df.loc['AAPL'].index\n",
    "\n",
    "# Create a function to check if a symbol has matching dates with AAPL\n",
    "def has_matching_dates(symbol_df, reference_dates):\n",
    "  return symbol_df.index.equals(reference_dates)\n",
    "\n",
    "# Filter symbols\n",
    "matching_symbols = []\n",
    "non_matching_symbols = []\n",
    "\n",
    "for symbol in symbols:\n",
    "  if has_matching_dates(df.loc[symbol], aapl_dates):\n",
    "    matching_symbols.append(symbol)\n",
    "  else:\n",
    "    non_matching_symbols.append(symbol)\n",
    "\n",
    "# Create a new DataFrame with only the symbols that have matching dates\n",
    "df_filtered = df.loc[matching_symbols]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total symbols: {len(symbols)}\")\n",
    "print(f\"Symbols with matching dates: {len(matching_symbols)}\")\n",
    "print(f\"Symbols with non-matching dates: {len(non_matching_symbols)}\")\n",
    "print(f\"\\nFirst few non-matching symbols: {non_matching_symbols[:5] if non_matching_symbols else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc['AAPL'].index)\n",
    "print(df.loc['GEV'].index)\n",
    "print(df.loc['EWTX'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter out symbols with Date index not matching the Date index of symbol \"AAPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Modified analyze_stock function to print debug info about data lengths\n",
    "def analyze_stock_debug(df, ticker, risk_free_rate=0.0):\n",
    "    try:\n",
    "        # Extract Adj Close prices for the specified ticker\n",
    "        adj_close_prices = df.loc[ticker]['Adj Close']\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns_series = utils.calculate_returns(adj_close_prices)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"\\n--- {ticker} ---\")\n",
    "        print(f\"Number of Adj Close days: {len(adj_close_prices)}\")\n",
    "        print(f\"Number of returns: {len(returns_series) if returns_series is not None else 0}\")\n",
    "        \n",
    "        if returns_series is not None:\n",
    "            n = len(returns_series) + 1  # Replicate the calculation from metrics function\n",
    "            print(f\"Calculated 'n' value: {n}d\")\n",
    "            \n",
    "            # Show first and last dates\n",
    "            print(f\"Date range: {adj_close_prices.index[0].date()} to {adj_close_prices.index[-1].date()}\")\n",
    "            \n",
    "            # Check sample of 5 dates\n",
    "            print(\"Sample dates:\", [d.date() for d in adj_close_prices.index[:5]])\n",
    "            \n",
    "        return None  # We're just debugging, not calculating actual metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with sample tickers from your data\n",
    "# sample_tickers = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOG']  # Replace with actual tickers from your data\n",
    "for ticker in symbols:\n",
    "    analyze_stock_debug(df, ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_symbols_with_missing_values(df):\n",
    "    \"\"\"\n",
    "    Filters out symbols from a MultiIndex DataFrame that have:\n",
    "    1. Any missing values in any columns\n",
    "    2. Missing any dates present in the original DataFrame's date index\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame with a MultiIndex, where the first\n",
    "                           level is the symbol and the second level is the date.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - filtered_df (pd.DataFrame): A DataFrame containing only the symbols\n",
    "              without missing values or dates. Returns empty if none are clean.\n",
    "            - symbols_with_missing (list): List of symbols with missing data/dates.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    # Get all unique dates from the original DataFrame\n",
    "    expected_dates = df.index.levels[1].unique()\n",
    "    \n",
    "    symbols_with_missing = []\n",
    "    filtered_data = []\n",
    "    kept_symbols = []\n",
    "\n",
    "    for symbol in df.index.get_level_values(0).unique():\n",
    "        symbol_df = df.loc[symbol]\n",
    "        \n",
    "        # Check for missing values\n",
    "        has_nan = symbol_df.isnull().any().any()\n",
    "        \n",
    "        # Check for missing dates\n",
    "        missing_dates = expected_dates.difference(symbol_df.index)\n",
    "        \n",
    "        if has_nan or len(missing_dates) > 0:\n",
    "            symbols_with_missing.append(symbol)\n",
    "        else:\n",
    "            filtered_data.append(symbol_df)\n",
    "            kept_symbols.append(symbol)\n",
    "\n",
    "    if filtered_data:\n",
    "        filtered_df = pd.concat(\n",
    "            filtered_data, \n",
    "            keys=kept_symbols, \n",
    "            names=['Symbol', 'Date']\n",
    "        )\n",
    "    else:\n",
    "        filtered_df = pd.DataFrame()\n",
    "\n",
    "    return filtered_df, symbols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your DataFrame 'df' loaded\n",
    "filtered_df, symbols_with_missing = filter_symbols_with_missing_values(df)\n",
    "\n",
    "print(\"Filtered DataFrame (without symbols with missing values):\")\n",
    "print(filtered_df.info())\n",
    "\n",
    "print(\"\\nSymbols with missing values:\")\n",
    "print(f\"Count: {len(symbols_with_missing)}\")\n",
    "print(f\"Symbols: {symbols_with_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_df.index.get_level_values(1).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the date index (second level of the MultiIndex)\n",
    "date_index_name = filtered_df.index.names[1]\n",
    "print(f\"Date index name: {date_index_name}\")\n",
    "\n",
    "\n",
    "print(\"\\nFirst few dates in the index:\")\n",
    "print(filtered_df.index.get_level_values(1).unique()[:5])\n",
    "print(f'\\nlen(filtered_df.index.get_level_values(1).unique()): {len(filtered_df.index.get_level_values(1).unique())}')\n",
    "\n",
    "print(\"\\nFirst few dates in the index:\")\n",
    "print(filtered_df.index.get_level_values(1)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tickers = filtered_df.index.get_level_values(0).unique().tolist()\n",
    "print(f'filtered_tickers, len {len(filtered_tickers)}: {filtered_tickers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WHY? EWTX has 500 rows\n",
    "#### Yahoo gave more data?, Filter Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc['EWTX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "# filtered_df.loc['EWTX'].to_csv('EWTX_data.csv')\n",
    "filtered_df.loc[\"AAPL\"].to_csv(\"AAPL_data.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all stocks in the DataFrame and concatenate the results\n",
    "all_results = []\n",
    "# for ticker in df.columns.levels[0]: #get unique tickers\n",
    "for ticker in filtered_tickers: \n",
    "    result_df = utils.analyze_stock(filtered_df, ticker, risk_free_rate=risk_free_rate)\n",
    "    if result_df is not None: # Correctly check for None\n",
    "        all_results.append(result_df)\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results)\n",
    "    print(\"\\nCombined performance metrics DataFrame:\")\n",
    "    print(combined_df)\n",
    "else:\n",
    "    print(\"No performance metrics were calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Analyze a single stock\n",
    "# ticker = 'NVDA'\n",
    "ticker = 'RSSL'  # Example ticker\n",
    "\n",
    "# risk_free_rate = 0.01  # Example risk-free rate\n",
    "\n",
    "result_df = utils.analyze_stock(filtered_df, ticker, risk_free_rate=risk_free_rate)\n",
    "if result_df is not None:  # Correctly check for None\n",
    "    print(f\"\\nPerformance metrics DataFrame for {ticker}:\")\n",
    "    print(result_df)\n",
    "else:\n",
    "    print(f\"\\nCould not calculate performance metrics for {ticker}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set the display option globally for your entire script\n",
    "pd.set_option('display.max_rows', 300)\n",
    "filtered_df.loc['RSSL'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index.levels[1].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc['IEV'].index\n",
    "df.loc['RSSL'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_sorted_intersection(list1, list2):\n",
    "#     \"\"\"\n",
    "#     Finds the intersection of two lists and returns a new sorted list containing\n",
    "#     the common elements.\n",
    "\n",
    "#     Args:\n",
    "#         list1 (list): The first list.\n",
    "#         list2 (list): The second list.\n",
    "\n",
    "#     Returns:\n",
    "#         list: A new sorted list containing the common elements of list1 and list2.\n",
    "#               Returns an empty list if there are no common elements or if either\n",
    "#               input list is None or empty.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if not list1 or not list2:  # Check for empty or None lists\n",
    "#         return []\n",
    "\n",
    "#     # Convert lists to sets for efficient intersection finding\n",
    "#     set1 = set(list1)\n",
    "#     set2 = set(list2)\n",
    "\n",
    "#     # Find the intersection using set intersection\n",
    "#     intersection_set = set1.intersection(set2)\n",
    "\n",
    "#     # Convert the intersection set to a list and sort it\n",
    "#     intersection_list = sorted(list(intersection_set))\n",
    "\n",
    "#     return intersection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_df = df.index.get_level_values(0).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tickers[:10])\n",
    "print(symbols_with_missing[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'VG' in tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tickers = [ticker for ticker in tickers if ticker not in symbols_with_missing]\n",
    "# filtered_tickers = [ticker for ticker in tickers if symbols_with_missing not in ticker]\n",
    "print(f'len(filtered_tickers): {len(filtered_tickers)}')\n",
    "print(f'len(tickers): {len(tickers)}')\n",
    "print(f'len(symbols_with_missing): {len(symbols_with_missing)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all stocks in the DataFrame and concatenate the results\n",
    "all_results = []\n",
    "# for ticker in df.columns.levels[0]: #get unique tickers\n",
    "for ticker in tickers: \n",
    "    result_df = utils.analyze_stock(df, ticker, risk_free_rate=risk_free_rate)\n",
    "    if result_df is not None: # Correctly check for None\n",
    "        all_results.append(result_df)\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results)\n",
    "    print(\"\\nCombined performance metrics DataFrame:\")\n",
    "    print(combined_df)\n",
    "else:\n",
    "    print(\"No performance metrics were calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc['NVDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_performance_ratios = 'performance_ratios.pkl'\n",
    "combined_df.to_pickle(fn_performance_ratios)\n",
    "print(f\"DataFrame pickled successfully to {fn_performance_ratios}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent 60 rows for each symbol\n",
    "recent_df = utils.get_recent_rows(df, num_rows=30)\n",
    "# recent_df = get_recent_rows(df)\n",
    "print(recent_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recent_df.loc['NVDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_pickle(fn_performance_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.loc['NVDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def analyze_stock(df, ticker, risk_free_rate=0.0, output_debug_data=False):\n",
    "#     \"\"\"\n",
    "#     Analyzes a single stock's performance based on its adjusted close prices.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Pandas DataFrame containing stock data, including 'Adj Close' column.\n",
    "#         ticker (str): The stock ticker symbol (e.g., 'NVDA').\n",
    "#         risk_free_rate (float): The annualized risk-free rate. Default is 0.0.\n",
    "#         output_debug_data (bool): If True, print Adj Close prices and returns (default: False).\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: A DataFrame with the ticker as index and 'Sharpe Ratio', 'Sortino Ratio', and 'Omega Ratio' as columns.\n",
    "#                        Returns None if there is an error.  Crucially changed to return None, not an empty dataframe.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Extract Adj Close prices for the specified ticker\n",
    "#         adj_close_prices = df.loc[ticker]['Adj Close']\n",
    "\n",
    "#         # Check if adj_close_prices is a Series\n",
    "#         if not isinstance(adj_close_prices, pd.Series):\n",
    "#              raise TypeError(f\"Expected a Pandas Series for Adj Close prices of {ticker}. Check that {ticker} exists in the DataFrame, and that 'Adj Close' is a valid column\")\n",
    "\n",
    "#         # Calculate returns\n",
    "#         returns_series = calculate_returns(adj_close_prices)\n",
    "\n",
    "#         if returns_series is not None:\n",
    "#             # Output debug data if requested\n",
    "#             if output_debug_data:\n",
    "#                 print(f\"--- Debug Data for {ticker} ---\")\n",
    "#                 print(\"\\nAdj Close Prices (Dates and Values):\")\n",
    "#                 print(adj_close_prices)  #This is a Series, prints the index(dates) and values.\n",
    "#                 print(\"\\nReturns:\")\n",
    "#                 print(returns_series)\n",
    "\n",
    "#             # Calculate performance metrics\n",
    "#             performance_metrics = calculate_performance_metrics(returns_series, risk_free_rate=risk_free_rate)\n",
    "\n",
    "#             if performance_metrics:\n",
    "#                 # Create a DataFrame from the metrics\n",
    "#                 metrics_df = pd.DataFrame(performance_metrics, index=[ticker])\n",
    "#                 return metrics_df\n",
    "#             else:\n",
    "#                 print(f\"Could not calculate performance metrics for {ticker}.\")\n",
    "#                 return None  # Return None, not an empty DataFrame\n",
    "#         else:\n",
    "#             print(f\"Failed to calculate returns for {ticker}.\")\n",
    "#             return None # Return None, not an empty DataFrame\n",
    "\n",
    "#     except KeyError:\n",
    "#         print(f\"Ticker '{ticker}' not found in DataFrame.\")\n",
    "#         return None # Return None, not an empty DataFrame\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during analysis: {e}\")\n",
    "#         return None # Return None, not an empty DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def calculate_performance_metrics(returns, risk_free_rate=0.0):\n",
    "#     \"\"\"\n",
    "#     Calculates Sortino Ratio, Sharpe Ratio, and Omega Ratio using PyFolio/Empyrical.\n",
    "\n",
    "#     Args:\n",
    "#         returns (pd.Series or np.array):  Daily returns of the investment.\n",
    "#                                          Must be a Pandas Series with a DatetimeIndex.\n",
    "#         risk_free_rate (float):  The risk-free rate (annualized). Default is 0.0.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the calculated ratios.\n",
    "#               Returns None if there is an error or the input is invalid.\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         # Ensure returns is a pandas Series with a DatetimeIndex.  Crucial for pyfolio.\n",
    "#         if not isinstance(returns, pd.Series):\n",
    "#             returns = pd.Series(returns)  # Convert to Series if needed\n",
    "#         if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "#             raise ValueError(\"Returns must be a Pandas Series with a DatetimeIndex.\")\n",
    "\n",
    "#         # Convert annualized risk-free rate to daily rate\n",
    "#         days_per_year = 252  # Standard for financial calculations\n",
    "#         daily_risk_free_rate = risk_free_rate / days_per_year\n",
    "\n",
    "#         # Calculate the Sharpe Ratio using empyrical (as pyfolio's is deprecated)\n",
    "#         sharpe_ratio = empyrical.sharpe_ratio(returns, risk_free=daily_risk_free_rate, annualization=days_per_year)\n",
    "\n",
    "#         # Calculate the Sortino Ratio using empyrical\n",
    "#         sortino_ratio = empyrical.sortino_ratio(returns, required_return=daily_risk_free_rate, annualization=days_per_year)\n",
    "\n",
    "#         # Calculate the Omega Ratio using empyrical\n",
    "#         omega_ratio = empyrical.omega_ratio(returns, risk_free=daily_risk_free_rate, annualization=days_per_year)\n",
    "\n",
    "\n",
    "#         return {\n",
    "#             \"Sharpe Ratio\": sharpe_ratio,\n",
    "#             \"Sortino Ratio\": sortino_ratio,\n",
    "#             \"Omega Ratio\": omega_ratio\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check on calculation for: Sharpe Ratio, Sortino Ratio, Omega Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_ratios(daily_close, risk_free_rate=0.02, debug=False):\n",
    "    \"\"\"\n",
    "    Calculate risk-adjusted performance metrics from daily price data.\n",
    "    \n",
    "    Parameters:\n",
    "        daily_close (pd.Series): Series of daily closing prices with DatetimeIndex\n",
    "        risk_free_rate (float): Annualized risk-free rate (default: 0.02)\n",
    "        debug (bool): Whether to print intermediate calculations (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing Sharpe, Sortino, and Omega ratios\n",
    "    \"\"\"\n",
    "    # Validate and sort input\n",
    "    if not isinstance(daily_close, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "        \n",
    "    # Sort by date (oldest first) to ensure proper return calculations\n",
    "    sorted_series = daily_close.sort_index(ascending=True)\n",
    "    \n",
    "    # Calculate daily returns from closing prices\n",
    "    returns = sorted_series.pct_change().dropna()\n",
    "    if returns.empty:\n",
    "        raise ValueError(\"Insufficient data to calculate returns\")\n",
    "    \n",
    "    # Common parameters\n",
    "    days_per_year = 252\n",
    "    daily_risk_free = risk_free_rate / days_per_year\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    debug_info = {\n",
    "        'sorted_dates': sorted_series.index,\n",
    "        'returns': returns\n",
    "    }\n",
    "\n",
    "    # Calculate Sharpe Ratio\n",
    "    avg_daily_return = returns.mean()\n",
    "    daily_std = returns.std()\n",
    "    sharpe_excess = avg_daily_return - daily_risk_free\n",
    "    results['Sharpe Ratio'] = (sharpe_excess / daily_std) * np.sqrt(days_per_year)\n",
    "    \n",
    "    # Store debug information\n",
    "    debug_info.update({\n",
    "        'avg_daily_return': avg_daily_return,\n",
    "        'daily_std': daily_std,\n",
    "        'sharpe_excess': sharpe_excess\n",
    "    })\n",
    "\n",
    "    # Calculate Sortino Ratio\n",
    "    excess_returns = returns - daily_risk_free\n",
    "    downside_returns = excess_returns[excess_returns < 0]\n",
    "    downside_std = np.sqrt(np.sum(downside_returns**2) / len(excess_returns))\n",
    "    results['Sortino Ratio'] = (excess_returns.mean() / downside_std) * np.sqrt(days_per_year)\n",
    "    \n",
    "    # Store debug information\n",
    "    debug_info.update({\n",
    "        'excess_returns': excess_returns,\n",
    "        'downside_returns': downside_returns,\n",
    "        'downside_std': downside_std\n",
    "    })\n",
    "\n",
    "    # Calculate Omega Ratio\n",
    "    threshold = daily_risk_free\n",
    "    returns_above = returns[returns > threshold]\n",
    "    returns_below = returns[returns <= threshold]\n",
    "    \n",
    "    sum_above = (returns_above - threshold).sum()\n",
    "    sum_below = (threshold - returns_below).sum()\n",
    "    \n",
    "    try:\n",
    "        results['Omega Ratio'] = sum_above / sum_below\n",
    "    except ZeroDivisionError:\n",
    "        results['Omega Ratio'] = np.nan\n",
    "    \n",
    "    # Store debug information\n",
    "    debug_info.update({\n",
    "        'threshold': threshold,\n",
    "        'sum_above': sum_above,\n",
    "        'sum_below': sum_below\n",
    "    })\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n=== Debug Information ===\")\n",
    "        print(f\"First 5 sorted dates: {debug_info['sorted_dates'][:5].strftime('%Y-%m-%d').tolist()}\")\n",
    "        print(f\"Last 5 sorted dates: {debug_info['sorted_dates'][-5:].strftime('%Y-%m-%d').tolist()}\")\n",
    "        print(f\"Number of trading days: {len(returns)}\")\n",
    "        print(f\"\\nSharpe Ratio Components:\")\n",
    "        print(f\"Average Daily Return: {debug_info['avg_daily_return']:.6f}\")\n",
    "        print(f\"Daily Std Dev: {debug_info['daily_std']:.6f}\")\n",
    "        print(f\"Daily Risk-Free Rate: {daily_risk_free:.6f}\")\n",
    "        print(f\"Sharpe Excess Return: {debug_info['sharpe_excess']:.6f}\")\n",
    "        \n",
    "        print(f\"\\nSortino Ratio Components:\")\n",
    "        print(f\"Excess Returns Mean: {excess_returns.mean():.6f}\")\n",
    "        print(f\"Downside Returns Count: {len(debug_info['downside_returns'])}\")\n",
    "        print(f\"Downside Std Dev: {debug_info['downside_std']:.6f}\")\n",
    "        \n",
    "        print(f\"\\nOmega Ratio Components:\")\n",
    "        print(f\"Threshold: {debug_info['threshold']:.6f}\")\n",
    "        print(f\"Returns Above Threshold: {len(returns_above)}\")\n",
    "        print(f\"Returns Below Threshold: {len(returns_below)}\")\n",
    "        print(f\"Sum Above Threshold: {debug_info['sum_above']:.6f}\")\n",
    "        print(f\"Sum Below Threshold: {debug_info['sum_below']:.6f}\")\n",
    "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Check calculation on NVDA ratios\n",
    "adj_close_series = df.loc['NVDA']['Adj Close']\n",
    "adj_close_series\n",
    "\n",
    "# Assuming that calculate_ratios returns a dictionary with keys like 'Sharpe Ratio', 'Sortino Ratio', etc.\n",
    "ratios = calculate_ratios(adj_close_series, risk_free_rate, debug=False)\n",
    "\n",
    "# Print the entire dictionary (for debugging)\n",
    "print(f'Santity check for NVDA ratios:\\n{ratios}\\n')  # Print the whole dictionary for inspection\n",
    "\n",
    "# Properly access and format the desired ratio (e.g., Sharpe Ratio):\n",
    "if 'Sharpe Ratio' in ratios:  # Check if the key exists to avoid KeyError\n",
    "    print(f'NVDA Sharpe Ratio: {ratios[\"Sharpe Ratio\"]:.6f}')\n",
    "else:\n",
    "    print(\"Sharpe Ratio not found in ratios dictionary.\")\n",
    "\n",
    "# Properly access and format the desired ratio (e.g., Sortino Ratio):\n",
    "if 'Sortino Ratio' in ratios:  # Check if the key exists to avoid KeyError\n",
    "    print(f'NVDA Sortino Ratio: {ratios[\"Sortino Ratio\"]:.6f}')\n",
    "else:\n",
    "    print(\"Sortino Ratio not found in ratios dictionary.\")\n",
    "\n",
    "# Properly access and format the desired ratio (e.g., Omega Ratio):\n",
    "if 'Omega Ratio' in ratios:  # Check if the key exists to avoid KeyError\n",
    "    print(f'NVDA Omega Ratio: {ratios[\"Omega Ratio\"]:.6f}')\n",
    "else:\n",
    "    print(\"Omega Ratio not found in ratios dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def save_returns_to_csv(returns, filename=\"returns.csv\"):\n",
    "#     \"\"\"\n",
    "#     Saves a Pandas Series of returns to a CSV file in the user's Downloads directory.\n",
    "\n",
    "#     Args:\n",
    "#         returns (pd.Series): The Pandas Series containing the returns data.  The index is assumed to be dates.\n",
    "#         filename (str, optional): The name of the CSV file to create. Defaults to \"returns.csv\".\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Get the user's Downloads directory\n",
    "#     downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "#     # Construct the full file path\n",
    "#     file_path = os.path.join(downloads_path, filename)\n",
    "\n",
    "#     try:\n",
    "#         # Write the Series to a CSV file\n",
    "#         returns.to_csv(file_path, header=True)  # Include header for column name\n",
    "\n",
    "#         print(f\"Returns data saved to: {file_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving returns to CSV: {e}\")\n",
    "\n",
    "\n",
    "# # Example Usage (replace with your actual returns Series)\n",
    "# # Create sample data\n",
    "# import numpy as np\n",
    "# dates = pd.to_datetime(pd.date_range('2023-01-01', periods=252))\n",
    "# returns = pd.Series(np.random.normal(0.0005, 0.01, 252), index=dates, name=\"Daily Return\") # important to give series a name!\n",
    "\n",
    "# # Save the data\n",
    "# save_returns_to_csv(returns, filename=\"my_returns_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
