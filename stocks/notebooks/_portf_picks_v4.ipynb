{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### String Representation of a List of Dictionaries\n",
    "\n",
    "# you are a highly skilled and experienced quantitative trader working for Goldman Sachs. you are also an expert python coder experience writing codes for quantitative trading. Your primary responsibility is to \n",
    "# write python codes to identify and construct portfolios that will outperform the market based on rigorous data analysis and sophisticated financial modeling. \n",
    "\n",
    "# Write python code to select 50 Tickers that is expected to outperform the market in the next 5, 10, 15, 30, 60 and 120 days. Use the correlation and or covariance matrices to reduce the 50 selected tickers to 10 tickers portfolio that is expected to outperform the market in the next 5, 10, 15, 30, 60 and 120 days.\n",
    "\n",
    "# You are given three pandas dataframes: df_data, df_corr, and df_cov.\n",
    "# df_data index:\n",
    "# ---\n",
    "# Ticker\n",
    "# ---  \n",
    "# df_data.info():\n",
    "# ---\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 1379 entries, UBS to PCVX\n",
    "# Data columns (total 49 columns):\n",
    "#  #   Column           Non-Null Count  Dtype  \n",
    "# ---  ------           --------------  -----  \n",
    "#  0   Dividend %       1118 non-null   float64\n",
    "#  1   Perf Week %      1379 non-null   float64\n",
    "#  2   Perf Month %     1379 non-null   float64\n",
    "#  3   Perf Quart %     1379 non-null   float64\n",
    "#  4   Perf Half %      1379 non-null   float64\n",
    "#  5   Perf Year %      1379 non-null   float64\n",
    "#  6   Perf YTD %       1379 non-null   float64\n",
    "#  7   Beta             1378 non-null   object \n",
    "#  8   ATR              1379 non-null   object \n",
    "#  9   Volatility W %   1379 non-null   float64\n",
    "#  10  Volatility M %   1379 non-null   float64\n",
    "#  11  SMA20 %          1379 non-null   float64\n",
    "#  12  SMA50 %          1379 non-null   float64\n",
    "#  13  SMA200 %         1379 non-null   float64\n",
    "#  14  50D High %       1379 non-null   float64\n",
    "#  15  50D Low %        1379 non-null   float64\n",
    "#  16  52W High %       1379 non-null   float64\n",
    "#  17  52W Low %        1379 non-null   float64\n",
    "#  18  All-Time High %  1378 non-null   float64\n",
    "#  19  All-Time Low %   1377 non-null   float64\n",
    "#  20  RSI              1379 non-null   object \n",
    "#  21  Gap %            1379 non-null   float64\n",
    "#  22  Rel Volume       1379 non-null   object \n",
    "#  23  Price            1379 non-null   object \n",
    "#  24  Change %         1379 non-null   float64\n",
    "#  25  MktCap AUM, M    980 non-null    float64\n",
    "#  26  Avg Volume, M    1379 non-null   float64\n",
    "#  27  Volume, M        1379 non-null   float64\n",
    "#  28  Sharpe 5d        1379 non-null   float64\n",
    "#  29  Sortino 5d       1379 non-null   float64\n",
    "#  30  Omega 5d         1329 non-null   float64\n",
    "#  31  Sharpe 10d       1379 non-null   float64\n",
    "#  32  Sortino 10d      1379 non-null   float64\n",
    "#  33  Omega 10d        1379 non-null   float64\n",
    "#  34  Sharpe 15d       1379 non-null   float64\n",
    "#  35  Sortino 15d      1379 non-null   float64\n",
    "#  36  Omega 15d        1379 non-null   float64\n",
    "#  37  Sharpe 30d       1379 non-null   float64\n",
    "#  38  Sortino 30d      1379 non-null   float64\n",
    "#  39  Omega 30d        1379 non-null   float64\n",
    "#  40  Sharpe 60d       1379 non-null   float64\n",
    "#  41  Sortino 60d      1379 non-null   float64\n",
    "#  42  Omega 60d        1379 non-null   float64\n",
    "#  43  Sharpe 120d      1379 non-null   float64\n",
    "#  44  Sortino 120d     1379 non-null   float64\n",
    "#  45  Omega 120d       1379 non-null   float64\n",
    "#  46  Sharpe 250d      1379 non-null   float64\n",
    "#  47  Sortino 250d     1379 non-null   float64\n",
    "#  48  Omega 250d       1379 non-null   float64\n",
    "# dtypes: float64(44), object(5)\n",
    "# memory usage: 538.7+ KB\n",
    "# ---\n",
    "# df_data column descripton:\n",
    "# ---  \n",
    "# Ticker: The stock symbol or identifier for the company.\n",
    "\n",
    "# Dividend %: The annual dividend yield expressed as a percentage of the current stock price.\n",
    "\n",
    "# Perf Week %: The percentage price change over the past week.\n",
    "\n",
    "# Perf Month %: The percentage price change over the past month.\n",
    "\n",
    "# Perf Quart %: The percentage price change over the past quarter (3 months).\n",
    "\n",
    "# Perf Half %: The percentage price change over the past half-year (6 months).\n",
    "\n",
    "# Perf Year %: The percentage price change over the past year (52 weeks).\n",
    "\n",
    "# Perf YTD %: The percentage price change from the beginning of the current calendar year to date.\n",
    "\n",
    "# Beta: A measure of a stock's volatility relative to the overall market (usually S&P 500). A beta of 1 indicates the stock moves with the market. Greater than 1 is more volatile, less than 1 is less volatile.\n",
    "\n",
    "# ATR (Average True Range): A technical analysis indicator measuring the average price range of a stock over a specific period (often 14 days). It represents the stock's volatility.\n",
    "\n",
    "# Volatility W %: The volatility (standard deviation of price changes) calculated over the past week.\n",
    "\n",
    "# Volatility M %: The volatility calculated over the past month.\n",
    "\n",
    "# SMA20 %: The percentage difference between the current price and the 20-day Simple Moving Average.\n",
    "\n",
    "# SMA50 %: The percentage difference between the current price and the 50-day Simple Moving Average.\n",
    "\n",
    "# SMA200 %: The percentage difference between the current price and the 200-day Simple Moving Average.\n",
    "\n",
    "# 50D High %: The percentage difference between the current price and the 50-day high price.\n",
    "\n",
    "# 50D Low %: The percentage difference between the current price and the 50-day low price.\n",
    "\n",
    "# 52W High %: The percentage difference between the current price and the 52-week high price.\n",
    "\n",
    "# 52W Low %: The percentage difference between the current price and the 52-week low price.\n",
    "\n",
    "# All-Time High %: The percentage difference between the current price and the all-time high price.\n",
    "\n",
    "# All-Time Low %: The percentage difference between the current price and the all-time low price.\n",
    "\n",
    "# RSI (Relative Strength Index): A momentum oscillator that measures the speed and change of price movements. Ranges from 0-100, commonly used to identify overbought (above 70) and oversold (below 30) conditions.\n",
    "\n",
    "# Gap %: The percentage difference between the previous day's closing price and the current day's opening price.\n",
    "\n",
    "# Rel Volume (Relative Volume): The ratio of the current trading volume to the average trading volume over a specified period (often the past 30 days). Values above 1 indicate higher than average volume.\n",
    "\n",
    "# Price: The current trading price of the stock.\n",
    "\n",
    "# Change %: The percentage change in the stock price from the previous day's close.\n",
    "\n",
    "# MktCap AUM, M (Market Capitalization or Assets Under Management, Millions): The total market value of a company's outstanding shares (market cap) or the total value of assets managed by a fund (AUM), expressed in millions of dollars.\n",
    "\n",
    "# Avg Volume, M (Average Volume, Millions): The average number of shares traded per day, expressed in millions.\n",
    "\n",
    "# Volume, M (Volume, Millions): The number of shares traded on the current day, expressed in millions.\n",
    "\n",
    "# Sharpe (5d, 10d, 15d, 30d, 60d, 120d, 250d): The Sharpe ratio measures risk-adjusted return. It represents the excess return earned per unit of risk (volatility). Higher is better. Calculated over different timeframes (5 days, 10 days, etc.).\n",
    "\n",
    "# Sortino (5d, 10d, 15d, 30d, 60d, 120d, 250d): Similar to the Sharpe ratio, but only penalizes downside risk (negative volatility). Higher is better. Calculated over different timeframes.\n",
    "\n",
    "# Omega (5d, 10d, 15d, 30d, 60d, 120d, 250d): A risk-return performance measure that uses the entire return distribution. Values above 1 indicate more upside potential than downside risk. Calculated over different timeframes.\n",
    "# ---\n",
    "# df_corr.info():\n",
    "# ---\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 1379 entries, UBS to PCVX\n",
    "# Columns: 1379 entries, UBS to PCVX\n",
    "# dtypes: float64(1379)\n",
    "# memory usage: 14.5+ MB\n",
    "# ---\n",
    "# df_cov.info():\n",
    "# ---\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 1379 entries, UBS to PCVX\n",
    "# Columns: 1379 entries, UBS to PCVX\n",
    "# dtypes: float64(1379)\n",
    "# memory usage: 14.5+ MB\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "# pd.set_option('display.width', None)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '..\\data\\df_finviz_n_ratios.pkl'\n",
    "path_corr = '..\\data\\df_corr_emv_matrix.pkl'\n",
    "path_cov = '..\\data\\df_cov_emv_matrix.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_column_values_above_threshold(df, column_name='Avg Volume, M', threshold=0.75):\n",
    "  \"\"\"\n",
    "  Analyzes the number and percentage of values in a DataFrame column that are above a specified threshold,\n",
    "  and returns the filtered DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to analyze. Defaults to 'Avg Volume, M'.\n",
    "    threshold (float): The threshold value to compare against. Defaults to 1.00.\n",
    "\n",
    "  Returns:\n",
    "    pd.DataFrame: A DataFrame containing only the rows where the specified column's value is above the threshold.\n",
    "  \"\"\"\n",
    "  \n",
    "  count_before = len(df)\n",
    "  above_threshold_df = df[df[column_name] > threshold]\n",
    "  count_after = len(above_threshold_df)\n",
    "  percentage = (count_after / len(df)) * 100\n",
    "\n",
    "  print(f\"count_before: {count_before}\")\n",
    "  print(f\"count_after above threshold ({threshold}): {count_after}\")\n",
    "  print(f\"Percentage above threshold ({threshold}): {percentage:.2f}%\")\n",
    "\n",
    "  return above_threshold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_pickle(path_data)\n",
    "\n",
    "# liquidity filter\n",
    "df_data = get_column_values_above_threshold(df_data, column_name='Avg Volume, M', threshold=0.75)\n",
    "\n",
    "df_corr = pd.read_pickle(path_corr)\n",
    "df_cov = pd.read_pickle(path_cov)\n",
    "\n",
    "print(f'\\ndf_cov.shape: {df_cov.shape}')\n",
    "display(df_cov.head())\n",
    "\n",
    "print(f'\\ndf_corr.shape: {df_corr.shape}')\n",
    "display(df_corr.head())\n",
    "\n",
    "print(f'\\ndf_data.shape: {df_data.shape}')\n",
    "display(df_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data.info()\n",
    "np.sqrt(df_cov.loc['VRSN']['VRSN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60, 120]\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.25,\n",
    "        'sortino': 0.25,\n",
    "        'omega': 0.20,\n",
    "        'momentum': 0.15,\n",
    "        'sma': 0.10,\n",
    "        'volatility': -0.05\n",
    "    }\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "    momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "                    'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols + \n",
    "                    momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    # Composite score calculation\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "            \n",
    "            # This calculation is on the column. \n",
    "            # Calculate z-scores for each row (i.e. ticker) in the column, the all tickers in the the column\n",
    "            # i.e. calculates the z-score 'Sharpe 5D' column\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "            # This calculation is on each row (i.e. ticker).\n",
    "            # For each row, average(z-scores for each category) * (feature weight of category)\n",
    "            # i.e. average(z-score for 'Sharpe 5D', 'Sharpe 10D'..)  * (feature weight of 'Sharpe')\n",
    "            components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "            \n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "    \n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "    \n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "        \n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "        \n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "        \n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.4f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.4f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.4f})\")\n",
    "        \n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.4f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.4f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.4f}/Vol: {np.sqrt(top_risk_adj['variance']):.4f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.4f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.4f}/Vol: {np.sqrt(top_raw_score['variance']):.4f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.4f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "        \n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "    \n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "    \n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "    \n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "final_portfolio = portfolio_optimizer(\n",
    "    df_data, \n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Above Works, Leave it alone\n",
    "#### Mess with the code in cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    # time_horizons = [5, 10, 15, 30, 60, 120]\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    # feature_weights = {\n",
    "    #     'sharpe': 0.25,\n",
    "    #     'sortino': 0.25,\n",
    "    #     'omega': 0.20,\n",
    "    #     'momentum': 0.15,\n",
    "    #     'sma': 0.10,\n",
    "    #     'volatility': -0.05\n",
    "    # }\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    # momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "    #                 'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "    \n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols + \n",
    "                    momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    # Composite score calculation\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    # def calculate_weighted_score(df):\n",
    "    #     components = {}\n",
    "    #     for category, cols in [('sharpe', sharpe_cols),\n",
    "    #                           ('sortino', sortino_cols),\n",
    "    #                           ('omega', omega_cols),\n",
    "    #                           ('momentum', momentum_cols),\n",
    "    #                           ('sma', sma_cols),\n",
    "    #                           ('volatility', volatility_cols)]:\n",
    "            \n",
    "    #         # # This calculation is on the column. \n",
    "    #         # # Calculate z-scores for each row (i.e. ticker) in the column, the all tickers in the the column\n",
    "    #         # # i.e. calculates the z-score 'Sharpe 5D' column\n",
    "    #         # z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "    #         # # This calculation is on each row (i.e. ticker).\n",
    "    #         # # For each row, average(z-scores for each category) * (feature weight of category)\n",
    "    #         # # i.e. average(z-score for 'Sharpe 5D', 'Sharpe 10D'..)  * (feature weight of 'Sharpe')\n",
    "    #         # components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "\n",
    "    #         # Replace z_scores.mean(axis=1) with weighted average\n",
    "    #         z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "    #         weighted_z = z_scores @ np.array(horizon_weights)  # Matrix multiplication\n",
    "    #         components[category] = weighted_z * feature_weights[category]\n",
    "           \n",
    "    #     return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    # def calculate_weighted_score(df):\n",
    "    #     components = {}\n",
    "        \n",
    "    #     # Time horizon weights (for features with multiple time periods)\n",
    "    #     horizon_weights = {\n",
    "    #         'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "    #         'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "    #         'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "    #     }\n",
    "        \n",
    "    #     # # Equal weights for non-time series features\n",
    "    #     # equal_weights = {\n",
    "    #     #     'momentum': [1/6]*6,\n",
    "    #     #     'sma': [1/3]*3,\n",
    "    #     #     'volatility': [0.6, 0.4]  # Weekly volatility weighted higher\n",
    "    #     # }\n",
    "    #     equal_weights = {\n",
    "    #         'momentum': [0.3, 0.3, 0.25, 0.15],  # Matches 4 momentum columns\n",
    "    #         'sma': [0.4, 0.4, 0.2],             # 3 SMA columns\n",
    "    #         'volatility': [0.6, 0.4]             # 2 volatility columns\n",
    "    #     }\n",
    "\n",
    "    #     for category, cols in [('sharpe', sharpe_cols),\n",
    "    #                           ('sortino', sortino_cols),\n",
    "    #                           ('omega', omega_cols),\n",
    "    #                           ('momentum', momentum_cols),\n",
    "    #                           ('sma', sma_cols),\n",
    "    #                           ('volatility', volatility_cols)]:\n",
    "            \n",
    "    #         z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            \n",
    "    #         if category in horizon_weights:\n",
    "    #             weights = np.array(horizon_weights[category])\n",
    "    #             weighted_z = z_scores @ weights\n",
    "    #         else:\n",
    "    #             weights = np.array(equal_weights[category])\n",
    "    #             weighted_z = z_scores @ weights\n",
    "                \n",
    "    #         components[category] = weighted_z * feature_weights[category]\n",
    "            \n",
    "    #     return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "        \n",
    "        # Time horizon weights (for features with multiple time periods)\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],  # 6 weights\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],  # 6 weights\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]     # 6 weights\n",
    "        }\n",
    "        \n",
    "        # Equal weights for non-time series features\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "            \n",
    "            # Validate columns exist\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "            \n",
    "            # Calculate z-scores\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            \n",
    "            # Debug: Identify the problematic category\n",
    "            print(f\"\\nüîç Processing {category}:\")\n",
    "            print(f\"   - Columns: {cols}\")\n",
    "            print(f\"   - Shape: {z_scores.shape}\")\n",
    "            \n",
    "            # Validate column/weights alignment\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "                assert len(cols) == len(weights), \\\n",
    "                    f\"{category} has {len(cols)} cols but {len(weights)} weights!\"\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "                assert len(cols) == len(weights), \\\n",
    "                    f\"{category} has {len(cols)} cols but {len(weights)} weights!\"\n",
    "            \n",
    "            # Calculate weighted score\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "            \n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "        # Top 50 Selection\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "        print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "        print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "        print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "        top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "        top_50_tickers = top_50.index.tolist()\n",
    "        \n",
    "        print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "        top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "        for i in range(0, 50, 5):\n",
    "            row = []\n",
    "            for j in range(5):\n",
    "                try:\n",
    "                    idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                    row.append(f\"{idx:5} {score:4.2f}\")\n",
    "                except IndexError:\n",
    "                    break\n",
    "            print(\" | \".join(row))\n",
    "\n",
    "        # Stage 2: Portfolio Optimization\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "        print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "        print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "        print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "        print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "        # Cluster analysis\n",
    "        corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "        distance_matrix = 1 - np.abs(corr_subset)\n",
    "        np.fill_diagonal(distance_matrix.values, 0)\n",
    "        linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "        clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "        # Cluster dataframe construction\n",
    "        cluster_df = pd.DataFrame({\n",
    "            'ticker': top_50_tickers,\n",
    "            'cluster': clusters,\n",
    "            'score': top_50['composite_score']\n",
    "        }).merge(\n",
    "            df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "            left_on='ticker',\n",
    "            right_index=True\n",
    "        )\n",
    "\n",
    "        # Risk calculations\n",
    "        epsilon = 1e-6\n",
    "        cluster_df = cluster_df.assign(\n",
    "            variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "            risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "        )\n",
    "\n",
    "        # Cluster statistics\n",
    "        print(\"\\nüìà Cluster Statistics:\")\n",
    "        cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "            Size=('ticker', 'count'),\n",
    "            Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "            Avg_Score=('score', 'mean'),\n",
    "        ).reset_index().round(2)\n",
    "        print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "        # # Modified final selection section with explanations\n",
    "        # print(\"\\n\" + \"=\"*80)\n",
    "        # print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "        # print(\"=\"*80)\n",
    "        \n",
    "        # final_portfolio = []\n",
    "        # for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        #     cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        #     selected = cluster_members.nlargest(1, 'risk_adj_score')\n",
    "        #     final_portfolio.append(selected['ticker'].values[0])\n",
    "            \n",
    "        #     # Cluster-specific explanation\n",
    "        #     print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        #     print(f\"   üèÜ Selected: {selected['ticker'].values[0]} \"\n",
    "        #           f\"(Score: {selected['score'].values[0]:.2f}, \"\n",
    "        #           f\"Volatility: {np.sqrt(selected['variance'].values[0]):.2f})\")\n",
    "            \n",
    "        #     # Special case explanation for Cluster 1\n",
    "        #     if cluster_id == 1 and 'OKTA' in cluster_members.ticker.values:\n",
    "        #         okta = cluster_members[cluster_members.ticker == 'OKTA'].iloc[0]\n",
    "        #         puk = cluster_members[cluster_members.ticker == 'PUK'].iloc[0]\n",
    "                \n",
    "        #         print(\"\\n   üîç Selection Rationale:\")\n",
    "        #         print(f\"   - OKTA Score: {okta.score:.2f} vs PUK Score: {puk.score:.2f}\")\n",
    "        #         print(f\"   - OKTA Volatility: {np.sqrt(okta.variance):.2f} vs PUK Volatility: {np.sqrt(puk.variance):.2f}\")\n",
    "        #         print(f\"   - Risk-Adjusted Scores:\")\n",
    "        #         print(f\"     OKTA: {okta.score/np.sqrt(okta.variance):.2f}\")\n",
    "        #         print(f\"     PUK:  {puk.score/np.sqrt(puk.variance):.2f}\")\n",
    "        #         print(\"   - Despite lower raw score, PUK provides better risk efficiency\")\n",
    "        #         print(\"   - Cluster correlation 0.67 indicates high similarity\")\n",
    "        #         print(\"   - Lower volatility allows larger position sizing\")\n",
    "            \n",
    "        #     print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Modified final selection section with generalized explanations\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        final_portfolio = []\n",
    "        for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "            cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "            cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "            \n",
    "            # Get top candidates by different metrics\n",
    "            top_risk_adj = cluster_members.iloc[0]\n",
    "            top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "            \n",
    "            selected_ticker = top_risk_adj['ticker']\n",
    "            final_portfolio.append(selected_ticker)\n",
    "            \n",
    "            # Cluster header\n",
    "            print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "            print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "                  f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "                  f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "                  f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            \n",
    "            # Add explanation when top raw score isn't selected\n",
    "            if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "                print(\"\\n   üîç Selection Rationale:\")\n",
    "                print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "                print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "                print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                      f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "                print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                      f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "                print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "                # print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance']):.2f}\")\n",
    "                print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "                print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "            \n",
    "            print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # New explanatory section\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"1. Composite Score Components:\")\n",
    "        print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "        print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "        print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "        print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "        print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "        print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "        \n",
    "        print(\"\\n2. Cluster Selection Criteria:\")\n",
    "        print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "        print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "        print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "        print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "        \n",
    "        print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "        print(\"   - Balances raw performance vs risk efficiency\")\n",
    "        print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "        print(\"   - Prevents overexposure to single risk factors\")\n",
    "        print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "        \n",
    "        print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "        print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "        print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "        print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "        print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "        print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "        print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "        print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "        return final_portfolio\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "# final_portfolio = portfolio_optimizer(\n",
    "#     df_data, \n",
    "#     df_corr,\n",
    "#     df_covar,\n",
    "# )\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data, \n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_score(df):\n",
    "    components = {}\n",
    "    \n",
    "    horizon_weights = {\n",
    "        'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "        'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "        'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "    }\n",
    "    \n",
    "    equal_weights = {\n",
    "        'momentum': [0.3, 0.3, 0.25, 0.15],  # 4 weights for 4 columns\n",
    "        'sma': [0.4, 0.4, 0.2],             # 3 weights for 3 columns\n",
    "        'volatility': [0.6, 0.4]             # 2 weights for 2 columns\n",
    "    }\n",
    "\n",
    "    for category, cols in [('sharpe', sharpe_cols),\n",
    "                          ('sortino', sortino_cols),\n",
    "                          ('omega', omega_cols),\n",
    "                          ('momentum', momentum_cols),\n",
    "                          ('sma', sma_cols),\n",
    "                          ('volatility', volatility_cols)]:\n",
    "        \n",
    "        # Debug: Print category and columns\n",
    "        print(f\"Processing category: {category}, columns: {cols}\")\n",
    "        print(f\"Number of columns: {len(cols)}\")\n",
    "        \n",
    "        z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "        \n",
    "        if category in horizon_weights:\n",
    "            weights = np.array(horizon_weights[category])\n",
    "            # Ensure columns match weights\n",
    "            assert len(cols) == len(weights), f\"Mismatch in {category}: {len(cols)} cols vs {len(weights)} weights\"\n",
    "        else:\n",
    "            weights = np.array(equal_weights[category])\n",
    "            # Ensure columns match weights\n",
    "            assert len(cols) == len(weights), f\"Mismatch in {category}: {len(cols)} cols vs {len(weights)} weights\"\n",
    "        \n",
    "        weighted_z = z_scores @ weights\n",
    "        components[category] = weighted_z * feature_weights[category]\n",
    "        \n",
    "    return pd.concat(components, axis=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    # time_horizons = [5, 10, 15, 30, 60, 120]\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    # feature_weights = {\n",
    "    #     'sharpe': 0.25,\n",
    "    #     'sortino': 0.25,\n",
    "    #     'omega': 0.20,\n",
    "    #     'momentum': 0.15,\n",
    "    #     'sma': 0.10,\n",
    "    #     'volatility': -0.05\n",
    "    # }\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    # momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "    #                 'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "    \n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols + \n",
    "                    momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "        \n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "        \n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "            \n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "            \n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            \n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            if len(cols) != len(weights):\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of columns: {len(cols)}\")\n",
    "                print(f\"Number of weights: {len(weights)}\")\n",
    "                raise ValueError(f\"{category} columns vs weights mismatch\")\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "            \n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "    \n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "        \n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "        \n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "        \n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "        \n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "        \n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "    \n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "    \n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "    \n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data, \n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            print(f\"Category: {category}\")  # Added print statements\n",
    "            print(f\"cols length: {len(cols)}\")\n",
    "            print(f\"weights length: {len(weights)}\")\n",
    "            print(f\"z_scores shape: {z_scores.shape}\")\n",
    "            print(f\"weights shape: {weights.shape}\")\n",
    "\n",
    "            if len(cols) != len(weights):\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of columns: {len(cols)}\")\n",
    "                print(f\"Number of weights: {len(weights)}\")\n",
    "                raise ValueError(f\"{category} columns vs weights mismatch\")\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 equal weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 equal weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "            \n",
    "            if len(cols) != len(weights):\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of columns: {len(cols)}\")\n",
    "                print(f\"Number of weights: {len(weights)}\")\n",
    "                raise ValueError(f\"{category} columns vs weights mismatch\")\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 equal weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 equal weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            # Remove rows with NaN in current columns BEFORE calculating z-scores\n",
    "            df_subset = df[cols].dropna()\n",
    "\n",
    "            # Calculate z-scores only if there are valid values\n",
    "            if not df_subset.empty:\n",
    "                z_scores = df_subset.apply(lambda x: (x - x.mean()) / x.std())\n",
    "            else:\n",
    "                # Handle the case where all values are NaN (or become NaN after dropping)\n",
    "                # Assign a zero-filled DataFrame with the correct number of columns\n",
    "                z_scores = pd.DataFrame(np.zeros((0, len(cols))), columns=cols) # IMPORTANT fix\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            if len(cols) != len(weights):\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of columns: {len(cols)}\")\n",
    "                print(f\"Number of weights: {len(weights)}\")\n",
    "                raise ValueError(f\"{category} columns vs weights mismatch\")\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 equal weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 equal weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            # Remove rows with NaN in current columns BEFORE calculating z-scores\n",
    "            df_subset = df[cols].copy()  # Create a copy to avoid modifying the original\n",
    "            \n",
    "            print(f\"Before NaN handling - Category: {category}, df_subset shape: {df_subset.shape}\") # Debugging\n",
    "\n",
    "            df_subset = df_subset.dropna()\n",
    "\n",
    "            print(f\"After NaN handling - Category: {category}, df_subset shape: {df_subset.shape}\")  # Debugging\n",
    "            \n",
    "            # Calculate z-scores only if there are valid values\n",
    "            if not df_subset.empty:\n",
    "                z_scores = df_subset.apply(lambda x: (x - x.mean()) / x.std())\n",
    "            else:\n",
    "                # Handle the case where all values are NaN (or become NaN after dropping)\n",
    "                # Assign a zero-filled DataFrame with the correct number of columns\n",
    "                z_scores = pd.DataFrame(np.zeros((0, len(cols))), columns=cols) # IMPORTANT fix\n",
    "            \n",
    "\n",
    "            print(f\"Category: {category}\")  # Added print statements\n",
    "            print(f\"cols length: {len(cols)}\")\n",
    "            print(f\"weights length: {len(weights)}\")\n",
    "            print(f\"z_scores shape: {z_scores.shape}\")\n",
    "            print(f\"weights shape: {weights.shape}\")\n",
    "\n",
    "            if len(cols) != z_scores.shape[1]:  # Corrected check here\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of expected columns: {len(cols)}\")\n",
    "                print(f\"Number of actual columns: {z_scores.shape[1]}\")\n",
    "                raise ValueError(f\"{category} columns vs z_scores columns mismatch\")\n",
    "\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 equal weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 equal weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            # Remove rows with NaN in current columns BEFORE calculating z-scores\n",
    "            df_subset = df[cols].copy()  # Create a copy to avoid modifying the original\n",
    "            \n",
    "            # Add this debugging to check for NaN values in momentum columns\n",
    "            if category == 'momentum':\n",
    "                print(\"Momentum columns before NaN handling:\\n\", df_subset.isnull().sum())\n",
    "\n",
    "            df_subset = df_subset.dropna()\n",
    "            \n",
    "            if category == 'momentum':\n",
    "                print(\"Momentum columns after NaN handling:\\n\", df_subset.isnull().sum())\n",
    "\n",
    "            # Calculate z-scores only if there are valid values\n",
    "            if not df_subset.empty:\n",
    "                z_scores = df_subset.apply(lambda x: (x - x.mean()) / x.std())\n",
    "            else:\n",
    "                # Handle the case where all values are NaN (or become NaN after dropping)\n",
    "                # Assign a zero-filled DataFrame with the correct number of columns\n",
    "                z_scores = pd.DataFrame(np.zeros((0, len(cols))), columns=cols) # IMPORTANT fix\n",
    "           \n",
    "            print(f\"Category: {category}\")  # Added print statements\n",
    "            print(f\"cols length: {len(cols)}\")\n",
    "            print(f\"weights length: {len(weights)}\")\n",
    "            print(f\"z_scores shape: {z_scores.shape}\")\n",
    "            print(f\"weights shape: {weights.shape}\")\n",
    "\n",
    "            if len(cols) != z_scores.shape[1]:  # Corrected check here\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of expected columns: {len(cols)}\")\n",
    "                print(f\"Number of actual columns: {z_scores.shape[1]}\")\n",
    "                raise ValueError(f\"{category} columns vs z_scores columns mismatch\")\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def portfolio_optimizer_(df_data, df_corr, df_cov):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [3, 5, 10, 15, 30, 60]  # Removed 120d, added 3d\n",
    "    horizon_weights = [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]  # Favors shorter horizons\n",
    "\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.20,    # Reduced from 0.25\n",
    "        'sortino': 0.20,   # Reduced from 0.25\n",
    "        'omega': 0.15,     # Reduced from 0.20\n",
    "        'momentum': 0.25,  # Increased from 0.15\n",
    "        'sma': 0.15,       # Increased from 0.10\n",
    "        'volatility': -0.10 # Increased penalty\n",
    "    }  # Adjusted weights to favor momentum and SMA\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "\n",
    "    momentum_cols = ['Perf 3D %', 'Perf Week %', 'Perf Month %', 'Perf Quart %']\n",
    "\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols +\n",
    "                     momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "\n",
    "        horizon_weights = {\n",
    "            'sharpe': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'sortino': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10],\n",
    "            'omega': [0.25, 0.20, 0.18, 0.15, 0.12, 0.10]\n",
    "        }\n",
    "\n",
    "        equal_weights = {\n",
    "            'momentum': [0.25, 0.25, 0.25, 0.25],  # 4 equal weights for 4 columns\n",
    "            'sma': [0.4, 0.4, 0.2],                # 3 equal weights for 3 columns\n",
    "            'volatility': [0.6, 0.4]                # 2 equal weights for 2 columns\n",
    "        }\n",
    "\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "\n",
    "            # Verify column existence\n",
    "            missing_cols = [col for col in cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns for {category}: {missing_cols}\")\n",
    "\n",
    "            # Remove rows with NaN in current columns BEFORE calculating z-scores\n",
    "            df_subset = df[cols].copy()  # Create a copy to avoid modifying the original\n",
    "            \n",
    "            # Add this debugging to check for NaN values in momentum columns\n",
    "            if category == 'momentum':\n",
    "                print(\"Momentum columns before NaN handling:\\n\", df_subset.isnull().sum())\n",
    "\n",
    "            df_subset = df_subset.dropna()\n",
    "            \n",
    "            if category == 'momentum':\n",
    "                print(\"Momentum columns after NaN handling:\\n\", df_subset.isnull().sum())\n",
    "\n",
    "            # Calculate z-scores only if there are valid values\n",
    "            if not df_subset.empty:\n",
    "                z_scores = df_subset.apply(lambda x: (x - x.mean()) / x.std())\n",
    "            else:\n",
    "                # Handle the case where all values are NaN (or become NaN after dropping)\n",
    "                # Assign a zero-filled DataFrame with the correct number of columns\n",
    "                z_scores = pd.DataFrame(np.zeros((0, len(cols))), columns=cols) # IMPORTANT fix\n",
    "\n",
    "            if category in horizon_weights:\n",
    "                weights = np.array(horizon_weights[category])\n",
    "            else:\n",
    "                weights = np.array(equal_weights[category])\n",
    "\n",
    "            # Debugging prints\n",
    "            print(f\"Category: {category}\")\n",
    "            print(f\"cols length: {len(cols)}\")\n",
    "            print(f\"z_scores shape: {z_scores.shape}\")\n",
    "            print(f\"weights shape: {weights.shape}\")\n",
    "            print(f\"weights length: {len(weights)}\")\n",
    "\n",
    "            if len(cols) != z_scores.shape[1]:  # Corrected check here\n",
    "                print(f\"Error in category: {category}\")\n",
    "                print(f\"Number of expected columns: {len(cols)}\")\n",
    "                print(f\"Number of actual columns: {z_scores.shape[1]}\")\n",
    "                raise ValueError(f\"{category} columns vs z_scores columns mismatch\")\n",
    "\n",
    "            weighted_z = z_scores @ weights\n",
    "            components[category] = weighted_z * feature_weights[category]\n",
    "\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(20%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_corr.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_cov.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Modified final selection section with generalized explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        cluster_members = cluster_members.sort_values('risk_adj_score', ascending=False)\n",
    "\n",
    "        # Get top candidates by different metrics\n",
    "        top_risk_adj = cluster_members.iloc[0]\n",
    "        top_raw_score = cluster_members.nlargest(1, 'score').iloc[0]\n",
    "\n",
    "        selected_ticker = top_risk_adj['ticker']\n",
    "        final_portfolio.append(selected_ticker)\n",
    "\n",
    "        # Cluster header\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected_ticker} \"\n",
    "              f\"(Raw Score: {top_risk_adj['score']:.2f}, \"\n",
    "              f\"Risk-Adj: {top_risk_adj['risk_adj_score']:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "\n",
    "        # Add explanation when top raw score isn't selected\n",
    "        if top_risk_adj['ticker'] != top_raw_score['ticker']:\n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - Higher raw score candidate: {top_raw_score['ticker']} ({top_raw_score['score']:.2f})\")\n",
    "            print(f\"   - Comparison of risk-adjusted returns:\")\n",
    "            print(f\"     {selected_ticker}: {top_risk_adj['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_risk_adj['score']:.2f}/Vol: {np.sqrt(top_risk_adj['variance']):.2f})\")\n",
    "            print(f\"     {top_raw_score['ticker']}: {top_raw_score['risk_adj_score']:.2f} \"\n",
    "                  f\"(Score: {top_raw_score['score']:.2f}/Vol: {np.sqrt(top_raw_score['variance']):.2f})\")\n",
    "            print(\"   - Selection prioritizes better reward/risk ratio over raw score\")\n",
    "            print(f\"   - Volatility difference: {abs(np.sqrt(top_risk_adj['variance']) - np.sqrt(top_raw_score['variance'])):.2f}\")\n",
    "            print(\"   - Lower volatility allows larger position size at same risk budget\")\n",
    "\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "\n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "\n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "\n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"   - OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "\n",
    "final_portfolio = portfolio_optimizer_(\n",
    "    df_data,\n",
    "    df_corr,\n",
    "    df_cov,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last column since it's a duplicate of the second-to-last column\n",
    "df_data = df_data.drop(columns='Perf 3D %') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
