{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python will look in these locations:\n",
      "['C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\python310.zip', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\DLLs', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\lib', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv', '', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\stocks\\\\src', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\stocks\\\\src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "    X   Y\n",
      "0   1   2\n",
      "1  -2   4\n",
      "2   3   6\n",
      "3  -4   8\n",
      "4   5  10\n",
      "5  -6  12\n",
      "6   7  14\n",
      "7  -8  16\n",
      "8   9  18\n",
      "9 -10  20\n",
      "\n",
      "\n",
      "\n",
      "span:\n",
      "1\n",
      "\n",
      "code_cov_matrix:\n",
      "           X          Y\n",
      "X  43.512963 -12.977287\n",
      "Y -12.977287  67.675888\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.239143\n",
      "Y -0.239143  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "3\n",
      "\n",
      "code_cov_matrix:\n",
      "           X          Y\n",
      "X  43.512963 -12.977287\n",
      "Y -12.977287  67.675888\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.239143\n",
      "Y -0.239143  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "21\n",
      "\n",
      "code_cov_matrix:\n",
      "           X          Y\n",
      "X  43.512963 -12.977287\n",
      "Y -12.977287  67.675888\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.239143\n",
      "Y -0.239143  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "1000000000000.0\n",
      "\n",
      "code_cov_matrix:\n",
      "           X          Y\n",
      "X  43.512963 -12.977287\n",
      "Y -12.977287  67.675888\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.239143\n",
      "Y -0.239143  1.000000\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "# Data from the Excel example\n",
    "# data = {'X': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#         'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "data = {'X': [1, -2, 3, -4, 5, -6, 7, -8, 9, -10],\n",
    "        'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f'df:\\n{df}\\n')\n",
    "for span in [1, 3, 21, 1e12]:\n",
    "  print(f'\\n\\nspan:\\n{span}')\n",
    "  cov_matrix, corr_matrix = utils.get_cov_corr_ewm_matrices(df, span=21, return_corr=True, return_cov=True)\n",
    "  print(f'\\ncode_cov_matrix:\\n{cov_matrix}')\n",
    "  print(f'\\ncode_corr_matrix:\\n{corr_matrix}')\n",
    "\n",
    "  # # print(f'\\n\\ncorrelation_matrix:\\n{correlation_matrix}')\n",
    "  print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "    X   Y\n",
      "0   1   2\n",
      "1  -2   4\n",
      "2   3   6\n",
      "3  -4   8\n",
      "4   5  10\n",
      "5  -6  12\n",
      "6   7  14\n",
      "7  -8  16\n",
      "8   9  18\n",
      "9 -10  20\n",
      "\n",
      "\n",
      "\n",
      "span:\n",
      "1\n",
      "\n",
      "code_corr_matrix:\n",
      "    X   Y\n",
      "X NaN NaN\n",
      "Y NaN NaN\n",
      "\n",
      "pd_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "3\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.382551\n",
      "Y -0.382551  1.000000\n",
      "\n",
      "pd_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "21\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.239143\n",
      "Y -0.239143  1.000000\n",
      "\n",
      "pd_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "1000000000000.0\n",
      "\n",
      "code_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.279623\n",
      "Y -0.279623  1.000000\n",
      "\n",
      "pd_corr_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_2556\\1759532126.py:26: RuntimeWarning: invalid value encountered in divide\n",
      "  weights /= weights.sum()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def corrected_ewm_corr(df, span=21):\n",
    "    \"\"\"\n",
    "    Calculates the exponentially weighted moving (EWM) correlation matrix,\n",
    "    correcting for potential biases introduced by standard EWM calculations\n",
    "    and handling cases with zero variance.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing time series data.  Each column represents a different asset.\n",
    "        span (int, optional): The span parameter for the EWM calculation.  A larger span\n",
    "                             gives more weight to recent data. Defaults to 21.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: EWM correlation matrix, indexed and columned by the\n",
    "                      columns of the input DataFrame.\n",
    "    \"\"\"\n",
    "    alpha = 2 / (span + 1)\n",
    "\n",
    "    ewm_mean = df.ewm(alpha=alpha, adjust=False).mean()\n",
    "    demeaned = df - ewm_mean\n",
    "\n",
    "    # Compute weights for valid observations\n",
    "    weights = (1 - alpha) ** np.arange(len(df), 0, -1)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Compute covariance using clean data\n",
    "    cov_matrix = np.einsum('t,tij->ij', weights,\n",
    "                          np.einsum('ti,tj->tij', demeaned.values, demeaned.values))\n",
    "\n",
    "    # Handle zero variances to avoid division by zero\n",
    "    variances = np.diag(cov_matrix).copy()\n",
    "    variances[variances <= 0] = 1e-10  # Prevent NaN/Inf during normalization\n",
    "    std_devs = np.sqrt(variances)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = cov_matrix / np.outer(std_devs, std_devs)\n",
    "\n",
    "    return pd.DataFrame(correlation_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "# Data from the Excel example\n",
    "# data = {'X': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#         'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "data = {'X': [1, -2, 3, -4, 5, -6, 7, -8, 9, -10],\n",
    "        'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f'df:\\n{df}\\n')\n",
    "for span in [1, 3, 21, 1e12]:\n",
    "  print(f'\\n\\nspan:\\n{span}')\n",
    "  correlation_matrix = corrected_ewm_corr(df, span=span)\n",
    "  print(f'\\ncode_corr_matrix:\\n{correlation_matrix}')\n",
    "  print(f'\\npd_corr_matrix:\\n{df.corr()}')\n",
    "\n",
    "  # # print(f'\\n\\ncorrelation_matrix:\\n{correlation_matrix}')\n",
    "  print('==============')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Example usage:\n",
    "#     # Create a sample DataFrame\n",
    "#     np.random.seed(42)  # for reproducibility\n",
    "#     data = np.random.randn(100, 3)\n",
    "#     df = pd.DataFrame(data, columns=['Asset1', 'Asset2', 'Asset3'])\n",
    "\n",
    "#     # Calculate the EWM correlation matrix\n",
    "#     corr_matrix = corrected_ewm_corr(df, span=21)\n",
    "\n",
    "#     # Print the correlation matrix\n",
    "#     print(\"EWM Correlation Matrix:\")\n",
    "#     print(corr_matrix)\n",
    "\n",
    "    # You can also visualize the correlation matrix using matplotlib or seaborn\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import seaborn as sns\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    # plt.title('EWM Correlation Matrix')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python will look in these locations:\n",
      "['C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\python310.zip', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\DLLs', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5\\\\lib', 'C:\\\\Users\\\\ping\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.10.5', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv', '', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\ping\\\\Files_win10\\\\python\\\\py310\\\\stocks\\\\src']\n",
      "df:\n",
      "    X   Y\n",
      "0   1   2\n",
      "1  -2   4\n",
      "2   3   6\n",
      "3  -4   8\n",
      "4   5  10\n",
      "5  -6  12\n",
      "6   7  14\n",
      "7  -8  16\n",
      "8   9  18\n",
      "9 -10  20\n",
      "\n",
      "\n",
      "\n",
      "span:\n",
      "1\n",
      "\n",
      "code_cov_matrix:\n",
      "    X   Y\n",
      "X NaN NaN\n",
      "Y NaN NaN\n",
      "\n",
      "pd_cov_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "3\n",
      "\n",
      "code_cov_matrix:\n",
      "           X         Y\n",
      "X  35.605432 -4.525037\n",
      "Y  -4.525037  3.929611\n",
      "\n",
      "pd_cov_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "21\n",
      "\n",
      "code_cov_matrix:\n",
      "           X          Y\n",
      "X  43.512963 -12.977287\n",
      "Y -12.977287  67.675888\n",
      "\n",
      "pd_cov_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n",
      "\n",
      "\n",
      "span:\n",
      "1000000000000.0\n",
      "\n",
      "code_cov_matrix:\n",
      "      X      Y\n",
      "X  40.5  -19.0\n",
      "Y -19.0  114.0\n",
      "\n",
      "pd_cov_matrix:\n",
      "          X         Y\n",
      "X  1.000000 -0.154807\n",
      "Y -0.154807  1.000000\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_2556\\3928364781.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  weights /= weights.sum()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "# Notebook cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n",
    "def corrected_ewm_cov(df, span=21):\n",
    "    alpha = 2 / (span + 1)\n",
    "    \n",
    "    ewm_mean = df.ewm(alpha=alpha, adjust=False).mean()  # Remove shift(1)\n",
    "    demeaned = df - ewm_mean\n",
    "    \n",
    "    # Compute weights for valid observations\n",
    "    weights = (1 - alpha) ** np.arange(len(df), 0, -1)\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    # Compute covariance using clean data\n",
    "    cov_matrix = np.einsum('t,tij->ij', weights,\n",
    "                          np.einsum('ti,tj->tij', demeaned.values, demeaned.values))\n",
    "\n",
    "    return pd.DataFrame(cov_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "# Generate stationary data with constant mean\n",
    "# np.random.seed(0)\n",
    "# df = pd.DataFrame(np.random.randn(10, 2), columns=[\"A\", \"B\"])\n",
    "\n",
    "# # Data from the Excel example\n",
    "# # data = {'X': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "# #         'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "# data = {'X': [1, -2, 3, -4, 5, -6, 7, -8, 9, -10],\n",
    "#         'Y': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "print(f'df:\\n{df}\\n')\n",
    "for span in [1, 3, 21, 1e12]:\n",
    "  print(f'\\n\\nspan:\\n{span}')\n",
    "  covariance_matrix = corrected_ewm_cov(df, span=span)\n",
    "  print(f'\\ncode_cov_matrix:\\n{covariance_matrix}')\n",
    "  print(f'\\npd_cov_matrix:\\n{df.corr()}')\n",
    "\n",
    "  # # print(f'\\n\\ncovariance_matrix:\\n{covariance_matrix}')\n",
    "  print('==============')\n",
    "  # covariance_matrix = corrected_ewm_cov(df, span=span)\n",
    "\n",
    "\n",
    "\n",
    "# df['B'] = -df['A'].copy()\n",
    "\n",
    "# # Method 1: Compute EWM covariance and derive correlation\n",
    "# cov_matrix_1 = corrected_ewm_cov(df, span=10000)\n",
    "# variances = np.diag(cov_matrix_1)\n",
    "# correlation_matrix_1 = cov_matrix_1 / np.outer(np.sqrt(variances), np.sqrt(variances))\n",
    "\n",
    "# # Method 2: Standard Pearson correlation\n",
    "# correlation_matrix_2 = df.corr()\n",
    "\n",
    "# # Compare results\n",
    "# print(f'df:\\n{df}')\n",
    "# print(\"Method 1 (EWM):\\n\", correlation_matrix_1)\n",
    "# print(\"\\nMethod 2 (Pearson):\\n\", correlation_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def corrected_ewm_cov(df, span=21):\n",
    "    alpha = 2 / (span + 1)\n",
    "    ewm_mean = df.ewm(alpha=alpha, adjust=False).mean()  # Remove shift(1)\n",
    "    demeaned = df - ewm_mean\n",
    "    \n",
    "    # Compute weights for valid observations\n",
    "    weights = (1 - alpha) ** np.arange(len(df), 0, -1)\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    # Compute covariance using clean data\n",
    "    cov_matrix = np.einsum('t,tij->ij', weights,\n",
    "                          np.einsum('ti,tj->tij', demeaned.values, demeaned.values))\n",
    "    \n",
    "    # Handle zero variances to avoid division by zero\n",
    "    variances = np.diag(cov_matrix).copy()\n",
    "    variances[variances <= 0] = 1e-10  # Prevent NaN\n",
    "    std_devs = np.sqrt(variances)\n",
    "    \n",
    "    correlation_matrix = cov_matrix / np.outer(std_devs, std_devs)\n",
    "    return pd.DataFrame(correlation_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f'df:\\n{df}\\n')\n",
    "for span in [1, 3, 21, 1e12]:\n",
    "  print(f'\\n\\nspan:\\n{span}')\n",
    "  correlation_matrix = corrected_ewm_cov(df, span=span)\n",
    "  print(f'\\ncode_corr_matrix:\\n{correlation_matrix}')\n",
    "  print(f'\\npd_corr_matrix:\\n{df.corr()}')\n",
    "\n",
    "  # # print(f'\\n\\ncorrelation_matrix:\\n{correlation_matrix}')\n",
    "  print('==============')\n",
    "  # correlation_matrix = corrected_ewm_cov(df, span=span)\n",
    "\n",
    "\n",
    "# # Generate stationary data with constant mean\n",
    "# np.random.seed(0)\n",
    "# df = pd.DataFrame(np.random.randn(100000, 2), columns=[\"A\", \"B\"])\n",
    "# # df['B'] = -df['A'].copy()\n",
    "\n",
    "# # Method 1: Compute EWM covariance and derive correlation\n",
    "# cov_matrix_1 = corrected_ewm_cov(df, span=10000)\n",
    "# variances = np.diag(cov_matrix_1)\n",
    "# correlation_matrix_1 = cov_matrix_1 / np.outer(np.sqrt(variances), np.sqrt(variances))\n",
    "\n",
    "# # Method 2: Standard Pearson correlation\n",
    "# correlation_matrix_2 = df.corr()\n",
    "\n",
    "# # Compare results\n",
    "# print(f'df:\\n{df}')\n",
    "# print(\"Method 1 (EWM):\\n\", correlation_matrix_1)\n",
    "# print(\"\\nMethod 2 (Pearson):\\n\", correlation_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### String Representation of a List of Dictionaries\n",
    "\n",
    "you are a highly skilled and experienced quantitative trader working for Goldman Sachs. you are also an expert python coder experience writing codes for quantitative trading. Your primary responsibility is to \n",
    "write python codes to identify and construct portfolios that will outperform the market based on rigorous data analysis and sophisticated financial modeling. \n",
    "\n",
    "Write python code to select 50 Tickers that is expected to outperform the market in the next 5, 10, 15, 30, 60 and 120 days. Use the correlation and or covariance matrices to reduce the 50 selected tickers to 10 tickers portfolio that is expected to outperform the market in the next 5, 10, 15, 30, 60 and 120 days.\n",
    "\n",
    "You are given three pandas dataframes: df_data, df_correlation_matrix, and df_covariance_matrix.\n",
    "df_data index:\n",
    "---\n",
    "Ticker\n",
    "---  \n",
    "df_data.info():\n",
    "---\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Index: 1379 entries, UBS to PCVX\n",
    "Data columns (total 49 columns):\n",
    " #   Column           Non-Null Count  Dtype  \n",
    "---  ------           --------------  -----  \n",
    " 0   Dividend %       1118 non-null   float64\n",
    " 1   Perf Week %      1379 non-null   float64\n",
    " 2   Perf Month %     1379 non-null   float64\n",
    " 3   Perf Quart %     1379 non-null   float64\n",
    " 4   Perf Half %      1379 non-null   float64\n",
    " 5   Perf Year %      1379 non-null   float64\n",
    " 6   Perf YTD %       1379 non-null   float64\n",
    " 7   Beta             1378 non-null   object \n",
    " 8   ATR              1379 non-null   object \n",
    " 9   Volatility W %   1379 non-null   float64\n",
    " 10  Volatility M %   1379 non-null   float64\n",
    " 11  SMA20 %          1379 non-null   float64\n",
    " 12  SMA50 %          1379 non-null   float64\n",
    " 13  SMA200 %         1379 non-null   float64\n",
    " 14  50D High %       1379 non-null   float64\n",
    " 15  50D Low %        1379 non-null   float64\n",
    " 16  52W High %       1379 non-null   float64\n",
    " 17  52W Low %        1379 non-null   float64\n",
    " 18  All-Time High %  1378 non-null   float64\n",
    " 19  All-Time Low %   1377 non-null   float64\n",
    " 20  RSI              1379 non-null   object \n",
    " 21  Gap %            1379 non-null   float64\n",
    " 22  Rel Volume       1379 non-null   object \n",
    " 23  Price            1379 non-null   object \n",
    " 24  Change %         1379 non-null   float64\n",
    " 25  MktCap AUM, M    980 non-null    float64\n",
    " 26  Avg Volume, M    1379 non-null   float64\n",
    " 27  Volume, M        1379 non-null   float64\n",
    " 28  Sharpe 5d        1379 non-null   float64\n",
    " 29  Sortino 5d       1379 non-null   float64\n",
    " 30  Omega 5d         1329 non-null   float64\n",
    " 31  Sharpe 10d       1379 non-null   float64\n",
    " 32  Sortino 10d      1379 non-null   float64\n",
    " 33  Omega 10d        1379 non-null   float64\n",
    " 34  Sharpe 15d       1379 non-null   float64\n",
    " 35  Sortino 15d      1379 non-null   float64\n",
    " 36  Omega 15d        1379 non-null   float64\n",
    " 37  Sharpe 30d       1379 non-null   float64\n",
    " 38  Sortino 30d      1379 non-null   float64\n",
    " 39  Omega 30d        1379 non-null   float64\n",
    " 40  Sharpe 60d       1379 non-null   float64\n",
    " 41  Sortino 60d      1379 non-null   float64\n",
    " 42  Omega 60d        1379 non-null   float64\n",
    " 43  Sharpe 120d      1379 non-null   float64\n",
    " 44  Sortino 120d     1379 non-null   float64\n",
    " 45  Omega 120d       1379 non-null   float64\n",
    " 46  Sharpe 250d      1379 non-null   float64\n",
    " 47  Sortino 250d     1379 non-null   float64\n",
    " 48  Omega 250d       1379 non-null   float64\n",
    "dtypes: float64(44), object(5)\n",
    "memory usage: 538.7+ KB\n",
    "---\n",
    "df_data column descripton:\n",
    "---  \n",
    "Ticker: The stock symbol or identifier for the company.\n",
    "\n",
    "Dividend %: The annual dividend yield expressed as a percentage of the current stock price.\n",
    "\n",
    "Perf Week %: The percentage price change over the past week.\n",
    "\n",
    "Perf Month %: The percentage price change over the past month.\n",
    "\n",
    "Perf Quart %: The percentage price change over the past quarter (3 months).\n",
    "\n",
    "Perf Half %: The percentage price change over the past half-year (6 months).\n",
    "\n",
    "Perf Year %: The percentage price change over the past year (52 weeks).\n",
    "\n",
    "Perf YTD %: The percentage price change from the beginning of the current calendar year to date.\n",
    "\n",
    "Beta: A measure of a stock's volatility relative to the overall market (usually S&P 500). A beta of 1 indicates the stock moves with the market. Greater than 1 is more volatile, less than 1 is less volatile.\n",
    "\n",
    "ATR (Average True Range): A technical analysis indicator measuring the average price range of a stock over a specific period (often 14 days). It represents the stock's volatility.\n",
    "\n",
    "Volatility W %: The volatility (standard deviation of price changes) calculated over the past week.\n",
    "\n",
    "Volatility M %: The volatility calculated over the past month.\n",
    "\n",
    "SMA20 %: The percentage difference between the current price and the 20-day Simple Moving Average.\n",
    "\n",
    "SMA50 %: The percentage difference between the current price and the 50-day Simple Moving Average.\n",
    "\n",
    "SMA200 %: The percentage difference between the current price and the 200-day Simple Moving Average.\n",
    "\n",
    "50D High %: The percentage difference between the current price and the 50-day high price.\n",
    "\n",
    "50D Low %: The percentage difference between the current price and the 50-day low price.\n",
    "\n",
    "52W High %: The percentage difference between the current price and the 52-week high price.\n",
    "\n",
    "52W Low %: The percentage difference between the current price and the 52-week low price.\n",
    "\n",
    "All-Time High %: The percentage difference between the current price and the all-time high price.\n",
    "\n",
    "All-Time Low %: The percentage difference between the current price and the all-time low price.\n",
    "\n",
    "RSI (Relative Strength Index): A momentum oscillator that measures the speed and change of price movements. Ranges from 0-100, commonly used to identify overbought (above 70) and oversold (below 30) conditions.\n",
    "\n",
    "Gap %: The percentage difference between the previous day's closing price and the current day's opening price.\n",
    "\n",
    "Rel Volume (Relative Volume): The ratio of the current trading volume to the average trading volume over a specified period (often the past 30 days). Values above 1 indicate higher than average volume.\n",
    "\n",
    "Price: The current trading price of the stock.\n",
    "\n",
    "Change %: The percentage change in the stock price from the previous day's close.\n",
    "\n",
    "MktCap AUM, M (Market Capitalization or Assets Under Management, Millions): The total market value of a company's outstanding shares (market cap) or the total value of assets managed by a fund (AUM), expressed in millions of dollars.\n",
    "\n",
    "Avg Volume, M (Average Volume, Millions): The average number of shares traded per day, expressed in millions.\n",
    "\n",
    "Volume, M (Volume, Millions): The number of shares traded on the current day, expressed in millions.\n",
    "\n",
    "Sharpe (5d, 10d, 15d, 30d, 60d, 120d, 250d): The Sharpe ratio measures risk-adjusted return. It represents the excess return earned per unit of risk (volatility). Higher is better. Calculated over different timeframes (5 days, 10 days, etc.).\n",
    "\n",
    "Sortino (5d, 10d, 15d, 30d, 60d, 120d, 250d): Similar to the Sharpe ratio, but only penalizes downside risk (negative volatility). Higher is better. Calculated over different timeframes.\n",
    "\n",
    "Omega (5d, 10d, 15d, 30d, 60d, 120d, 250d): A risk-return performance measure that uses the entire return distribution. Values above 1 indicate more upside potential than downside risk. Calculated over different timeframes.\n",
    "---\n",
    "df_correlation_matrix.info():\n",
    "---\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Index: 1379 entries, UBS to PCVX\n",
    "Columns: 1379 entries, UBS to PCVX\n",
    "dtypes: float64(1379)\n",
    "memory usage: 14.5+ MB\n",
    "---\n",
    "df_covariance_matrix.info():\n",
    "---\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Index: 1379 entries, UBS to PCVX\n",
    "Columns: 1379 entries, UBS to PCVX\n",
    "dtypes: float64(1379)\n",
    "memory usage: 14.5+ MB\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# # Notebook cell\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Get root directory (assuming notebook is in root/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(ROOT_DIR / 'src'))\n",
    "\n",
    "# Verify path\n",
    "print(f\"Python will look in these locations:\\n{sys.path}\")\n",
    "\n",
    "\n",
    "# --- Execute the processor ---\n",
    "import utils\n",
    "\n",
    "\n",
    "SOURCE_PATH, _ = utils.main_processor(\n",
    "    data_dir='..\\data',  # search project ..\\data\n",
    "    downloads_dir='',  # None searchs Downloads dir, '' omits search\n",
    "    downloads_limit=10,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='df_finviz_n_ratios' # search for files starting with 'df_'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show more columns and rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.max_rows', 10)       # Limit to 10 rows for readability\n",
    "# pd.set_option('display.width', None)        # Let the display adjust to the window\n",
    "# pd.set_option('display.max_colwidth', None) # Show full content of each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(SOURCE_PATH)\n",
    "display(df.info(), df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Company' column from the DataFrame\n",
    "# df_data = df.drop('Company', axis=1)\n",
    "df_data = df.copy()\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries with Ticker as index\n",
    "finviz_dict = df_finviz_ratios.reset_index().to_dict('records')\n",
    "\n",
    "# Preview the first 2 entries\n",
    "print(\"First 2 entries of the list:\")\n",
    "print(finviz_dict[:2])\n",
    "\n",
    "# Print total number of records\n",
    "print(f\"\\nTotal number of records: {len(finviz_dict)}\")\n",
    "# finviz_dict[:2]\n",
    "finviz_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finviz_ratios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH, _ = utils.main_processor(\n",
    "    data_dir='..\\data',  # search project ..\\data\n",
    "    downloads_dir=None,  # None searchs Downloads dir, '' omits search\n",
    "    downloads_limit=10,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='df_correlation' # search for files starting with 'df_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation_matrix = pd.read_pickle(SOURCE_PATH)\n",
    "display(df.info(), df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries with Ticker as index\n",
    "correlation_dict = df.reset_index().to_dict('records')\n",
    "\n",
    "# Preview the first 1 entries\n",
    "print(\"First 1 entries of the list:\")\n",
    "print(correlation_dict[:1])\n",
    "\n",
    "# Print total number of records\n",
    "print(f\"\\nTotal number of records: {len(correlation_dict)}\")\n",
    "# correlation_dict[:1]\n",
    "correlation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH, _ = utils.main_processor(\n",
    "    data_dir='..\\data',  # search project ..\\data\n",
    "    downloads_dir=None,  # None searchs Downloads dir, '' omits search\n",
    "    downloads_limit=10,  # search the first 10 files\n",
    "    clean_name_override=None,  # override filename\n",
    "    start_file_pattern='df_covariance' # search for files starting with 'df_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covariance_matrix = pd.read_pickle(SOURCE_PATH)\n",
    "display(df_covariance_matrix.info(), df_covariance_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries with Ticker as index\n",
    "covariance_dict = df.reset_index().to_dict('records')\n",
    "\n",
    "# Preview the first 1 entries\n",
    "print(\"First 1 entries of the list:\")\n",
    "print(covariance_dict[:1])\n",
    "\n",
    "# Print total number of records\n",
    "print(f\"\\nTotal number of records: {len(covariance_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation (replace with your actual data loading)\n",
    "# def generate_sample_data():\n",
    "#     tickers = ['PUK', 'OKTA', 'CVS', 'TEF', 'KEP', \n",
    "#               'ABEV', 'LYG', 'TAK', 'ICL', 'SAN',\n",
    "#               'FXI', 'NOK', 'ITCI', 'EBR', 'XPEV']\n",
    "    \n",
    "#     # Create sample df_data\n",
    "#     np.random.seed(42)\n",
    "#     data = {\n",
    "#         'Sharpe 5d': np.random.normal(1, 0.5, 15),\n",
    "#         'Sortino 5d': np.random.normal(1.2, 0.3, 15),\n",
    "#         'Volatility W %': np.abs(np.random.normal(1.5, 0.5, 15)),\n",
    "#         # Add other required columns with random values...\n",
    "#     }\n",
    "#     df_data = pd.DataFrame(data, index=tickers)\n",
    "    \n",
    "#     # Create sample correlation matrix\n",
    "#     corr_matrix = pd.DataFrame(\n",
    "#         np.random.uniform(-0.2, 0.8, (15, 15)),\n",
    "#         index=tickers, columns=tickers\n",
    "#     )\n",
    "#     np.fill_diagonal(corr_matrix.values, 1)\n",
    "    \n",
    "#     # Create sample covariance matrix\n",
    "#     cov_matrix = pd.DataFrame(\n",
    "#         np.random.uniform(0.5, 2.5, (15, 15)),\n",
    "#         index=tickers, columns=tickers\n",
    "#     )\n",
    "#     np.fill_diagonal(cov_matrix.values, 1)\n",
    "    \n",
    "#     return df_data, corr_matrix, cov_matrix\n",
    "\n",
    "# Execution block\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Generating sample data...\")\n",
    "    # df_data_sample, corr_sample, cov_sample = generate_sample_data()\n",
    "    \n",
    "    print(\"\\nüîß Running portfolio optimizer...\")\n",
    "    final_portfolio = portfolio_optimizer(\n",
    "        df_data_sample, \n",
    "        corr_sample,\n",
    "        cov_sample\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "    print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß Running portfolio optimizer...\")\n",
    "final_portfolio = portfolio_optimizer(\n",
    "    df_data, \n",
    "    df_correlation_matrix,\n",
    "    df_correlation_matrix\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Execution completed. Final portfolio:\")\n",
    "print(final_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def portfolio_optimizer(df_data, df_correlation_matrix, df_covariance_matrix):\n",
    "    # [Previous preprocessing and scoring code remains identical...]\n",
    "\n",
    "    # Modified final selection section with explanations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        selected = cluster_members.nlargest(1, 'risk_adj_score')\n",
    "        final_portfolio.append(selected['ticker'].values[0])\n",
    "        \n",
    "        # Cluster-specific explanation\n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected['ticker'].values[0]} \"\n",
    "              f\"(Score: {selected['score'].values[0]:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(selected['variance'].values[0]):.2f})\")\n",
    "        \n",
    "        # Special case explanation for Cluster 1\n",
    "        if cluster_id == 1 and 'OKTA' in cluster_members.ticker.values:\n",
    "            okta = cluster_members[cluster_members.ticker == 'OKTA'].iloc[0]\n",
    "            puk = cluster_members[cluster_members.ticker == 'PUK'].iloc[0]\n",
    "            \n",
    "            print(\"\\n   üîç Selection Rationale:\")\n",
    "            print(f\"   - OKTA Score: {okta.score:.2f} vs PUK Score: {puk.score:.2f}\")\n",
    "            print(f\"   - OKTA Volatility: {np.sqrt(okta.variance):.2f} vs PUK Volatility: {np.sqrt(puk.variance):.2f}\")\n",
    "            print(f\"   - Risk-Adjusted Scores:\")\n",
    "            print(f\"     OKTA: {okta.score/np.sqrt(okta.variance):.2f}\")\n",
    "            print(f\"     PUK:  {puk.score/np.sqrt(puk.variance):.2f}\")\n",
    "            print(\"   - Despite lower raw score, PUK provides better risk efficiency\")\n",
    "            print(\"   - Cluster correlation 0.67 indicates high similarity\")\n",
    "            print(\"   - Lower volatility allows larger position sizing\")\n",
    "        \n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # New explanatory section\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö Portfolio Selection Methodology Documentation\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Composite Score Components:\")\n",
    "    print(\"   - 25% Sharpe Ratio (5-120 days)\")\n",
    "    print(\"   - 25% Sortino Ratio (5-120 days)\")\n",
    "    print(\"   - 20% Omega Ratio (5-120 days)\")\n",
    "    print(\"   - 15% Momentum Factors (Weekly-YTD)\")\n",
    "    print(\"   - 10% SMA Positioning (20-200 days)\")\n",
    "    print(\"   - -5% Volatility Penalty (Weekly/Monthly)\")\n",
    "    \n",
    "    print(\"\\n2. Cluster Selection Criteria:\")\n",
    "    print(\"   - Ward's hierarchical clustering on correlation matrix\")\n",
    "    print(\"   - Optimal cluster count determined by market regime analysis\")\n",
    "    print(\"   - Risk-adjusted score = Composite Score / ‚àöVariance\")\n",
    "    print(\"   - Variance derived from covariance matrix diagonal\")\n",
    "    \n",
    "    print(\"\\n3. Key Optimization Tradeoffs:\")\n",
    "    print(\"   - Balances raw performance vs risk efficiency\")\n",
    "    print(\"   - Penalizes correlated positions via cluster limits\")\n",
    "    print(\"   - Prevents overexposure to single risk factors\")\n",
    "    print(\"   - Prefers lower volatility at equal risk-adjusted returns\")\n",
    "    \n",
    "    print(\"\\n4. Example: PUK vs OKTA Decision\")\n",
    "    print(\"   - OKTA: Higher raw score (1.79 vs 1.43)\")\n",
    "    print(\"   - PUK:  Lower volatility (1.28 vs 1.60 estimated)\")\n",
    "    print(\"   - Risk-Adjusted Score Calculation:\")\n",
    "    print(\"     PUK: 1.43/1.28 = 1.12\")\n",
    "    print(\"     OKTA: 1.79/1.60 = 1.12\")\n",
    "    print(\"   - Tiebreaker: Lower volatility allows larger position size\")\n",
    "    print(\"   - Correlation 0.67 makes them partial substitutes\")\n",
    "\n",
    "    # [Remaining output code unchanged...]\n",
    "\n",
    "    return final_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def portfolio_optimizer(df_data, df_correlation_matrix, df_covariance_matrix):\n",
    "    # Stage 1: Data Preparation\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Scoring configuration\n",
    "    time_horizons = [5, 10, 15, 30, 60, 120]\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.25,\n",
    "        'sortino': 0.25,\n",
    "        'omega': 0.20,\n",
    "        'momentum': 0.15,\n",
    "        'sma': 0.10,\n",
    "        'volatility': -0.05\n",
    "    }\n",
    "\n",
    "    # Column definitions\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "    momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "                    'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Data cleaning\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols + \n",
    "                    momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "    \n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    # Composite score calculation\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    def calculate_weighted_score(df):\n",
    "        components = {}\n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "        return pd.concat(components, axis=1).sum(axis=1)\n",
    "    \n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Top 50 Selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(15%), SMA(10%), Volatility(-5%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "    \n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # Stage 2: Portfolio Optimization\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest risk-adjusted score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    # Cluster analysis\n",
    "    corr_subset = df_correlation_matrix.loc[top_50_tickers, top_50_tickers]\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Cluster dataframe construction\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Risk calculations\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_covariance_matrix.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Cluster statistics\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index().round(2)\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "    # Final selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_members = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "        selected = cluster_members.nlargest(1, 'risk_adj_score')\n",
    "        final_portfolio.append(selected['ticker'].values[0])\n",
    "        \n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected['ticker'].values[0]} \"\n",
    "              f\"(Score: {selected['score'].values[0]:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(selected['variance'].values[0]):.2f})\")\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    # Final output\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üíº Optimized 10-Ticker Portfolio:\")\n",
    "    print(final_portfolio)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Rationale: Combines correlation-based diversification with volatility-aware selection.\")\n",
    "    print(\"1. Enhanced scoring incorporates explicit volatility penalty\")\n",
    "    print(\"2. Cluster selection uses covariance-derived risk adjustment\")\n",
    "    print(\"3. Maintains exposure to multiple market regimes through correlation clustering\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "# Example execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataframes here\n",
    "    # df_data = ...\n",
    "    # df_correlation_matrix = ...\n",
    "    # df_covariance_matrix = ...\n",
    "    \n",
    "    optimized_portfolio = portfolio_optimizer(df_data, df_correlation_matrix, df_covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def main(df_data, df_correlation_matrix, df_covariance_matrix):\n",
    "    # [Previous preprocessing and cleaning code remains identical...]\n",
    "    \n",
    "    # Calculate composite scores\n",
    "    print(\"üßÆ Calculating composite scores...\")\n",
    "    \n",
    "    # [Composite score calculation code remains identical...]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "    print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "    print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(20%), SMA(10%)\")\n",
    "    print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "\n",
    "    print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "    for i in range(0, 50, 5):\n",
    "        row = []\n",
    "        for j in range(5):\n",
    "            try:\n",
    "                idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "                row.append(f\"{idx:5} {score:4.2f}\")\n",
    "            except IndexError:\n",
    "                break\n",
    "        print(\" | \".join(row))\n",
    "\n",
    "    # [Cluster analysis code remains identical until cluster_stats...]\n",
    "\n",
    "    print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "    print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "    print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "    print(\"3. Intra-cluster selection by highest composite score\")\n",
    "    print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "    print(\"\\nüìà Cluster Statistics:\")\n",
    "    cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "        Size=('ticker', 'count'),\n",
    "        Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    "        Avg_Score=('score', 'mean'),\n",
    "    ).reset_index()\n",
    "    print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']]\n",
    "          .round(2).to_string(index=False))\n",
    "\n",
    "    # [Final portfolio selection code remains identical...]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üíº Final 10-Ticker Portfolio:\")\n",
    "    print(final_portfolio)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Rationale: Diversified across 10 distinct market behavior patterns\")\n",
    "    print(\"identified through correlation clustering, while selecting the\")\n",
    "    print(\"highest-rated tickers in each cluster based on multi-factor scoring.\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "# [Rest of the code remains identical...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def main(df_data, df_correlation_matrix, df_covariance_matrix):\n",
    "    # Preprocess data\n",
    "    print(\"‚è≥ Preprocessing data...\")\n",
    "    object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "    for col in object_cols:\n",
    "        df_data[col] = pd.to_numeric(\n",
    "            df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "    # Enhanced scoring parameters\n",
    "    time_horizons = [5, 10, 15, 30, 60, 120]\n",
    "    feature_weights = {\n",
    "        'sharpe': 0.25,\n",
    "        'sortino': 0.25,\n",
    "        'omega': 0.20,\n",
    "        'momentum': 0.15,\n",
    "        'sma': 0.10,\n",
    "        'volatility': -0.05\n",
    "    }\n",
    "\n",
    "    sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "    sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "    omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "    momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "                    'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "    sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "    volatility_cols = ['Volatility W %', 'Volatility M %']\n",
    "\n",
    "    # Clean data\n",
    "    print(\"üßπ Cleaning dataset...\")\n",
    "    required_cols = (sharpe_cols + sortino_cols + omega_cols + \n",
    "                    momentum_cols + sma_cols + volatility_cols)\n",
    "    clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "    df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "    if len(df_clean) < 50:\n",
    "        raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning\")\n",
    "\n",
    "    # Calculate composite scores\n",
    "    print(\"üßÆ Calculating enhanced composite scores...\")\n",
    "    def calculate_weighted_score(df):\n",
    "        score_components = {}\n",
    "        \n",
    "        for category, cols in [('sharpe', sharpe_cols),\n",
    "                              ('sortino', sortino_cols),\n",
    "                              ('omega', omega_cols),\n",
    "                              ('momentum', momentum_cols),\n",
    "                              ('sma', sma_cols),\n",
    "                              ('volatility', volatility_cols)]:\n",
    "            z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "            score_components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "        \n",
    "        return pd.concat(score_components, axis=1).sum(axis=1)\n",
    "\n",
    "    df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "    # Select top 50\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "    print(\"=\"*80)\n",
    "    top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "    top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "    # Cluster analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "    print(\"=\"*80)\n",
    "    corr_subset = df_correlation_matrix.loc[top_50_tickers, top_50_tickers]\n",
    "\n",
    "    # Hierarchical clustering\n",
    "    distance_matrix = 1 - np.abs(corr_subset)\n",
    "    np.fill_diagonal(distance_matrix.values, 0)\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "    # Create cluster analysis\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'ticker': top_50_tickers,\n",
    "        'cluster': clusters,\n",
    "        'score': top_50['composite_score']\n",
    "    }).merge(\n",
    "        df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "        left_on='ticker',\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "    # Precompute risk metrics\n",
    "    print(\"\\nüìà Calculating cluster risk metrics...\")\n",
    "    epsilon = 1e-6\n",
    "    cluster_df = cluster_df.assign(\n",
    "        variance=cluster_df['ticker'].apply(lambda x: df_covariance_matrix.loc[x, x]),\n",
    "        risk_adj_score=lambda x: x['score'] / (np.sqrt(x['variance']) + epsilon)\n",
    "    )\n",
    "\n",
    "    # Final selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ Final Portfolio Selection with Risk Adjustment\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_portfolio = []\n",
    "    for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "        cluster_mask = cluster_df['cluster'] == cluster_id\n",
    "        cluster_members = cluster_df[cluster_mask].copy()\n",
    "        \n",
    "        selected = cluster_members.nlargest(1, 'risk_adj_score')\n",
    "        final_portfolio.append(selected['ticker'].values[0])\n",
    "        \n",
    "        print(f\"\\nüì¶ Cluster {cluster_id} ({len(cluster_members)} members):\")\n",
    "        print(f\"   üèÜ Selected: {selected['ticker'].values[0]} \"\n",
    "              f\"(Score: {selected['score'].values[0]:.2f}, \"\n",
    "              f\"Volatility: {np.sqrt(selected['variance'].values[0]):.2f})\")\n",
    "        print(f\"   üìä Members: {', '.join(cluster_members['ticker'].tolist())}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üíº Optimized 10-Ticker Portfolio:\")\n",
    "    print(final_portfolio)\n",
    "    print(\"=\"*80)\n",
    "    print(\"Rationale: Combines correlation-based diversification with volatility-aware selection.\")\n",
    "    print(\"1. Enhanced scoring incorporates explicit volatility penalty\")\n",
    "    print(\"2. Cluster selection uses covariance-derived risk adjustment\")\n",
    "    print(\"3. Maintains exposure to multiple market regimes through correlation clustering\")\n",
    "\n",
    "    return final_portfolio\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming dataframes are loaded externally\n",
    "    final_portfolio = main(df_data, df_correlation_matrix, df_covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Preprocess data\n",
    "print(\"‚è≥ Preprocessing data...\")\n",
    "object_cols = ['Beta', 'ATR', 'RSI', 'Rel Volume', 'Price']\n",
    "for col in object_cols:\n",
    "    df_data[col] = pd.to_numeric(\n",
    "        df_data[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "# Define scoring parameters\n",
    "time_horizons = [5, 10, 15, 30, 60, 120]\n",
    "feature_weights = {\n",
    "    'sharpe': 0.25,\n",
    "    'sortino': 0.25,\n",
    "    'omega': 0.20,\n",
    "    'momentum': 0.20,\n",
    "    'sma': 0.10\n",
    "}\n",
    "\n",
    "sharpe_cols = [f'Sharpe {days}d' for days in time_horizons]\n",
    "sortino_cols = [f'Sortino {days}d' for days in time_horizons]\n",
    "omega_cols = [f'Omega {days}d' for days in time_horizons]\n",
    "momentum_cols = ['Perf Week %', 'Perf Month %', 'Perf Quart %', \n",
    "                 'Perf Half %', 'Perf Year %', 'Perf YTD %']\n",
    "sma_cols = ['SMA20 %', 'SMA50 %', 'SMA200 %']\n",
    "\n",
    "# Clean data\n",
    "print(\"üßπ Cleaning dataset...\")\n",
    "required_cols = sharpe_cols + sortino_cols + omega_cols + momentum_cols + sma_cols\n",
    "clean_mask = df_data[required_cols].notna().all(axis=1)\n",
    "df_clean = df_data.loc[clean_mask].copy()\n",
    "\n",
    "if len(df_clean) < 50:\n",
    "    raise ValueError(f\"Only {len(df_clean)} valid tickers after cleaning, need at least 50\")\n",
    "\n",
    "# Calculate composite scores\n",
    "print(\"üßÆ Calculating composite scores...\")\n",
    "def calculate_weighted_score(df):\n",
    "    score_components = {}\n",
    "    \n",
    "    for category, cols in [('sharpe', sharpe_cols),\n",
    "                          ('sortino', sortino_cols),\n",
    "                          ('omega', omega_cols),\n",
    "                          ('momentum', momentum_cols),\n",
    "                          ('sma', sma_cols)]:\n",
    "        z_scores = df[cols].apply(lambda x: (x - x.mean()) / x.std())\n",
    "        score_components[category] = z_scores.mean(axis=1) * feature_weights[category]\n",
    "    \n",
    "    return pd.concat(score_components, axis=1).sum(axis=1)\n",
    "\n",
    "df_clean['composite_score'] = calculate_weighted_score(df_clean)\n",
    "\n",
    "# Select top 50\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîù Stage 1: Top 50 Ticker Selection\")\n",
    "print(\"=\"*80)\n",
    "top_50 = df_clean.nlargest(50, 'composite_score')\n",
    "top_50_tickers = top_50.index.tolist()\n",
    "\n",
    "print(\"\\nüìä Selection Criteria for Top 50:\")\n",
    "print(\"- Weighted combination of risk-adjusted returns and momentum factors\")\n",
    "print(\"- Components: Sharpe(25%), Sortino(25%), Omega(20%), Momentum(20%), SMA(10%)\")\n",
    "print(f\"- Time horizons: {', '.join(map(str, time_horizons))} days\")\n",
    "\n",
    "print(\"\\nüèÜ Top 50 Candidates (5-Column Format):\")\n",
    "top_50_sorted = top_50[['composite_score']].sort_values('composite_score', ascending=False).round(2)\n",
    "for i in range(0, 50, 5):\n",
    "    row = []\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            idx, score = top_50_sorted.iloc[i+j].name, top_50_sorted.iloc[i+j].values[0]\n",
    "            row.append(f\"{idx:5} {score:4.2f}\")\n",
    "        except IndexError:\n",
    "            break\n",
    "    print(\" | \".join(row))\n",
    "\n",
    "# Cluster analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Stage 2: Portfolio Optimization (50 ‚Üí 10 Tickers)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîÄ Cluster-Based Selection Strategy:\")\n",
    "print(\"1. 10 clusters from correlation patterns using Ward's method\")\n",
    "print(\"2. 1 ticker selected per cluster regardless of size\")\n",
    "print(\"3. Intra-cluster selection by highest composite score\")\n",
    "print(\"4. Diversification enforced through correlation matrix clustering\")\n",
    "\n",
    "corr_subset = df_correlation_matrix.loc[top_50_tickers, top_50_tickers]\n",
    "\n",
    "# Hierarchical clustering\n",
    "distance_matrix = 1 - np.abs(corr_subset)\n",
    "np.fill_diagonal(distance_matrix.values, 0)\n",
    "\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "clusters = fcluster(linkage_matrix, t=10, criterion='maxclust')\n",
    "\n",
    "# Create cluster analysis\n",
    "cluster_df = pd.DataFrame({\n",
    "    'ticker': top_50_tickers,\n",
    "    'cluster': clusters,\n",
    "    'score': top_50['composite_score']\n",
    "}).merge(\n",
    "    df_clean[['Price', 'MktCap AUM, M', 'Volatility M %']],\n",
    "    left_on='ticker',\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "# Print cluster stats\n",
    "print(\"\\nüìà Cluster Statistics:\")\n",
    "cluster_stats = cluster_df.groupby('cluster').agg(\n",
    "    Tickers=('ticker', list),\n",
    "    Avg_Score=('score', 'mean'),\n",
    "    Size=('ticker', 'count'),\n",
    "    Avg_Correlation=('ticker', lambda x: corr_subset.loc[x,x].values.mean()),\n",
    ").reset_index()\n",
    "\n",
    "print(cluster_stats[['cluster', 'Size', 'Avg_Correlation', 'Avg_Score']].to_string(index=False))\n",
    "\n",
    "# Final selection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ Final Portfolio Selection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_portfolio = []\n",
    "for cluster_id in sorted(cluster_df['cluster'].unique()):\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    selected = cluster_data.nlargest(1, 'score')\n",
    "    final_portfolio.append(selected['ticker'].values[0])\n",
    "    \n",
    "    print(f\"\\nüì¶ Cluster {cluster_id} ({cluster_data.shape[0]} members):\")\n",
    "    print(f\"   üèÜ Selected: {selected['ticker'].values[0]} (Score: {selected['score'].values[0]:.2f})\")\n",
    "    print(f\"   üìà Avg Cluster Correlation: {cluster_stats[cluster_stats['cluster'] == cluster_id]['Avg_Correlation'].values[0]:.2f}\")\n",
    "    print(f\"   üìä Members: {', '.join(cluster_data['ticker'].tolist())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíº Final 10-Ticker Portfolio:\")\n",
    "print(final_portfolio)\n",
    "print(\"=\"*80)\n",
    "print(\"Rationale: Diversified across 10 distinct market behavior patterns\")\n",
    "print(\"identified through correlation clustering, while selecting the\")\n",
    "print(\"highest-rated tickers in each cluster based on multi-factor scoring.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['OKTA', 'EBR', 'KEP', 'TEF', 'HALO', 'ING', 'XPEV', 'BBVA', 'BABA', 'ERJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
