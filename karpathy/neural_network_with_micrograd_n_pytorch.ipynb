{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, torch\n",
    "import numpy as np\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose = True   # print calculation output and weights and bias matrices \n",
    "verbose = False  # print calculation output only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "  # import matplotlib.pyplot as plt\n",
    "  \n",
    "  # Create a list of iterations\n",
    "  iterations = range(len(losses))\n",
    "\n",
    "  # Plot the loss as a function of iteration\n",
    "  plt.plot(iterations, losses)\n",
    "\n",
    "  # Add a title to the plot\n",
    "  plt.title('Loss vs. Iteration')\n",
    "\n",
    "  # Add labels to the x-axis and y-axis\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(parameters):\n",
    "  # number of parameters (e.g sum (weights + bias to each neuron and output))\n",
    "  # MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n",
    "  # print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(parameters())}\\n')\n",
    "  print(f'Total parameters: {len(parameters())}\\n')  \n",
    "\n",
    "  # print first 5 parameters\n",
    "  for i, v in enumerate(parameters()):\n",
    "    if i < 5:\n",
    "      print(f'i: {i:>2}, {v.data:>14.10f}')\n",
    "  \n",
    "  print('---')\n",
    "\n",
    "  # print last 5 parameters   \n",
    "  for i, v in enumerate(parameters()):\n",
    "    if i >= len(parameters()) - 5:\n",
    "      print(f'i: {i:>2}, {v.data:>14.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wt_n_b_mats(layers, verbose=False):\n",
    "  ''' Get neuron's weights and bias for each layer.\n",
    "  Inputs: If n = MLP(2, [3, 3, 1]), input is n.layers.\n",
    "\n",
    "  return: two lists of np.arrays. The first list is weight matrix for each layer\n",
    "          The second list is the bias matrix for each layer \n",
    "  '''\n",
    "  layer_cnt = len(layers)  # number of layers\n",
    "  w_mats = []  # list of weights matrix for each layer \n",
    "  b_mats = []  # list of bias matrix for each layer\n",
    "  if verbose:\n",
    "    print(f'layer_cnt: {layer_cnt}\\n')\n",
    "  for i, layer in enumerate(layers):\n",
    "      neuron_cnt = len(layer.neurons)  # numbers of neurons in the layer\n",
    "      if verbose: \n",
    "        print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n",
    "\n",
    "        print('----')\n",
    "      b_mat = []  # accumulate neuon's bias for each row     \n",
    "      for j, neuron in enumerate(layer.neurons):\n",
    "          if verbose:\n",
    "            print(f'layer: {i}, neuron {j}')\n",
    "          b = neuron.b.data  # bias of neuron \n",
    "          w_row = []  # accumulate neuon's weights for each row\n",
    "          b_row = []  # accumulate neuon's bias for each row\n",
    "          for k, w in enumerate(neuron.w):\n",
    "              w_row.append(w.data)\n",
    "              if verbose:\n",
    "                print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n",
    "          if j == 0:            \n",
    "              w_mat = np.array([w_row])\n",
    "          else:\n",
    "              w_mat = np.vstack((w_mat, w_row))\n",
    "          \n",
    "          b_mat.append(b)\n",
    "          if verbose:\n",
    "            print(f'b:  {b:10.7f}\\n')\n",
    "            print(f'b:  {b:10.7f}')        \n",
    "            print(f'b_mat:  {b_mat}\\n')\n",
    "      w_mats.append(w_mat)  \n",
    "      b_mats.append(np.array([b_mat]))        \n",
    "      if verbose:\n",
    "          print('------')\n",
    "\n",
    "  zipped_w_n_b = zip(w_mats, b_mats)\n",
    "  if verbose:\n",
    "    for i, w_n_b in enumerate(zipped_w_n_b):\n",
    "      print(f'layer: {i}')  # 1st layer is 0    \n",
    "      print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n",
    "      print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n",
    "\n",
    "  return w_mats, b_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(layers, verbose=verbose):\n",
    "  # Get Neural Network's Weights and Biases Matrices\n",
    "  # w_mats, b_mats = get_wt_n_b_mats(n.layers, verbose=verbose)\n",
    "  w_mats, b_mats = get_wt_n_b_mats(layers, verbose=verbose)\n",
    "\n",
    "  # Calculate Neural Network Output and Loss with Matrix Multiplication\n",
    "  for layer in range(len(layers)):\n",
    "    if layer == 0:  # first layer, use given inputs xs as inputs\n",
    "      input = xs_mats_T[layer]\n",
    "    else:  # after first layer, use outputs from preceding layers as inputs\n",
    "      input = output\n",
    "\n",
    "    weights = w_mats[layer]\n",
    "    bias = np.transpose(b_mats[layer])\n",
    "\n",
    "    weights_x_input = np.matmul(weights, input)\n",
    "    weights_x_input_plus_bias = weights_x_input + bias\n",
    "\n",
    "    # output = np.tanh(np.matmul(weights, input) + bias)\n",
    "    output = np.tanh(weights_x_input_plus_bias)\n",
    "\n",
    "    print(f'{\"-\"*50}')\n",
    "    print(f'Calculate Output of Layer: {layer}')    \n",
    "    print(f'weights {weights.shape}:\\n{weights}\\n')\n",
    "    print(f'input {input.shape}:\\n{input}\\n')\n",
    "\n",
    "    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n",
    "    print(f'bias {bias.shape}:\\n{bias}\\n')\n",
    "    print(f'weights_x_inputs_+_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')    \n",
    "\n",
    "    # print(f'output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n",
    "    print(f'Layer {layer} Output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n",
    "\n",
    "  yout = output[0]\n",
    "  err = (yout - ys)\n",
    "  err_sq = (err**2)\n",
    "  loss_sum = err_sq.sum()\n",
    "  loss_mean = err_sq.mean()\n",
    "\n",
    "  # print(f'-- Manual calculation results of neural network output and prediction error --')\n",
    "  print(f'-- Results of neural network outputs and Loss --')  \n",
    "  print(f'yout:           {yout}')   \n",
    "  print(f'desired output: {ys}')   \n",
    "  print(f'err:            {err}')\n",
    "  print(f'err_sq:         {err_sq}')\n",
    "  print(f'loss_sum:       {loss_sum}')\n",
    "  print(f'loss_mean:      {loss_mean}')\n",
    "\n",
    "  return yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrace\u001b[39m(root):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n",
    "  nodes, edges = set(), set()\n",
    "\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n",
    "\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # For any value in the graph, create a rectangular ('record') node for it.\n",
    "    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n",
    "\n",
    "    if n._op:\n",
    "      # If this value is a result of some operation, create an op node.\n",
    "      dot.node(name=uid + n._op, label=n._op)\n",
    "      # And connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # Connect nl to the op node of n2.\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data = {self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward    \n",
    "\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)        \n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        # topological sort\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1  # initialize\n",
    "        for node in reversed(topo):\n",
    "            node._backward()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        # random numbers evenly distributed between -1 and 1    \n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "#### my add ##########################################\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Neuron(w = {self.w}, b = {self.b})\"\n",
    "######################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        # print(list(zip(self.w, x)), self.b)\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        # print(f'w: {self.w}, b: {[self.b]}')\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "#### my add ##########################################\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Layer(neurons = {self.neurons})\"\n",
    "######################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        # params = []\n",
    "        # for neuron in self.neurons:\n",
    "        #     ps = neuron.parameters()\n",
    "        #     params.extend(ps)\n",
    "        # return params\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        # for layer in self.layers:\n",
    "        #     ps = layer.parameters()\n",
    "        #     params.extend(ps)\n",
    "        # return params\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   &nbsp;\n",
    "# - Human Brain and Artificial Neural Network - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons in Human Brain\n",
    "![](..\\karpathy\\img\\neuron_of_human_brain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuron Function\n",
    "\n",
    "<img src=\"..\\karpathy\\img\\Artificial Neuron Function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Artificial Neural Network<br>* input layer: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 nodes<br>* hidden layer 1: &nbsp;3 nodes<br>* hidden layer 2:&nbsp;&nbsp;3 nodes<br>*  output layer: &nbsp;&nbsp;&nbsp; 1 node<br>* node's bias and activation function are not shown\n",
    "\n",
    "<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n",
    "<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;\n",
    "# - Visualize Math Operations in a Hidden Layer -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Assume hidden layer with two inputs (X0, X1), and three neurons (b0, b1, b2)<br>* Two sets of inputs (X0, X1) are shown in different shades of gray<br>* Two sets of outputs (Y0, Y1, Y2) are shown in corresponding shades of gray<br>* Both sets of inputs are processed in one matrix operation \n",
    "\n",
    "<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;\n",
    "# - Create Simple Neural Network with Micrograd -\n",
    "##### MLP(2, [3, 3, 1])<br>* 2 input nodes<br>* 3 neurons in hidden layer 1<br>* 3 neurons in hidden layer 2<br>* 1 output node\n",
    "##### Initialize Neurons Parameters <br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = &nbsp;&nbsp;&nbsp;&nbsp;  9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = &nbsp;&nbsp;&nbsp; 4<br>*  total parameters: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron parameters, initialized with random numbers\n",
      "Total parameters: 25\n",
      "\n",
      "i:  0,   0.8899036883\n",
      "i:  1,   0.6581265054\n",
      "i:  2,  -0.2240616161\n",
      "i:  3,   0.1714180630\n",
      "i:  4,   0.8255614769\n",
      "---\n",
      "i: 20,  -0.7697832947\n",
      "i: 21,   0.1688963122\n",
      "i: 22,   0.0520137053\n",
      "i: 23,  -0.5471706554\n",
      "i: 24,  -0.9145694660\n"
     ]
    }
   ],
   "source": [
    "# create neural network and initialize weights and biases\n",
    "n = MLP(2, [3, 3, 1])\n",
    "\n",
    "# if verbose:\n",
    "if True:\n",
    "  print(\"Neuron parameters, initialized with random numbers\")\n",
    "  print_parameters(n.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;\n",
    "# - Set Inputs, Desired Outputs, Learning Rate -\n",
    "##### Inputs<br>* 1st set: [2.0, 1.0]<br>* 2nd set: [3.0, -2.0]\n",
    "##### Desired Outputs<br>* [1.0, -1.0] for all input sets\n",
    "##### Learning Rate<br>* 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "xs = [\n",
    "  [2.0, 1.0],\n",
    "  [3.0, -2.0]\n",
    "]\n",
    "\n",
    "# desired targets\n",
    "ys = [1.0, -1.0]\n",
    "\n",
    "# learning rate (i.e. step size)\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons' weights and bias of each layer\n",
      "layer: 0, neuron_cnt: 3, layer: Layer(neurons = [Neuron(w = [Value(data = 0.8899036882690474), Value(data = 0.6581265053572443)], b = Value(data = -0.22406161612183473)), Neuron(w = [Value(data = 0.1714180629604629), Value(data = 0.8255614769486865)], b = Value(data = -0.15983168018624094)), Neuron(w = [Value(data = -0.5179221907271578), Value(data = 0.6187998930300957)], b = Value(data = -0.4746346123562597))])\n",
      "layer: 1, neuron_cnt: 3, layer: Layer(neurons = [Neuron(w = [Value(data = 0.6127555993310718), Value(data = 0.7553960788010159), Value(data = -0.4572727385934998)], b = Value(data = 0.5723610764194322)), Neuron(w = [Value(data = 0.7670856435077669), Value(data = -0.5574178381054773), Value(data = -0.221338740031628)], b = Value(data = 0.9987670666694999)), Neuron(w = [Value(data = -0.38132655034565244), Value(data = -0.9114798796520234), Value(data = -0.7094968795486423)], b = Value(data = -0.7697832946963472))])\n",
      "layer: 2, neuron_cnt: 1, layer: Layer(neurons = [Neuron(w = [Value(data = 0.16889631217099077), Value(data = 0.052013705317604186), Value(data = -0.547170655423358)], b = Value(data = -0.9145694660035044))])\n"
     ]
    }
   ],
   "source": [
    "# if verbose:\n",
    "if True:\n",
    "\t# print weights and bias of each layer\n",
    "\tprint(f\"neurons' weights and bias of each layer\")\n",
    "\tfor i, layer in enumerate(n.layers):\n",
    "\t\tneuron_cnt = len(layer.neurons)  # numbers of neurons in the layer \n",
    "\t\tprint(f'layer: {i}, neuron_cnt: {neuron_cnt}, layer: {layer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;\n",
    "# - Calculate Neural Network Outputs and Loss (i.e. Prediction Errors) -\n",
    "##### * transpose inputs<br>* select activation function<br>* calculate outputs, (a.k.a) Forward Pass<br>* calculate Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transpose Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs_mats[0].shape: (2, 2)\n",
      "xs_mats:\n",
      "[array([[ 2.,  1.],\n",
      "       [ 3., -2.]])]\n",
      "\n",
      "xs_mats_T[0].shape: (2, 2)\n",
      "xs_mats_T:\n",
      "[array([[ 2.,  3.],\n",
      "       [ 1., -2.]])]\n"
     ]
    }
   ],
   "source": [
    "xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n",
    "xs_mats_T = []\n",
    "for mat in xs_mats:\n",
    "  mat_transpose = np.transpose(mat)\n",
    "  xs_mats_T.append(mat_transpose)\n",
    "\n",
    "print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n",
    "print(f'xs_mats:\\n{xs_mats}\\n')\n",
    "print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n",
    "print(f'xs_mats_T:\\n{xs_mats_T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Common Activation Functions\n",
    "x = np.arange(-5, 5, 0.2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "ax1.plot(x, np.tanh(x))\n",
    "ax1.set_title(\"Tanh\")\n",
    "ax1.grid()\n",
    "\n",
    "a = x[x < 0]*0.01\n",
    "b = x[x >= 0]\n",
    "y = np.concatenate((a, b))\n",
    "ax2.grid()\n",
    "ax2.plot(x, y)\n",
    "ax2.set_title(\"Leaky ReLU\")\n",
    "\n",
    "plt_title = \"Common Activation Functions\\nIntroduce non-linearity into the network\\nto learn complex relationships\"\n",
    "plt.suptitle(plt_title , fontsize=12, y=1.2)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Outputs and Loss Using Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 0\n",
      "weights (3, 2):\n",
      "[[ 0.88990369  0.65812651]\n",
      " [ 0.17141806  0.82556148]\n",
      " [-0.51792219  0.61879989]]\n",
      "\n",
      "input (2, 2):\n",
      "[[ 2.  3.]\n",
      " [ 1. -2.]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 2.43793388  1.35345805]\n",
      " [ 1.1683976  -1.13686877]\n",
      " [-0.41704449 -2.79136636]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[-0.22406162]\n",
      " [-0.15983168]\n",
      " [-0.47463461]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 2.21387227  1.12939644]\n",
      " [ 1.00856592 -1.29670045]\n",
      " [-0.8916791  -3.26600097]]\n",
      "\n",
      "Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.97639904  0.81081259]\n",
      " [ 0.76516822 -0.86087133]\n",
      " [-0.71222208 -0.99709206]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 1\n",
      "weights (3, 3):\n",
      "[[ 0.6127556   0.75539608 -0.45727274]\n",
      " [ 0.76708564 -0.55741784 -0.22133874]\n",
      " [-0.38132655 -0.91147988 -0.70949688]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.97639904  0.81081259]\n",
      " [ 0.76516822 -0.86087133]\n",
      " [-0.71222208 -0.99709206]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 1.5019788   0.30247415]\n",
      " [ 0.48010561  1.32252283]\n",
      " [-0.56444297  1.18291623]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[ 0.57236108]\n",
      " [ 0.99876707]\n",
      " [-0.76978329]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 2.07433987  0.87483523]\n",
      " [ 1.47887268  2.3212899 ]\n",
      " [-1.33422627  0.41313293]]\n",
      "\n",
      "Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.96892012  0.70382246]\n",
      " [ 0.90125656  0.98091818]\n",
      " [-0.87027847  0.39112958]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 2\n",
      "weights (1, 3):\n",
      "[[ 0.16889631  0.05201371 -0.54717066]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.96892012  0.70382246]\n",
      " [ 0.90125656  0.98091818]\n",
      " [-0.87027847  0.39112958]]\n",
      "\n",
      "weights_x_inputs (1, 2):\n",
      "[[ 0.68671557 -0.04412042]]\n",
      "\n",
      "bias (1, 1):\n",
      "[[-0.91456947]]\n",
      "\n",
      "weights_x_inputs_+_bias (1, 2):\n",
      "[[-0.2278539  -0.95868988]]\n",
      "\n",
      "Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n",
      "[[-0.22399091 -0.74369192]]\n",
      "\n",
      "-- Results of neural network outputs and Loss --\n",
      "yout:           [-0.22399091 -0.74369192]\n",
      "desired output: [1.0, -1.0]\n",
      "err:            [-1.22399091  0.25630808]\n",
      "err_sq:         [1.49815374 0.06569383]\n",
      "loss_sum:       1.5638475722954768\n",
      "loss_mean:      0.7819237861477384\n"
     ]
    }
   ],
   "source": [
    "yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Neural network outputs and Loss --\n",
      "yout:           [-0.22399091 -0.74369192] <-- neural network output\n",
      "desired output: [1.0, -1.0] <-- target\n",
      "err:            [-1.22399091  0.25630808] <-- prediction_error\n",
      "err_sq:         [1.49815374 0.06569383] <-- prediction_error^2\n",
      "loss_sum:       1.5638475722954768 <-- sum(prediction_error)^2\n",
      "loss_mean:      0.7819237861477384 <-- mean(prediction_error)^2\n"
     ]
    }
   ],
   "source": [
    "print(f'-- Neural network outputs and Loss --')\n",
    "print(f'yout:           {yout} <-- neural network output')   \n",
    "print(f'desired output: {ys} <-- target')   \n",
    "print(f'err:            {err} <-- prediction_error')\n",
    "print(f'err_sq:         {err_sq} <-- prediction_error^2')\n",
    "print(f'loss_sum:       {loss_sum} <-- sum(prediction_error)^2')\n",
    "print(f'loss_mean:      {loss_mean} <-- mean(prediction_error)^2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original parameters\n",
    "param_org = [p.data for p in n.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   &nbsp;\n",
    "# - How Artificial Neural Network Learns -\n",
    "\n",
    "##### * calculate gradients (i.e. changes in Loss w.r.t. changes in each parameter)<br>* use gradients to adjust parameters in direction of less Loss<br>* repeat the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual gradient calculation for parameter W0<br>* calculate outputs and Loss<br>* increase W0 by small amount, e.g. 0.00001<br>* recalculate outputs and Loss<br>* calculate gradient (W0_grad = changes_in_Loss / changes_in_W0)\n",
    "##### Increase W0 by small amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_mean before increase Wo:   0.7819238\n",
      "W0_before:                      0.8899037\n",
      "W0_after:                       0.8899047\n",
      "W0_dif:                         0.0000010 <-- increased W0 by a small amount\n"
     ]
    }
   ],
   "source": [
    "# Increase W0 by h\n",
    "h = .000001\n",
    "loss_mean_before = loss_mean\n",
    "print(f'loss_mean before increase Wo:  {loss_mean_before:10.7f}')\n",
    "W0_before = n.parameters()[0].data  # W1\n",
    "print(f'W0_before:                     {W0_before:10.7f}')\n",
    "n.parameters()[0].data += h\n",
    "W0_after = n.parameters()[0].data\n",
    "print(f'W0_after:                      {W0_after:10.7f}') \n",
    "W0_dif = W0_after - W0_before\n",
    "print(f'W0_dif:                        {W0_dif:10.7f} <-- increased W0 by a small amount') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recalculate output and Loss with small changes in W0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 0\n",
      "weights (3, 2):\n",
      "[[ 0.88990469  0.65812651]\n",
      " [ 0.17141806  0.82556148]\n",
      " [-0.51792219  0.61879989]]\n",
      "\n",
      "input (2, 2):\n",
      "[[ 2.  3.]\n",
      " [ 1. -2.]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 2.43793588  1.35346105]\n",
      " [ 1.1683976  -1.13686877]\n",
      " [-0.41704449 -2.79136636]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[-0.22406162]\n",
      " [-0.15983168]\n",
      " [-0.47463461]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 2.21387427  1.12939944]\n",
      " [ 1.00856592 -1.29670045]\n",
      " [-0.8916791  -3.26600097]]\n",
      "\n",
      "Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.97639914  0.81081362]\n",
      " [ 0.76516822 -0.86087133]\n",
      " [-0.71222208 -0.99709206]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 1\n",
      "weights (3, 3):\n",
      "[[ 0.6127556   0.75539608 -0.45727274]\n",
      " [ 0.76708564 -0.55741784 -0.22133874]\n",
      " [-0.38132655 -0.91147988 -0.70949688]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.97639914  0.81081362]\n",
      " [ 0.76516822 -0.86087133]\n",
      " [-0.71222208 -0.99709206]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 1.50197885  0.30247478]\n",
      " [ 0.48010568  1.32252362]\n",
      " [-0.56444301  1.18291584]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[ 0.57236108]\n",
      " [ 0.99876707]\n",
      " [-0.76978329]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 2.07433993  0.87483586]\n",
      " [ 1.47887275  2.32129069]\n",
      " [-1.3342263   0.41313254]]\n",
      "\n",
      "Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.96892012  0.70382278]\n",
      " [ 0.90125658  0.98091821]\n",
      " [-0.87027848  0.39112924]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 2\n",
      "weights (1, 3):\n",
      "[[ 0.16889631  0.05201371 -0.54717066]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.96892012  0.70382278]\n",
      " [ 0.90125658  0.98091821]\n",
      " [-0.87027848  0.39112924]]\n",
      "\n",
      "weights_x_inputs (1, 2):\n",
      "[[ 0.68671557 -0.04412018]]\n",
      "\n",
      "bias (1, 1):\n",
      "[[-0.91456947]]\n",
      "\n",
      "weights_x_inputs_+_bias (1, 2):\n",
      "[[-0.22785389 -0.95868965]]\n",
      "\n",
      "Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n",
      "[[-0.2239909  -0.74369181]]\n",
      "\n",
      "-- Results of neural network outputs and Loss --\n",
      "yout:           [-0.2239909  -0.74369181]\n",
      "desired output: [1.0, -1.0]\n",
      "err:            [-1.2239909   0.25630819]\n",
      "err_sq:         [1.49815373 0.06569389]\n",
      "loss_sum:       1.5638476125807594\n",
      "loss_mean:      0.7819238062903797\n"
     ]
    }
   ],
   "source": [
    "yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Calculate outputs and changes in Loss --\n",
      "yout:              [-0.2239909  -0.74369181]\n",
      "desired output:    [1.0, -1.0]\n",
      "err:               [-1.2239909   0.25630819]\n",
      "err_sq:            [1.49815373 0.06569389]\n",
      "loss_mean_before:  0.7819237861477384\n",
      "loss_mean_after:   0.7819238062903797\n",
      "\n",
      "-- Calcuclate gradient --\n",
      "loss_mean_dif:     2.014264133265442e-08 <-- change in loss_mean\n",
      "W0_dif:            1.0000000000287557e-06 <-- change in W0\n",
      "W0_grad:           0.020142641332075207 <-- (changes in loss_mean) / (changes in W0), manual calculation\n"
     ]
    }
   ],
   "source": [
    "loss_mean_after = loss_mean\n",
    "loss_mean_dif = loss_mean_after - loss_mean_before\n",
    "W0_grad = loss_mean_dif / W0_dif\n",
    "\n",
    "print(f'-- Calculate outputs and changes in Loss --')\n",
    "\n",
    "print(f'yout:              {yout}')   \n",
    "print(f'desired output:    {ys}')   \n",
    "print(f'err:               {err}')\n",
    "print(f'err_sq:            {err_sq}')\n",
    "print(f'loss_mean_before:  {loss_mean_before}')\n",
    "print(f'loss_mean_after:   {loss_mean_after}\\n')\n",
    "print(f'-- Calcuclate gradient --')\n",
    "print(f'loss_mean_dif:     {loss_mean_dif} <-- change in loss_mean')\n",
    "print(f'W0_dif:            {W0_dif} <-- change in W0')\n",
    "print(f'W0_grad:           {W0_grad} <-- (changes in loss_mean) / (changes in W0), manual calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate output and Loss with Micrograd<br>* change W0 back to initial value<br>* compare manual calculation vs Micrograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Calculate neural network Loss and gradient using Micrograd --\n",
      "W0:          0.8899036882690474\n",
      "ypred_data:  [-0.2239909065709496, -0.743691917944241]\n",
      "ys:          [1.0, -1.0]\n",
      "err_sq:      [1.49815373 0.06569389]\n",
      "loss_mean:   0.7819237861477384 <-- loss_mean, Micrograd calculation same as manual calc. 0.7819237861477384\n"
     ]
    }
   ],
   "source": [
    "# change W0 back before Micrograd calculation\n",
    "n.parameters()[0].data = W0_before\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred_data = [v.data for v in ypred]  # extract data \n",
    "loss_mean = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) / len(ys) # low loss is better, perfect is loss = 0\n",
    "\n",
    "print(f'-- Calculate neural network Loss and gradient using Micrograd --')\n",
    "print(f'W0:          {n.parameters()[0].data}')\n",
    "print(f'ypred_data:  {ypred_data}')\n",
    "print(f'ys:          {ys}')\n",
    "print(f'err_sq:      {err_sq}')\n",
    "print(f'loss_mean:   {loss_mean.data} <-- loss_mean, Micrograd calculation same as manual calc. {loss_mean_before}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate gradients and adjust parameters using Micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- adjust parameters,  parameter_after = parameter_before - gradient * learning_rate --\n",
      "  i  parameter before         gradient     learning rate      parameter after\n",
      "  0      0.8899036883     0.0201426827           0.05000         0.8888965541 <-- gradient same as manual calc. W0_grad  0.0201426413\n",
      "  1      0.6581265054    -0.0215829387           0.05000         0.6592056523\n",
      "  2     -0.2240616161     0.0055493013           0.05000        -0.2243390812\n",
      "  3      0.1714180630    -0.0756863160           0.05000         0.1752023788\n",
      "  4      0.8255614769    -0.0882611305           0.05000         0.8299745335\n",
      "  5     -0.1598316802    -0.0450457255           0.05000        -0.1575793939\n",
      "  6     -0.5179221907    -0.0994416314           0.05000        -0.5129501092\n",
      "  7      0.6187998930    -0.0503946761           0.05000         0.6213196268\n",
      "  8     -0.4746346124    -0.0498170815           0.05000        -0.4721437583\n",
      "  9      0.6127555993    -0.0038160619           0.05000         0.6129464024\n",
      " 10      0.7553960788    -0.0175989046           0.05000         0.7562760240\n",
      " 11     -0.4572727386    -0.0011769007           0.05000        -0.4572138936\n",
      " 12      0.5723610764    -0.0022525757           0.05000         0.5724737052\n",
      " 13      0.7670856435    -0.0109019227           0.05000         0.7676307396\n",
      " 14     -0.5574178381    -0.0088804227           0.05000        -0.5569738170\n",
      " 15     -0.2213387400     0.0078609115           0.05000        -0.2217317856\n",
      " 16      0.9987670667    -0.0111272437           0.05000         0.9993234289\n",
      " 17     -0.3813265503     0.1076467803           0.05000        -0.3867088894\n",
      " 18     -0.9114798797     0.1637955426           0.05000        -0.9196696568\n",
      " 19     -0.7094968795    -0.0569855320           0.05000        -0.7066476029\n",
      " 20     -0.7697832947     0.1012453457           0.05000        -0.7748455620\n",
      " 21      0.1688963122    -1.0458253196           0.05000         0.2211875782\n",
      " 22      0.0520137053    -0.9354197019           0.05000         0.0987846904\n",
      " 23     -0.5471706554     1.0565729650           0.05000        -0.5999993037\n",
      " 24     -0.9145694660    -1.0480311196           0.05000        -0.8621679100\n"
     ]
    }
   ],
   "source": [
    "# backward pass to calculate gradients\n",
    "for p in n.parameters():\n",
    "  p.grad = 0.0  # zero gradients before calculating gradient wth backward pass \n",
    "loss_mean.backward()\n",
    "\n",
    "# update weights and bias\n",
    "print('-- adjust parameters,  parameter_after = parameter_before - gradient * learning_rate --')\n",
    "print(f'  i  parameter before         gradient     learning rate      parameter after')\n",
    "for i, p in enumerate(n.parameters()):\n",
    "  p_before = p.data\n",
    "  p.data += -learning_rate * p.grad\n",
    "\n",
    "  if i == 0:  \n",
    "    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f} <-- gradient same as manual calc. W0_grad {W0_grad:13.10f}')\n",
    "  else:\n",
    "    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the data and gradients, a bit confusing to visualized \n",
    "# draw_dot(loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repeat the steps using Micrograd:<br>* calculate Loss<br>* calculate gradient<br>* adjust parameters in direction of less Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ypred: [Value(data = -0.03253384302854122), Value(data = -0.6865849300962256)]\n",
      "step: 0, loss_mean: 0.5821775715210379\n",
      "ypred: [Value(data = 0.13301517737724486), Value(data = -0.6355928129941629)]\n",
      "step: 1, loss_mean: 0.4422276402998586\n",
      "ypred: [Value(data = 0.25616594051450936), Value(data = -0.6007579237108636)]\n",
      "step: 2, loss_mean: 0.3563416717651625\n",
      "ypred: [Value(data = 0.34313578593006655), Value(data = -0.5833122154608329)]\n",
      "step: 3, loss_mean: 0.30254965275493534\n",
      "ypred: [Value(data = 0.4056126107007105), Value(data = -0.5791056984566505)]\n",
      "step: 4, loss_mean: 0.26522419081484455\n",
      "ypred: [Value(data = 0.4525076913272891), Value(data = -0.5833422251928353)]\n",
      "step: 5, loss_mean: 0.23667576468151647\n",
      "ypred: [Value(data = 0.48938796501643955), Value(data = -0.5923971135105178)]\n",
      "step: 6, loss_mean: 0.21343238167230522\n",
      "ypred: [Value(data = 0.5195779521032095), Value(data = -0.6039293587302463)]\n",
      "step: 7, loss_mean: 0.19383864849059002\n",
      "ypred: [Value(data = 0.5450820396295581), Value(data = -0.6165356073121875)]\n",
      "step: 8, loss_mean: 0.17699764556351788\n",
      "ypred: [Value(data = 0.5671474457024993), Value(data = -0.6294104452698978)]\n",
      "step: 9, loss_mean: 0.16234897591846312\n",
      "ypred: [Value(data = 0.5865822960997173), Value(data = -0.6421082371206754)]\n",
      "step: 10, loss_mean: 0.14950035591752622\n",
      "ypred: [Value(data = 0.6039331635472305), Value(data = -0.6543940156165221)]\n",
      "step: 11, loss_mean: 0.13815621768968883\n",
      "ypred: [Value(data = 0.619585429928268), Value(data = -0.6661543470671317)]\n",
      "step: 12, loss_mean: 0.1280840825525169\n",
      "ypred: [Value(data = 0.6338213889450376), Value(data = -0.6773450354709947)]\n",
      "step: 13, loss_mean: 0.11909650066467756\n",
      "ypred: [Value(data = 0.6468548819791825), Value(data = -0.6879606358703393)]\n",
      "step: 14, loss_mean: 0.11104001957419005\n",
      "ypred: [Value(data = 0.6588526137470833), Value(data = -0.698016710843026)]\n",
      "step: 15, loss_mean: 0.10378772303863068\n",
      "ypred: [Value(data = 0.6699477087668557), Value(data = -0.7075395184634811)]\n",
      "step: 16, loss_mean: 0.09723382410441041\n",
      "ypred: [Value(data = 0.6802486200890022), Value(data = -0.7165600479888475)]\n",
      "step: 17, loss_mean: 0.09128957567553582\n",
      "ypred: [Value(data = 0.689845172657403), Value(data = -0.7251106117226132)]\n",
      "step: 18, loss_mean: 0.08588009635571606\n",
      "ypred: [Value(data = 0.6988127881593038), Value(data = -0.7332229464389088)]\n",
      "step: 19, loss_mean: 0.08094186644155485\n",
      "ypred: [Value(data = 0.707215522213836), Value(data = -0.7409272082186431)]\n",
      "step: 20, loss_mean: 0.07642073093695154\n",
      "ypred: [Value(data = 0.7151083040093946), Value(data = -0.7482514949781705)]\n",
      "step: 21, loss_mean: 0.07227029411256483\n",
      "ypred: [Value(data = 0.7225386259028819), Value(data = -0.755221677949365)]\n",
      "step: 22, loss_mean: 0.06845062053089265\n",
      "ypred: [Value(data = 0.7295478440300005), Value(data = -0.7618614104620982)]\n",
      "step: 23, loss_mean: 0.0649271782479611\n",
      "ypred: [Value(data = 0.7361721973159546), Value(data = -0.7681922344830079)]\n",
      "step: 24, loss_mean: 0.06166997481153619\n",
      "ypred: [Value(data = 0.7424436182032331), Value(data = -0.7742337368198349)]\n",
      "step: 25, loss_mean: 0.05865284769728876\n",
      "ypred: [Value(data = 0.7483903862882143), Value(data = -0.7800037260495506)]\n",
      "step: 26, loss_mean: 0.0558528791321376\n",
      "ypred: [Value(data = 0.7540376613762362), Value(data = -0.7855184129714521)]\n",
      "step: 27, loss_mean: 0.05324991159777782\n",
      "ypred: [Value(data = 0.7594079225158994), Value(data = -0.7907925846315246)]\n",
      "step: 28, loss_mean: 0.05082614519663664\n",
      "ypred: [Value(data = 0.7645213326956105), Value(data = -0.7958397664538246)]\n",
      "step: 29, loss_mean: 0.04856580185854012\n",
      "ypred: [Value(data = 0.7693960440394192), Value(data = -0.8006723698012563)]\n",
      "step: 30, loss_mean: 0.0464548443326583\n",
      "ypred: [Value(data = 0.7740484548627711), Value(data = -0.8053018240131673)]\n",
      "step: 31, loss_mean: 0.04448074024125044\n",
      "ypred: [Value(data = 0.7784934274121011), Value(data = -0.8097386930242169)]\n",
      "step: 32, loss_mean: 0.04263226331588564\n",
      "ypred: [Value(data = 0.7827444732312887), Value(data = -0.8139927773013602)]\n",
      "step: 33, loss_mean: 0.0408993254038058\n",
      "ypred: [Value(data = 0.7868139116821755), Value(data = -0.8180732021953043)]\n",
      "step: 34, loss_mean: 0.03927283400586294\n",
      "ypred: [Value(data = 0.7907130060653912), Value(data = -0.821988493985283)]\n",
      "step: 35, loss_mean: 0.03774457105190632\n",
      "ypred: [Value(data = 0.7944520809513972), Value(data = -0.8257466449627434)]\n",
      "step: 36, loss_mean: 0.036307089383475585\n",
      "ypred: [Value(data = 0.7980406236765409), Value(data = -0.8293551688906502)]\n",
      "step: 37, loss_mean: 0.034953624034649536\n",
      "ypred: [Value(data = 0.8014873724398308), Value(data = -0.8328211481173265)]\n",
      "step: 38, loss_mean: 0.03367801590882565\n",
      "ypred: [Value(data = 0.8048003930205104), Value(data = -0.8361512735354942)]\n",
      "step: 39, loss_mean: 0.03247464586449381\n",
      "ypred: [Value(data = 0.807987145800563), Value(data = -0.8393518784712914)]\n",
      "step: 40, loss_mean: 0.03133837756425849\n",
      "ypred: [Value(data = 0.8110545445024225), Value(data = -0.8424289674738072)]\n",
      "step: 41, loss_mean: 0.03026450772227876\n",
      "ypred: [Value(data = 0.8140090078273057), Value(data = -0.8453882408596619)]\n",
      "step: 42, loss_mean: 0.029248722616926577\n",
      "ypred: [Value(data = 0.8168565049937409), Value(data = -0.8482351157541183)]\n",
      "step: 43, loss_mean: 0.028287059926636778\n",
      "ypred: [Value(data = 0.8196025960214969), Value(data = -0.85097474426338)]\n",
      "step: 44, loss_mean: 0.027375875104774114\n",
      "ypred: [Value(data = 0.8222524674772711), Value(data = -0.8536120293142426)]\n",
      "step: 45, loss_mean: 0.026511811639706367\n",
      "ypred: [Value(data = 0.8248109642906021), Value(data = -0.8561516386083492)]\n",
      "step: 46, loss_mean: 0.025691774653925838\n",
      "ypred: [Value(data = 0.8272826181578306), Value(data = -0.8585980170594929)]\n",
      "step: 47, loss_mean: 0.02491290738496061\n",
      "ypred: [Value(data = 0.8296716729756209), Value(data = -0.8609553980137263)]\n",
      "step: 48, loss_mean: 0.024172570164222536\n",
      "ypred: [Value(data = 0.831982107681182), Value(data = -0.8632278134930959)]\n",
      "step: 49, loss_mean: 0.02346832157056865\n",
      "ypred: [Value(data = 0.8342176568219318), Value(data = -0.8654191036539874)]\n",
      "step: 50, loss_mean: 0.02279790148545347\n",
      "ypred: [Value(data = 0.8363818291313515), Value(data = -0.8675329256095351)]\n",
      "step: 51, loss_mean: 0.02215921581798561\n",
      "ypred: [Value(data = 0.8384779243487923), Value(data = -0.8695727617314121)]\n",
      "step: 52, loss_mean: 0.021550322702522735\n",
      "ypred: [Value(data = 0.8405090484879782), Value(data = -0.8715419275186651)]\n",
      "step: 53, loss_mean: 0.02096941999991498\n",
      "ypred: [Value(data = 0.8424781277308967), Value(data = -0.8734435790991804)]\n",
      "step: 54, loss_mean: 0.020414833957194557\n",
      "ypred: [Value(data = 0.8443879210999473), Value(data = -0.8752807204120059)]\n",
      "step: 55, loss_mean: 0.019885008900272234\n",
      "ypred: [Value(data = 0.84624103204096), Value(data = -0.8770562101053437)]\n",
      "step: 56, loss_mean: 0.019378497850745233\n",
      "ypred: [Value(data = 0.8480399190324114), Value(data = -0.8787727681748427)]\n",
      "step: 57, loss_mean: 0.018893953971833254\n",
      "ypred: [Value(data = 0.8497869053214546), Value(data = -0.880432982359265)]\n",
      "step: 58, loss_mean: 0.01843012276020273\n",
      "ypred: [Value(data = 0.8514841878747743), Value(data = -0.8820393143051497)]\n",
      "step: 59, loss_mean: 0.017985834910407297\n",
      "ypred: [Value(data = 0.8531338456215046), Value(data = -0.8835941055082649)]\n",
      "step: 60, loss_mean: 0.0175599997871745\n",
      "ypred: [Value(data = 0.8547378470561917), Value(data = -0.8850995830371164)]\n",
      "step: 61, loss_mean: 0.017151599448057425\n",
      "ypred: [Value(data = 0.8562980572618287), Value(data = -0.8865578650421851)]\n",
      "step: 62, loss_mean: 0.01675968316525587\n",
      "ypred: [Value(data = 0.8578162444061217), Value(data = -0.8879709660537057)]\n",
      "step: 63, loss_mean: 0.016383362400859837\n",
      "ypred: [Value(data = 0.8592940857582196), Value(data = -0.8893408020704486)]\n",
      "step: 64, loss_mean: 0.01602180619451344\n",
      "ypred: [Value(data = 0.8607331732679918), Value(data = -0.8906691954419665)]\n",
      "step: 65, loss_mean: 0.01567423692665505\n",
      "ypred: [Value(data = 0.8621350187454637), Value(data = -0.8919578795470112)]\n",
      "step: 66, loss_mean: 0.015339926424145887\n",
      "ypred: [Value(data = 0.8635010586741173), Value(data = -0.8932085032712006)]\n",
      "step: 67, loss_mean: 0.01501819237833197\n",
      "ypred: [Value(data = 0.8648326586883531), Value(data = -0.8944226352874798)]\n",
      "step: 68, loss_mean: 0.014708395048449875\n",
      "ypred: [Value(data = 0.8661311177424057), Value(data = -0.8956017681433839)]\n",
      "step: 69, loss_mean: 0.014409934225842712\n",
      "ypred: [Value(data = 0.8673976719953684), Value(data = -0.8967473221595866)]\n",
      "step: 70, loss_mean: 0.014122246436732044\n",
      "ypred: [Value(data = 0.8686334984346553), Value(data = -0.897860649144633)]\n",
      "step: 71, loss_mean: 0.013844802363336729\n",
      "ypred: [Value(data = 0.8698397182581513), Value(data = -0.8989430359311416)]\n",
      "step: 72, loss_mean: 0.013577104464965985\n",
      "ypred: [Value(data = 0.8710174000334778), Value(data = -0.8999957077390638)]\n",
      "step: 73, loss_mean: 0.013318684782367316\n",
      "ypred: [Value(data = 0.8721675626511523), Value(data = -0.9010198313718529)]\n",
      "step: 74, loss_mean: 0.013069102910101757\n",
      "ypred: [Value(data = 0.8732911780869649), Value(data = -0.9020165182515705)]\n",
      "step: 75, loss_mean: 0.012827944123067035\n",
      "ypred: [Value(data = 0.8743891739875779), Value(data = -0.9029868272990993)]\n",
      "step: 76, loss_mean: 0.012594817644508885\n",
      "ypred: [Value(data = 0.8754624360921771), Value(data = -0.9039317676656907)]\n",
      "step: 77, loss_mean: 0.01236935504396695\n",
      "ypred: [Value(data = 0.8765118105019393), Value(data = -0.9048523013221078)]\n",
      "step: 78, loss_mean: 0.012151208754603963\n",
      "ypred: [Value(data = 0.8775381058081209), Value(data = -0.9057493455115916)]\n",
      "step: 79, loss_mean: 0.011940050700278171\n",
      "ypred: [Value(data = 0.8785420950887077), Value(data = -0.9066237750728171)]\n",
      "step: 80, loss_mean: 0.011735571023546183\n",
      "ypred: [Value(data = 0.8795245177827679), Value(data = -0.9074764246389116)]\n",
      "step: 81, loss_mean: 0.011537476906536807\n",
      "ypred: [Value(data = 0.8804860814509465), Value(data = -0.9083080907184755)]\n",
      "step: 82, loss_mean: 0.01134549147732055\n",
      "ypred: [Value(data = 0.8814274634298784), Value(data = -0.9091195336643988)]\n",
      "step: 83, loss_mean: 0.01115935279502458\n",
      "ypred: [Value(data = 0.8823493123877053), Value(data = -0.9099114795361032)]\n",
      "step: 84, loss_mean: 0.010978812907509853\n",
      "ypred: [Value(data = 0.8832522497873385), Value(data = -0.9106846218606488)]\n",
      "step: 85, loss_mean: 0.010803636975946647\n",
      "ypred: [Value(data = 0.8841368712636162), Value(data = -0.9114396232979581)]\n",
      "step: 86, loss_mean: 0.010633602461095706\n",
      "ypred: [Value(data = 0.8850037479200398), Value(data = -0.9121771172152071)]\n",
      "step: 87, loss_mean: 0.010468498366534615\n",
      "ypred: [Value(data = 0.8858534275503671), Value(data = -0.912897709175229)]\n",
      "step: 88, loss_mean: 0.010308124534461133\n",
      "ypred: [Value(data = 0.8866864357899481), Value(data = -0.9136019783435654)]\n",
      "step: 89, loss_mean: 0.010152290990065649\n",
      "ypred: [Value(data = 0.8875032772013435), Value(data = -0.9142904788185993)]\n",
      "step: 90, loss_mean: 0.010000817330791374\n",
      "ypred: [Value(data = 0.8883044362984365), Value(data = -0.914963740888991)]\n",
      "step: 91, loss_mean: 0.009853532157102347\n",
      "ypred: [Value(data = 0.889090378512953), Value(data = -0.915622272222441)]\n",
      "step: 92, loss_mean: 0.009710272541651944\n",
      "ypred: [Value(data = 0.8898615511070304), Value(data = -0.9162665589895986)]\n",
      "step: 93, loss_mean: 0.009570883533995826\n",
      "ypred: [Value(data = 0.8906183840352175), Value(data = -0.9168970669267534)]\n",
      "step: 94, loss_mean: 0.009435217698221827\n",
      "ypred: [Value(data = 0.8913612907590601), Value(data = -0.9175142423407413)]\n",
      "step: 95, loss_mean: 0.009303134681079715\n",
      "ypred: [Value(data = 0.8920906690172007), Value(data = -0.918118513059333)]\n",
      "step: 96, loss_mean: 0.009174500808384976\n",
      "ypred: [Value(data = 0.8928069015537325), Value(data = -0.9187102893301872)]\n",
      "step: 97, loss_mean: 0.009049188707646535\n",
      "ypred: [Value(data = 0.89351035680735), Value(data = -0.9192899646712829)]\n",
      "step: 98, loss_mean: 0.008927076955030335\n",
      "ypred: [Value(data = 0.8942013895636763), Value(data = -0.9198579166755928)]\n",
      "step: 99, loss_mean: 0.0088080497449166\n",
      "ypred: [Value(data = 0.8948803415729841), Value(data = -0.9204145077725971)]\n",
      "step: 100, loss_mean: 0.008691996580445244\n",
      "ypred: [Value(data = 0.8955475421353787), Value(data = -0.9209600859490943)]\n",
      "step: 101, loss_mean: 0.008578811983567528\n",
      "ypred: [Value(data = 0.8962033086553788), Value(data = -0.9214949854316259)]\n",
      "step: 102, loss_mean: 0.008468395223235598\n",
      "ypred: [Value(data = 0.8968479471677014), Value(data = -0.9220195273327023)]\n",
      "step: 103, loss_mean: 0.008360650060466237\n",
      "ypred: [Value(data = 0.8974817528359367), Value(data = -0.922534020262884)]\n",
      "step: 104, loss_mean: 0.008255484509111621\n",
      "ypred: [Value(data = 0.8981050104256955), Value(data = -0.9230387609106626)]\n",
      "step: 105, loss_mean: 0.008152810611256892\n",
      "ypred: [Value(data = 0.8987179947537063), Value(data = -0.9235340345919654)]\n",
      "step: 106, loss_mean: 0.00805254422624651\n",
      "ypred: [Value(data = 0.8993209711142437), Value(data = -0.9240201157710063)]\n",
      "step: 107, loss_mean: 0.00795460483241512\n",
      "ypred: [Value(data = 0.8999141956841825), Value(data = -0.9244972685541014)]\n",
      "step: 108, loss_mean: 0.007858915340667797\n",
      "ypred: [Value(data = 0.9004979159078946), Value(data = -0.9249657471579755)]\n",
      "step: 109, loss_mean: 0.007765401919116633\n",
      "ypred: [Value(data = 0.9010723708631211), Value(data = -0.9254257963539949)]\n",
      "step: 110, loss_mean: 0.007673993828039846\n",
      "ypred: [Value(data = 0.9016377916088877), Value(data = -0.925877651889676)]\n",
      "step: 111, loss_mean: 0.007584623264482331\n",
      "ypred: [Value(data = 0.9021944015164634), Value(data = -0.9263215408887442)]\n",
      "step: 112, loss_mean: 0.007497225215865891\n",
      "ypred: [Value(data = 0.9027424165842995), Value(data = -0.926757682230936)]\n",
      "step: 113, loss_mean: 0.0074117373220232395\n",
      "ypred: [Value(data = 0.9032820457378297), Value(data = -0.9271862869126738)]\n",
      "step: 114, loss_mean: 0.007328099745111366\n",
      "ypred: [Value(data = 0.9038134911149582), Value(data = -0.9276075583896709)]\n",
      "step: 115, loss_mean: 0.007246255046898563\n",
      "ypred: [Value(data = 0.9043369483380131), Value(data = -0.9280216929024664)]\n",
      "step: 116, loss_mean: 0.0071661480729554115\n",
      "ypred: [Value(data = 0.9048526067728923), Value(data = -0.9284288797858269)]\n",
      "step: 117, loss_mean: 0.007087725843312744\n",
      "ypred: [Value(data = 0.9053606497760918), Value(data = -0.9288293017629002)]\n",
      "step: 118, loss_mean: 0.007010937449179937\n",
      "ypred: [Value(data = 0.90586125493026), Value(data = -0.9292231352249528)]\n",
      "step: 119, loss_mean: 0.006935733955345403\n",
      "ypred: [Value(data = 0.9063545942688821), Value(data = -0.9296105504974737)]\n",
      "step: 120, loss_mean: 0.006862068307907199\n",
      "ypred: [Value(data = 0.9068408344906712), Value(data = -0.9299917120933839)]\n",
      "step: 121, loss_mean: 0.006789895247005079\n",
      "ypred: [Value(data = 0.9073201371642003), Value(data = -0.9303667789540454)]\n",
      "step: 122, loss_mean: 0.006719171224248706\n",
      "ypred: [Value(data = 0.9077926589232829), Value(data = -0.9307359046787237)]\n",
      "step: 123, loss_mean: 0.006649854324556448\n",
      "ypred: [Value(data = 0.9082585516535786), Value(data = -0.9310992377431208)]\n",
      "step: 124, loss_mean: 0.006581904192139051\n",
      "ypred: [Value(data = 0.9087179626708758), Value(data = -0.9314569217075602)]\n",
      "step: 125, loss_mean: 0.0065152819603795775\n",
      "ypred: [Value(data = 0.9091710348914728), Value(data = -0.9318090954153698)]\n",
      "step: 126, loss_mean: 0.006449950185378097\n",
      "ypred: [Value(data = 0.9096179069950585), Value(data = -0.9321558931819816)]\n",
      "step: 127, loss_mean: 0.006385872782944295\n",
      "ypred: [Value(data = 0.9100587135804715), Value(data = -0.9324974449752298)]\n",
      "step: 128, loss_mean: 0.0063230149688358935\n",
      "ypred: [Value(data = 0.9104935853146906), Value(data = -0.9328338765873115)]\n",
      "step: 129, loss_mean: 0.006261343202053534\n",
      "ypred: [Value(data = 0.9109226490753963), Value(data = -0.9331653097988412)]\n",
      "step: 130, loss_mean: 0.006200825131014934\n",
      "ypred: [Value(data = 0.9113460280874188), Value(data = -0.9334918625354022)]\n",
      "step: 131, loss_mean: 0.006141429542443287\n",
      "ypred: [Value(data = 0.9117638420533734), Value(data = -0.9338136490169895)]\n",
      "step: 132, loss_mean: 0.006083126312814151\n",
      "ypred: [Value(data = 0.912176207278766), Value(data = -0.934130779900699)]\n",
      "step: 133, loss_mean: 0.006025886362216211\n",
      "ypred: [Value(data = 0.91258323679184), Value(data = -0.9344433624170121)]\n",
      "step: 134, loss_mean: 0.005969681610489374\n",
      "ypred: [Value(data = 0.9129850404584118), Value(data = -0.9347515004999961)]\n",
      "step: 135, loss_mean: 0.005914484935513121\n",
      "ypred: [Value(data = 0.9133817250919426), Value(data = -0.9350552949117289)]\n",
      "step: 136, loss_mean: 0.005860270133525157\n",
      "ypred: [Value(data = 0.9137733945590671), Value(data = -0.9353548433612363)]\n",
      "step: 137, loss_mean: 0.005807011881358303\n",
      "ypred: [Value(data = 0.9141601498807979), Value(data = -0.9356502406182151)]\n",
      "step: 138, loss_mean: 0.0057546857004903505\n",
      "ypred: [Value(data = 0.9145420893296069), Value(data = -0.9359415786218)]\n",
      "step: 139, loss_mean: 0.005703267922807955\n",
      "ypred: [Value(data = 0.914919308522578), Value(data = -0.93622894658462)]\n",
      "step: 140, loss_mean: 0.005652735657991763\n",
      "ypred: [Value(data = 0.9152919005108126), Value(data = -0.9365124310923733)]\n",
      "step: 141, loss_mean: 0.005603066762435367\n",
      "ypred: [Value(data = 0.9156599558652617), Value(data = -0.9367921161991408)]\n",
      "step: 142, loss_mean: 0.0055542398096162585\n",
      "ypred: [Value(data = 0.9160235627591494), Value(data = -0.9370680835186441)]\n",
      "step: 143, loss_mean: 0.0055062340618414335\n",
      "ypred: [Value(data = 0.9163828070471403), Value(data = -0.9373404123116428)]\n",
      "step: 144, loss_mean: 0.0054590294432953525\n",
      "ypred: [Value(data = 0.9167377723414037), Value(data = -0.9376091795696597)]\n",
      "step: 145, loss_mean: 0.005412606514321438\n",
      "ypred: [Value(data = 0.9170885400847107), Value(data = -0.9378744600952041)]\n",
      "step: 146, loss_mean: 0.005366946446873499\n",
      "ypred: [Value(data = 0.9174351896206974), Value(data = -0.9381363265786644)]\n",
      "step: 147, loss_mean: 0.0053220310010759315\n",
      "ypred: [Value(data = 0.9177777982614239), Value(data = -0.9383948496720229)]\n",
      "step: 148, loss_mean: 0.00527784250283588\n",
      "ypred: [Value(data = 0.9181164413523453), Value(data = -0.9386500980595469)]\n",
      "step: 149, loss_mean: 0.005234363822453561\n",
      "ypred: [Value(data = 0.9184511923348104), Value(data = -0.9389021385255916)]\n",
      "step: 150, loss_mean: 0.005191578354180042\n",
      "ypred: [Value(data = 0.9187821228061976), Value(data = -0.9391510360196543)]\n",
      "step: 151, loss_mean: 0.005149469996674483\n",
      "ypred: [Value(data = 0.9191093025777879), Value(data = -0.9393968537188033)]\n",
      "step: 152, loss_mean: 0.005108023134315997\n",
      "ypred: [Value(data = 0.9194327997304766), Value(data = -0.9396396530876049)]\n",
      "step: 153, loss_mean: 0.005067222619327085\n",
      "ypred: [Value(data = 0.9197526806684144), Value(data = -0.939879493935659)]\n",
      "step: 154, loss_mean: 0.005027053754668969\n",
      "ypred: [Value(data = 0.9200690101706673), Value(data = -0.9401164344728573)]\n",
      "step: 155, loss_mean: 0.004987502277670237\n",
      "ypred: [Value(data = 0.9203818514409802), Value(data = -0.9403505313624626)]\n",
      "step: 156, loss_mean: 0.004948554344353355\n",
      "ypred: [Value(data = 0.9206912661557234), Value(data = -0.9405818397721089)]\n",
      "step: 157, loss_mean: 0.004910196514424826\n",
      "ypred: [Value(data = 0.9209973145100998), Value(data = -0.9408104134228161)]\n",
      "step: 158, loss_mean: 0.004872415736897015\n",
      "ypred: [Value(data = 0.9213000552626854), Value(data = -0.941036304636106)]\n",
      "step: 159, loss_mean: 0.004835199336311231\n",
      "ypred: [Value(data = 0.9215995457783709), Value(data = -0.9412595643793054)]\n",
      "step: 160, loss_mean: 0.004798534999533367\n",
      "ypred: [Value(data = 0.9218958420697727), Value(data = -0.9414802423091136)]\n",
      "step: 161, loss_mean: 0.004762410763094969\n",
      "ypred: [Value(data = 0.9221889988371763), Value(data = -0.9416983868135144)]\n",
      "step: 162, loss_mean: 0.004726815001053772\n",
      "ypred: [Value(data = 0.9224790695070699), Value(data = -0.941914045052099)]\n",
      "step: 163, loss_mean: 0.004691736413349642\n",
      "ypred: [Value(data = 0.9227661062693286), Value(data = -0.9421272629948753)]\n",
      "step: 164, loss_mean: 0.004657164014632485\n",
      "ypred: [Value(data = 0.9230501601131014), Value(data = -0.9423380854596217)]\n",
      "step: 165, loss_mean: 0.0046230871235406115\n",
      "ypred: [Value(data = 0.9233312808614551), Value(data = -0.9425465561478553)]\n",
      "step: 166, loss_mean: 0.004589495352408311\n",
      "ypred: [Value(data = 0.9236095172048236), Value(data = -0.9427527176794653)]\n",
      "step: 167, loss_mean: 0.004556378597383568\n",
      "ypred: [Value(data = 0.9238849167333119), Value(data = -0.9429566116260768)]\n",
      "step: 168, loss_mean: 0.004523727028936553\n",
      "ypred: [Value(data = 0.9241575259678968), Value(data = -0.9431582785431923)]\n",
      "step: 169, loss_mean: 0.004491531082741779\n",
      "ypred: [Value(data = 0.9244273903905728), Value(data = -0.943357758001163)]\n",
      "step: 170, loss_mean: 0.004459781450916857\n",
      "ypred: [Value(data = 0.9246945544734799), Value(data = -0.9435550886150433)]\n",
      "step: 171, loss_mean: 0.0044284690736016465\n",
      "ypred: [Value(data = 0.9249590617070558), Value(data = -0.9437503080733678)]\n",
      "step: 172, loss_mean: 0.004397585130863248\n",
      "ypred: [Value(data = 0.9252209546272493), Value(data = -0.9439434531659032)]\n",
      "step: 173, loss_mean: 0.004367121034911597\n",
      "ypred: [Value(data = 0.9254802748418316), Value(data = -0.9441345598104083)]\n",
      "step: 174, loss_mean: 0.004337068422612903\n",
      "ypred: [Value(data = 0.9257370630558394), Value(data = -0.9443236630784514)]\n",
      "step: 175, loss_mean: 0.004307419148287088\n",
      "ypred: [Value(data = 0.9259913590961839), Value(data = -0.9445107972203177)]\n",
      "step: 176, loss_mean: 0.00427816527677735\n",
      "ypred: [Value(data = 0.9262432019354568), Value(data = -0.9446959956890456)]\n",
      "step: 177, loss_mean: 0.004249299076779929\n",
      "ypred: [Value(data = 0.9264926297149639), Value(data = -0.9448792911636256)]\n",
      "step: 178, loss_mean: 0.004220813014422885\n",
      "ypred: [Value(data = 0.926739679767015), Value(data = -0.9450607155713985)]\n",
      "step: 179, loss_mean: 0.004192699747083145\n",
      "ypred: [Value(data = 0.9269843886364983), Value(data = -0.9452403001096796)]\n",
      "step: 180, loss_mean: 0.004164952117431932\n",
      "ypred: [Value(data = 0.9272267921017658), Value(data = -0.9454180752666465)]\n",
      "step: 181, loss_mean: 0.004137563147698541\n",
      "ypred: [Value(data = 0.9274669251948556), Value(data = -0.9455940708415114)]\n",
      "step: 182, loss_mean: 0.004110526034143577\n",
      "ypred: [Value(data = 0.9277048222210739), Value(data = -0.9457683159640129)]\n",
      "step: 183, loss_mean: 0.004083834141732837\n",
      "ypred: [Value(data = 0.9279405167779627), Value(data = -0.9459408391132508)]\n",
      "step: 184, loss_mean: 0.004057480999003253\n",
      "ypred: [Value(data = 0.928174041773674), Value(data = -0.9461116681358884)]\n",
      "step: 185, loss_mean: 0.004031460293113274\n",
      "ypred: [Value(data = 0.9284054294447718), Value(data = -0.9462808302637494)]\n",
      "step: 186, loss_mean: 0.004005765865069825\n",
      "ypred: [Value(data = 0.9286347113734832), Value(data = -0.9464483521308307)]\n",
      "step: 187, loss_mean: 0.003980391705124779\n",
      "ypred: [Value(data = 0.928861918504419), Value(data = -0.9466142597897538)]\n",
      "step: 188, loss_mean: 0.003955331948333911\n",
      "ypred: [Value(data = 0.9290870811607811), Value(data = -0.9467785787276779)]\n",
      "step: 189, loss_mean: 0.003930580870271813\n",
      "ypred: [Value(data = 0.9293102290600761), Value(data = -0.9469413338816913)]\n",
      "step: 190, loss_mean: 0.0039061328828965333\n",
      "ypred: [Value(data = 0.929531391329353), Value(data = -0.9471025496537041)]\n",
      "step: 191, loss_mean: 0.0038819825305578103\n",
      "ypred: [Value(data = 0.9297505965199798), Value(data = -0.9472622499248606)]\n",
      "step: 192, loss_mean: 0.0038581244861432672\n",
      "ypred: [Value(data = 0.9299678726219782), Value(data = -0.9474204580694876)]\n",
      "step: 193, loss_mean: 0.0038345535473569893\n",
      "ypred: [Value(data = 0.9301832470779283), Value(data = -0.9475771969685969)]\n",
      "step: 194, loss_mean: 0.003811264633125446\n",
      "ypred: [Value(data = 0.9303967467964613), Value(data = -0.9477324890229574)]\n",
      "step: 195, loss_mean: 0.0037882527801255937\n",
      "ypred: [Value(data = 0.9306083981653522), Value(data = -0.9478863561657564)]\n",
      "step: 196, loss_mean: 0.0037655131394303413\n",
      "ypred: [Value(data = 0.9308182270642278), Value(data = -0.9480388198748583)]\n",
      "step: 197, loss_mean: 0.0037430409732670837\n",
      "ypred: [Value(data = 0.9310262588769019), Value(data = -0.9481899011846827)]\n",
      "step: 198, loss_mean: 0.0037208316518845454\n",
      "ypred: [Value(data = 0.9312325185033523), Value(data = -0.9483396206977103)]\n",
      "step: 199, loss_mean: 0.003698880650524113\n"
     ]
    }
   ],
   "source": [
    "# Create a list of losses\n",
    "losses_mean = []\n",
    "for k in range(200):\n",
    "  # forward pass\n",
    "  ypred = [n(x) for x in xs]\n",
    "  # loss is mean-square-errors\n",
    "  loss_mean = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) / len(ys) # low loss is better, perfect is loss = 0  \n",
    "  losses_mean.append(loss_mean.data)\n",
    "\n",
    "  # backward pass to calculate gradients\n",
    "  for p in n.parameters():\n",
    "    p.grad = 0.0  # zero the gradient \n",
    "  loss_mean.backward()\n",
    "\n",
    "  # update weights and bias\n",
    "  for p in n.parameters():\n",
    "      p.data += -learning_rate * p.grad\n",
    "\n",
    "  # print(f'x: {x}')\n",
    "  print(f'ypred: {ypred}')\n",
    "  print(f'step: {k}, loss_mean: {loss_mean.data}')   \n",
    "  # print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(losses_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate outputs and Loss after last parameter adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Calculate outputs and Loss at end of iteration --\n",
      "ypred_data: [0.9314370303713482, -0.9484879985956327]\n",
      "loss_mean: 0.0036771835464914854\n",
      "\n",
      "\n",
      "Calculate outputs and Loss using optimized parameters\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 0\n",
      "weights (3, 2):\n",
      "[[ 0.55670425  0.89480168]\n",
      " [ 0.126938    0.98963617]\n",
      " [-0.41348608  0.67338988]]\n",
      "\n",
      "input (2, 2):\n",
      "[[ 2.  3.]\n",
      " [ 1. -2.]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 2.00821017 -0.11949062]\n",
      " [ 1.24351216 -1.59845835]\n",
      " [-0.15358227 -2.587238  ]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[-0.33305064]\n",
      " [-0.15545532]\n",
      " [-0.42207771]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 1.67515953 -0.45254126]\n",
      " [ 1.08805684 -1.75391367]\n",
      " [-0.57565998 -3.00931571]]\n",
      "\n",
      "Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.93223056 -0.42398568]\n",
      " [ 0.79616782 -0.94181932]\n",
      " [-0.51950386 -0.99514582]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 1\n",
      "weights (3, 3):\n",
      "[[ 0.59280343  0.90243007 -0.32975482]\n",
      " [ 0.79248286 -0.52955821 -0.23154205]\n",
      " [-0.40760176 -1.19930781 -0.82507095]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.93223056 -0.42398568]\n",
      " [ 0.79616782 -0.94181932]\n",
      " [-0.51950386 -0.99514582]]\n",
      "\n",
      "weights_x_inputs (3, 2):\n",
      "[[ 1.44242416 -0.77311212]\n",
      " [ 0.43744652  0.39316488]\n",
      " [-0.90620156  2.12341448]]\n",
      "\n",
      "bias (3, 1):\n",
      "[[ 0.45256173]\n",
      " [ 1.01927489]\n",
      " [-0.7031533 ]]\n",
      "\n",
      "weights_x_inputs_+_bias (3, 2):\n",
      "[[ 1.8949859  -0.32055038]\n",
      " [ 1.45672141  1.41243976]\n",
      " [-1.60935486  1.42026117]]\n",
      "\n",
      "Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n",
      "[[ 0.95580614 -0.3100045 ]\n",
      " [ 0.89701395  0.88801111]\n",
      " [-0.92306464  0.8896534 ]]\n",
      "\n",
      "--------------------------------------------------\n",
      "Calculate Output of Layer: 2\n",
      "weights (1, 3):\n",
      "[[ 0.69355389  0.31127747 -1.43702454]]\n",
      "\n",
      "input (3, 2):\n",
      "[[ 0.95580614 -0.3100045 ]\n",
      " [ 0.89701395  0.88801111]\n",
      " [-0.92306464  0.8896534 ]]\n",
      "\n",
      "weights_x_inputs (1, 2):\n",
      "[[ 2.26858983 -1.21704074]]\n",
      "\n",
      "bias (1, 1):\n",
      "[[-0.59945633]]\n",
      "\n",
      "weights_x_inputs_+_bias (1, 2):\n",
      "[[ 1.6691335  -1.81649707]]\n",
      "\n",
      "Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n",
      "[[ 0.93143703 -0.948488  ]]\n",
      "\n",
      "-- Results of neural network outputs and Loss --\n",
      "yout:           [ 0.93143703 -0.948488  ]\n",
      "desired output: [1.0, -1.0]\n",
      "err:            [-0.06856297  0.051512  ]\n",
      "err_sq:         [0.00470088 0.00265349]\n",
      "loss_sum:       0.007354367092982971\n",
      "loss_mean:      0.0036771835464914854\n"
     ]
    }
   ],
   "source": [
    "# calculate outputs and Loss after last parameter adjustment\n",
    "ypred = [n(x) for x in xs]\n",
    "loss_mean = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) / len(ys) # low loss is better, perfect is loss = 0\n",
    "ypred_data = [value.data for value in ypred]\n",
    "\n",
    "print(f'\\n-- Calculate outputs and Loss at end of iteration --')\n",
    "print(f'ypred_data: {ypred_data}')\n",
    "print(f'loss_mean: {loss_mean.data}\\n\\n')\n",
    "\n",
    "# check calculation using forward pass function\n",
    "print(f'Calculate outputs and Loss using optimized parameters')\n",
    "yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare changes in parameters after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  param. optimized  param. orginal     param. dif\n",
      "0      0.5567042460    0.8899036883  -0.3331994422\n",
      "1      0.8948016781    0.6581265054   0.2366751727\n",
      "2     -0.3330506381   -0.2240616161  -0.1089890220\n",
      "3      0.1269379970    0.1714180630  -0.0444800660\n",
      "4      0.9896361680    0.8255614769   0.1640746910\n",
      "5     -0.1554553240   -0.1598316802   0.0043763561\n",
      "6     -0.4134860784   -0.5179221907   0.1044361123\n",
      "7      0.6733898828    0.6187998930   0.0545899898\n",
      "8     -0.4220777086   -0.4746346124   0.0525569038\n",
      "9      0.5928034339    0.6127555993  -0.0199521654\n",
      "10     0.9024300697    0.7553960788   0.1470339909\n",
      "11    -0.3297548183   -0.4572727386   0.1275179203\n",
      "12     0.4525617332    0.5723610764  -0.1197993432\n",
      "13     0.7924828553    0.7670856435   0.0253972118\n",
      "14    -0.5295582100   -0.5574178381   0.0278596281\n",
      "15    -0.2315420520   -0.2213387400  -0.0102033120\n",
      "16     1.0192748864    0.9987670667   0.0205078197\n",
      "17    -0.4076017557   -0.3813265503  -0.0262752054\n",
      "18    -1.1993078093   -0.9114798797  -0.2878279297\n",
      "19    -0.8250709451   -0.7094968795  -0.1155740656\n",
      "20    -0.7031533037   -0.7697832947   0.0666299910\n",
      "21     0.6935538865    0.1688963122   0.5246575744\n",
      "22     0.3112774681    0.0520137053   0.2592637628\n",
      "23    -1.4370245433   -0.5471706554  -0.8898538879\n",
      "24    -0.5994563343   -0.9145694660   0.3151131317\n"
     ]
    }
   ],
   "source": [
    "# save parameters\n",
    "param_optmz = [p.data for p in n.parameters()]\n",
    "param_comp = zip(param_org, param_optmz)\n",
    "\n",
    "# print(f'i        param_new      param_org      param_dif')\n",
    "print(f'i  param. optimized  param. orginal     param. dif')\n",
    "for i, param_org_new in enumerate(param_comp):\n",
    "  p_org = param_org_new[0]\n",
    "  p_new = param_org_new[1]  \n",
    "  p_dif = p_new - p_org\n",
    "  print(f'{i:<4} {p_new:>14.10f}  {p_org:>14.10f} {p_dif:>14.10f}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   &nbsp;\n",
    "# - Build the Same Model Using PyTorch -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[-0.0766],\n",
      "        [-0.5617]])\n",
      "Epoch 0 loss: 0.6755706667900085\n",
      "Predictions:\n",
      "tensor([[ 0.6780],\n",
      "        [-0.7916]])\n",
      "Epoch 10 loss: 0.07357970625162125\n",
      "Predictions:\n",
      "tensor([[ 0.9280],\n",
      "        [-0.9514]])\n",
      "Epoch 20 loss: 0.0037751966156065464\n",
      "Predictions:\n",
      "tensor([[ 0.9852],\n",
      "        [-0.9901]])\n",
      "Epoch 30 loss: 0.0001578655792400241\n",
      "Predictions:\n",
      "tensor([[ 0.9970],\n",
      "        [-0.9980]])\n",
      "Epoch 40 loss: 6.338427283480996e-06\n",
      "Predictions:\n",
      "tensor([[ 0.9994],\n",
      "        [-0.9996]])\n",
      "Epoch 50 loss: 2.5251816282434447e-07\n",
      "Predictions:\n",
      "tensor([[ 0.9999],\n",
      "        [-0.9999]])\n",
      "Epoch 60 loss: 1.004434047047198e-08\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 70 loss: 3.972964179865812e-10\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 80 loss: 1.6274981362585095e-11\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 90 loss: 7.283063041541027e-13\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 100 loss: 1.3145040611561853e-13\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 110 loss: 1.2789769243681803e-13\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 120 loss: 1.2789769243681803e-13\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 130 loss: 9.237055564881302e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 140 loss: 8.881784197001252e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 150 loss: 8.881784197001252e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 160 loss: 8.881784197001252e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 170 loss: 8.881784197001252e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 180 loss: 8.881784197001252e-14\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Epoch 190 loss: 8.881784197001252e-14\n",
      "\n",
      "Predictions:\n",
      "tensor([[ 1.0000],\n",
      "        [-1.0000]])\n",
      "Loss: 8.881784197001252e-14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP_torch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_torch, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)  \n",
    "        return x\n",
    "\n",
    "model = MLP_torch()\n",
    "\n",
    "# # inputs\n",
    "# xs = [\n",
    "#   [2.0, 3.0, -1.0],\n",
    "#   [3.0, -1.0, 0.5]\n",
    "# ]\n",
    "\n",
    "# # desired targets\n",
    "# ys = [1.0, -1.0]\n",
    "\n",
    "# convert to tensor\n",
    "t_xs = torch.tensor(xs)\n",
    "\n",
    "# add a dimension to the index=1 position to target tensor,\n",
    "#  e.g. change size from [2] to [2, 1]\n",
    "t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n",
    "\n",
    "# # learning rate (i.e. step size)\n",
    "# learning_rate = 0.05\n",
    "\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    # forward pass\n",
    "    predictions = model(t_xs)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.nn.functional.mse_loss(predictions, t_ys)\n",
    "\n",
    "    # remove loss gradient \n",
    "    losses.append(loss.detach())\n",
    "\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad.data\n",
    "\n",
    "    # zero gradients\n",
    "    for p in model.parameters():\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Predictions:\\n{predictions.detach()}\")\n",
    "        print(f\"Epoch {epoch} loss: {loss}\")\n",
    "\n",
    "predictions = model(t_xs)\n",
    "print('')\n",
    "print(f\"Predictions:\\n{predictions.detach()}\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input xs:\n",
      "[[2.0, 1.0], [3.0, -2.0]]\n",
      "\n",
      "target ys:\n",
      "[1.0, -1.0]\n",
      "---------\n",
      "\n",
      "-- manual calculation of output for each layer from the optimized pytorch model --\n",
      "layer: 0.0,  i: 0\n",
      "\n",
      "w,  torch.Size([3, 2]):\n",
      "tensor([[-0.5055,  0.1866],\n",
      "        [ 0.1096, -0.6706],\n",
      "        [-0.1131, -0.7871]])\n",
      "\n",
      "input,  torch.Size([2, 2]):\n",
      "tensor([[ 2.,  3.],\n",
      "        [ 1., -2.]])\n",
      "\n",
      "w * input,  torch.Size([3, 2]):\n",
      "tensor([[-0.8244, -1.8897],\n",
      "        [-0.4514,  1.6702],\n",
      "        [-1.0132,  1.2350]])\n",
      "\n",
      "bT,  torch.Size([3, 1]):\n",
      "tensor([[-0.0177],\n",
      "        [-0.6092],\n",
      "        [ 0.1815]])\n",
      "\n",
      "w * input + bT,  torch.Size([3, 2]):\n",
      "tensor([[-0.8421, -1.9074],\n",
      "        [-1.0606,  1.0610],\n",
      "        [-0.8317,  1.4164]])\n",
      "\n",
      "output,  torch.Size([3, 2]):\n",
      "tensor([[-0.6869, -0.9569],\n",
      "        [-0.7859,  0.7860],\n",
      "        [-0.6814,  0.8889]])\n",
      "\n",
      "\n",
      "layer: 1.0,  i: 2\n",
      "\n",
      "w,  torch.Size([3, 3]):\n",
      "tensor([[-0.3651, -0.5669, -0.3400],\n",
      "        [-0.3089,  0.5902,  0.5870],\n",
      "        [-0.0203, -0.4551,  0.3922]])\n",
      "\n",
      "input,  torch.Size([3, 2]):\n",
      "tensor([[-0.6869, -0.9569],\n",
      "        [-0.7859,  0.7860],\n",
      "        [-0.6814,  0.8889]])\n",
      "\n",
      "w * input,  torch.Size([3, 2]):\n",
      "tensor([[ 0.9281, -0.3985],\n",
      "        [-0.6516,  1.2812],\n",
      "        [ 0.1044,  0.0103]])\n",
      "\n",
      "bT,  torch.Size([3, 1]):\n",
      "tensor([[-0.3045],\n",
      "        [-0.4407],\n",
      "        [ 0.0540]])\n",
      "\n",
      "w * input + bT,  torch.Size([3, 2]):\n",
      "tensor([[ 0.6235, -0.7030],\n",
      "        [-1.0923,  0.8405],\n",
      "        [ 0.1584,  0.0643]])\n",
      "\n",
      "output,  torch.Size([3, 2]):\n",
      "tensor([[ 0.5536, -0.6063],\n",
      "        [-0.7977,  0.6861],\n",
      "        [ 0.1571,  0.0642]])\n",
      "\n",
      "\n",
      "layer: 2.0,  i: 4\n",
      "\n",
      "w,  torch.Size([1, 3]):\n",
      "tensor([[ 0.7918, -0.7268,  0.0347]])\n",
      "\n",
      "input,  torch.Size([3, 2]):\n",
      "tensor([[ 0.5536, -0.6063],\n",
      "        [-0.7977,  0.6861],\n",
      "        [ 0.1571,  0.0642]])\n",
      "\n",
      "w * input,  torch.Size([1, 2]):\n",
      "tensor([[ 1.0236, -0.9764]])\n",
      "\n",
      "bT,  torch.Size([1, 1]):\n",
      "tensor([[-0.0236]])\n",
      "\n",
      "w * input + bT,  torch.Size([1, 2]):\n",
      "tensor([[ 1.0000, -1.0000]])\n",
      "\n",
      "output,  torch.Size([1, 2]):\n",
      "tensor([[ 1.0000, -1.0000]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'input xs:\\n{xs}\\n')\n",
    "print(f'target ys:\\n{ys}')\n",
    "print('---------\\n')\n",
    "\n",
    "\n",
    "print(f'-- manual calculation of output for each layer from the optimized pytorch model --')\n",
    "# l_items is a list of [weight matrix, bias matrix, ..., weight matrix, bias matrix] \n",
    "l_items = list(model.parameters())\n",
    "if len(l_items) % 2 == 0:  # True divisible by 2\n",
    "  for i in range(0, len(l_items), 2):  # i: 0, 2, ..., len(l_items)-2\n",
    "    if i == 0:  # use transposed t_xs as input only at the first time\n",
    "      x0 = torch.clone(t_xs).detach()  # clone t_xs without autograd history\n",
    "      input = torch.transpose(x0, 0, 1)  # columns of x0 becomes rows of input\n",
    "    else:  # use previous output as input \n",
    "      input = output\n",
    "\n",
    "    w = l_items[i].detach()  # remove gradient\n",
    "    b_ = l_items[i + 1].detach()  # remove gradient\n",
    "    b = torch.clone(b_).detach()  # remove gradient\n",
    "    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n",
    "    w_input = torch.matmul(w, input)\n",
    "    w_input_bT = torch.add(w_input, bT)\n",
    "\n",
    "    if i == len(l_items) - 2:  # skip tanh activation on output node\n",
    "      output = w_input_bT\n",
    "    else:  \n",
    "      output = torch.tanh(w_input_bT)      \n",
    "\n",
    "    print(f'layer: {i / 2},  i: {i}\\n')\n",
    "    print(f'w,  {w.shape}:\\n{w}\\n')\n",
    "    print(f'input,  {input.shape}:\\n{input}\\n')\n",
    "    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n",
    "    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n",
    "    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n",
    "    print(f'output,  {output.shape}:\\n{output}\\n')            \n",
    "    print('')\n",
    "else:\n",
    "  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   &nbsp;\n",
    "# - Additional Information from PyTorch Model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_torch\n",
      "Linear\n",
      "Linear\n",
      "Linear\n"
     ]
    }
   ],
   "source": [
    "for module in model.modules():\n",
    "  classname = module.__class__.__name__\n",
    "  print(classname)\n",
    "  # if 'Linear' in classname:\n",
    "  #   # module = nn.Sequential(...)\n",
    "  #   print(classname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      2\u001b[0m summary(model, t_xs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, t_xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: fc1.weight\n",
      "param: tensor([[-0.5151, -0.4704],\n",
      "        [-0.1427,  0.5018],\n",
      "        [-0.0669, -0.8121]])\n",
      "weight matrix\n",
      "\n",
      "\n",
      "name: fc1.bias\n",
      "param: tensor([ 0.0193,  0.5063, -0.2212])\n",
      "bias matrix\n",
      "\n",
      "\n",
      "name: fc2.weight\n",
      "param: tensor([[ 0.1870,  0.1734,  0.3976],\n",
      "        [-0.1671,  0.6909, -0.6446],\n",
      "        [ 0.0162, -0.1791, -0.4243]])\n",
      "weight matrix\n",
      "\n",
      "\n",
      "name: fc2.bias\n",
      "param: tensor([-0.3684, -0.2267,  0.1334])\n",
      "bias matrix\n",
      "\n",
      "\n",
      "name: fc3.weight\n",
      "param: tensor([[-0.3295,  1.1515,  0.2406]])\n",
      "weight matrix\n",
      "\n",
      "\n",
      "name: fc3.bias\n",
      "param: tensor([-0.1078])\n",
      "bias matrix\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  param = param.detach()  # remove grad\n",
    "  print(f'name: {name}')\n",
    "  print(f'param: {param}')\n",
    "\n",
    "  if name.find('weight') !=-1: # found\n",
    "    print(f'weight matrix')\n",
    "  elif name.find('bias') !=-1: # found\n",
    "    print(f'bias matrix')\n",
    "  else:\n",
    "    raise ValueError(f\"parameter name does not have 'weight' or 'bias' in the name\")\n",
    " \n",
    "  print('\\n')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
