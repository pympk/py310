{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# import openai\n",
    "# from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.indexes import VectorstoreIndexCreator\n",
    "# from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_secret_key'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py3810.myUtils import pickle_dump, pickle_load\n",
    "path_lumen_docs = '..\\langchain\\docs\\lumen\\\\docs\\\\'\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('.env\\my_api_key.env')) # read local .env file\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"SECRET_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very long string that needs to be wrapped to fit within 80 columns. It\n",
      "can contain spaces, punctuation, and even newlines.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, width=80):\n",
    "  \"\"\"\n",
    "  Prints a long string to the console, wrapped to fit within a specified width.\n",
    "\n",
    "  Args:\n",
    "      text: The long string to be wrapped.\n",
    "      width: The desired width for each line (default: 80 columns).\n",
    "  \"\"\"\n",
    "  wrapped_text = textwrap.wrap(text, width=width)\n",
    "  for line in wrapped_text:\n",
    "    print(line)\n",
    "\n",
    "# Example usage\n",
    "long_string = \"This is a very long string that needs to be wrapped to fit within 80 columns. It can contain spaces, punctuation, and even newlines.\"\n",
    "print_wrapped(long_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"Where is Lumen Optometric?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a technique where complex tasks are broken down into\n",
      "smaller and simpler steps to enhance model performance. This process involves\n",
      "transforming big tasks into manageable tasks through steps like Chain of Thought\n",
      "or Tree of Thoughts. Task decomposition can be achieved through simple\n",
      "prompting, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is Task Decomposition?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
