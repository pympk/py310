{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafb515b",
   "metadata": {},
   "source": [
    "### Read Excel file, extract the data from column A into a list, and treat first line as a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from py3810.myUtils import pickle_dump, pickle_load\n",
    "\n",
    "path_lumen_docs = '..\\langchain\\docs\\lumen\\docs\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40edc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_chars(items, chars):\n",
    "  for i, item in enumerate(items):\n",
    "    for char in chars:\n",
    "      item = item.rstrip(char)\n",
    "    items[i] = item\n",
    "\n",
    "  return items\n",
    "\n",
    "# Example usage\n",
    "chars = [\".\", \",\", \":\", \"'\"]\n",
    "items = ['apple.', 'banana:', 'cherry\"', 'a.apple', '.apple']\n",
    "print(remove_trailing_chars(items, chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0635649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for sublist in nested_list:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_item_on_chars(items, chars):\n",
    "  rtn_list = []\n",
    "  for i, item in enumerate(items):\n",
    "    for char in chars:\n",
    "      if char in item:\n",
    "        my_list = item.split(char)\n",
    "        my_list[0] = my_list[0] + char\n",
    "        rtn_list.append(my_list)\n",
    "\n",
    "  rtn_list = flatten_list(rtn_list)        \n",
    "\n",
    "  return rtn_list\n",
    "\n",
    "# Example usage\n",
    "chars = [\".\", \",\", \":\", '\"']\n",
    "items = ['apple.app', 'banana:ban', 'cherry\"che', 'a.apple', '.apple']\n",
    "\n",
    "print(f'chars: {chars}')\n",
    "print(f'items: {items}')\n",
    "print(f'rtn_list: {separate_item_on_chars(items, chars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\ping\\AppData\\Roaming\\nltk_data\\corpora\\words\\en\n",
    "import nltk\n",
    "# nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "def find_misspelled_words(text):\n",
    "  my_words = set(text.split())\n",
    "  english_words = set(words.words())\n",
    "  return [word for word in my_words if word.lower() not in english_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70f8ad",
   "metadata": {},
   "source": [
    "#### Process/Clean documents downloaded using LangChain WebBaseLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aace1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document, BaseDocumentTransformer\n",
    "from typing import Any, Sequence\n",
    "import re\n",
    "\n",
    "class PreprocessTransformer(BaseDocumentTransformer):\n",
    "    def transform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "        for document in documents:\n",
    "            # Access the page_content field\n",
    "            content = document.page_content\n",
    "\n",
    "            # Apply your preprocessing steps here\n",
    "            # # For example, convert the content to lowercase\n",
    "            # document.page_content = content.lower()\n",
    "\n",
    "            # document.page_content = re.sub(r'\\n+', ' ', document.page_content)  # replace \\n\\n\\n..\\n with ' '\n",
    "            document.page_content = re.sub(r'\\n+', '\\n', document.page_content)  # replace \\n\\n\\n..\\n with \\n\n",
    "            document.page_content = re.sub(r'\\xa0', ' ', document.page_content)# replace '\\xa0' with ' '\n",
    "            # document.page_content = re.sub(r'\\n', ' ', document.page_content)  # replace newline with a space  \n",
    "            document.page_content = re.sub(r' +(?=\\s)', '', document.page_content)  # replace consecutive spaces with single space \n",
    "            document.page_content = re.sub(r'\\n+', '\\n', document.page_content)  # replace \\n\\n\\n..\\n with \\n\n",
    "            document.page_content = document.page_content.strip()  # replace leading and trailing spaces\n",
    "\n",
    "        return documents\n",
    "\n",
    "    async def atransform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "        # Implement the asynchronous version of the method\n",
    "        return self.transform_documents(documents, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccea282",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle_load(filename_pickle='lumen_docs_raw_videos', path_pickle_dump=path_lumen_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the path to the directory containing the Excel file\n",
    "path_lumen_docs = \"../langchain/docs/lumen/docs/\"\n",
    "excel_filename = \"lumen_sitemapXML.xlsx\"\n",
    "\n",
    "# Define a helper function to flatten a nested list\n",
    "def fatten_list(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "# Construct the file path of the Excel file\n",
    "file = path_lumen_docs + excel_filename\n",
    "\n",
    "# Extract the data from Excel file column A, first line as a header\n",
    "df = pd.read_excel(file, usecols='A', header=0)\n",
    "\n",
    "# Convert the data frame to a list\n",
    "column_A = df.values.tolist()\n",
    "\n",
    "# Create a list of unique URLs by flattening the list and removing duplicates\n",
    "lumen_urls = sorted(fatten_list(column_A))\n",
    "\n",
    "# Print the length of the lumen_urls list\n",
    "print(f'len(lumen_urls): {len(lumen_urls)}')\n",
    "\n",
    "# Print the lumen_urls list\n",
    "print(lumen_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(lumen_urls)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1decc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(file_to_pickle=data, filename_pickle='lumen_website_docs_raw', path_pickle_dump=path_lumen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635250c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw = pickle_load(filename_pickle='lumen_website_docs_raw', path_pickle_dump=path_lumen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PreprocessTransformer()\n",
    "# documents = [...]  # assume you have a list of Document objects\n",
    "lumen_website_docs_processed = transformer.transform_documents(data_raw)\n",
    "pickle_dump(file_to_pickle=lumen_website_docs_processed, filename_pickle='lumen_website_docs_processed', path_pickle_dump=path_lumen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumen_website_docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed435f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the pages into a single string\n",
    "str_text = ''\n",
    "for page in data:\n",
    "  str_text += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493eb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(f'len(str_text) before replacement: {len(str_text)}')\n",
    "\n",
    "# str_text = re.sub(r'\\n+', ' ', str_text)  # replace \\n\\n\\n..\\n with ' '\n",
    "str_text = re.sub(r'\\n+', '\\n', str_text)  # replace \\n\\n\\n..\\n with \\n\n",
    "str_text = re.sub(r'\\xa0', ' ', str_text)# replace '\\xa0' with ' '\n",
    "# str_text = re.sub(r'\\n', ' ', str_text)  # replace newline with a space  \n",
    "str_text = re.sub(r' +(?=\\s)', '', str_text)  # replace consecutive spaces with single space \n",
    "str_text = re.sub(r'\\n+', '\\n', str_text)  # replace \\n\\n\\n..\\n with \\n\n",
    "str_text = str_text.strip()  # replace leading and trailing spaces\n",
    "\n",
    "print(f'len(str_text) after  replacement: {len(str_text)}')\n",
    "\n",
    "# # Optionally, you can also remove any stopwords or punctuation\n",
    "# from nltk.corpus import stopwords\n",
    "# str_text = ' '.join([word for word in str_text.split() if word.lower() not in stopwords.words('english')])\n",
    "# str_text = str_text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a36954",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_misspelled_words = find_misspelled_words(str_text)\n",
    "print(f'l_misspelled_words, len={len(l_misspelled_words)}:\\n{l_misspelled_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [\".\", \",\", \":\", \"'\", '\"', '‚Äù', '?', '(', ')']\n",
    "l_misspelled_words = remove_trailing_chars(l_misspelled_words, chars)\n",
    "print(f'l_misspelled_words, len={len(l_misspelled_words)}:\\n{l_misspelled_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_misspelled_words = \" \".join(l_misspelled_words)  # turn it into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_misspelled_words = find_misspelled_words(str_misspelled_words)\n",
    "print(f'l_misspelled_words, len={len(l_misspelled_words)}:\\n{l_misspelled_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_misspelled_words = \" \".join(l_misspelled_words)  # turn it into a string\n",
    "str_misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebceaf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ['apple.', 'app']\n",
    "_emty = []\n",
    "_emty.append(item for item in _)\n",
    "_emty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_char = '?'\n",
    "my_list = ['Appointment?Do']\n",
    "my_list[0].split(sep_char)\n",
    "# my_list = my_list[0].split(sep_char)\n",
    "# my_list[0] = my_list[0] + sep_char\n",
    "# print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b71ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1228da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = set(text.split())\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb56d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da89d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_w = []\n",
    "for word in my_words:\n",
    "  if word.lower() not in english_words:\n",
    "    misspelled_w.append(word)\n",
    "\n",
    "print(f'len(misspelled_w): {len(misspelled_w)}')                        \n",
    "# print(f'len(misspelled_w): {len(misspelled_w)}')\n",
    "misspelled_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a97172",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'By'\n",
    "english_words = set(words.words('en'))\n",
    "if word.lower() not in english_words:\n",
    "  print(f'{word} NOT in english_words')\n",
    "else:\n",
    "  print(f'{word} IN english_words')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(misspelled_words): {len(misspelled_words)}')\n",
    "misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57048fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled_words = remove_trailing_char(misspelled_words, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bde4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_items(items):\n",
    "  return \" \".join(items)\n",
    "\n",
    "# Example usage\n",
    "items = [\"hello\", \"world\"]\n",
    "print(combine_items(items))  # Output: \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_misspelled_words = combine_items(misspelled_words)\n",
    "misspelled_words = find_misspelled_words(str_misspelled_words)\n",
    "print(f'len(misspelled_words): {len(misspelled_words)}')\n",
    "misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = set(text.split())\n",
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(text) before removing HTML tangs: {len(text)}')\n",
    "text = re.sub(r'\\n', ' ', text)  # replace \\n\\n\\n..\\n with \\n\n",
    "print(f'len(text) after  removing HTML tangs: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"  hello\\nworld  \"\n",
    "text = re.sub(r'\\n(?=\\w)', ' ', text).strip()\n",
    "print(text)  # Output: \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# The (?=\\w) pattern is a positive lookahead assertion that matches a position in the string\n",
    "# where the next character is a word character.\n",
    "pattern = r'\\b\\w+(?=\\s+\\d+(\\W+|$))'\n",
    "\n",
    "text = 'The quick brown fox jumps over the 12345 lazy dog.'\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches)  # Output: ['jumps', 'over', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed630c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a website\n",
    "loader = WebBaseLoader('https://example.com')\n",
    "pages = loader.load()\n",
    "\n",
    "# Concatenate all the pages into a single string\n",
    "text = ''\n",
    "for page in pages:\n",
    "    text += page.page_content\n",
    "\n",
    "# Clean the text by removing any HTML tags\n",
    "import re\n",
    "text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "# Optionally, you can also remove any stopwords or punctuation\n",
    "from nltk.corpus import stopwords\n",
    "text = ' '.join([word for word in text.split() if word.lower() not in stopwords.words('english')])\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
