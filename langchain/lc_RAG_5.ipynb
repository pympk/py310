{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.indexes import VectorstoreIndexCreator\n",
    "# from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_secret_key'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from py3810.myUtils import pickle_dump, pickle_load\n",
    "path_lumen_docs = '..\\langchain\\docs\\lumen\\\\docs\\\\'\n",
    "path_lumen_docs_chatbot = '..\\langchain\\docs\\lumen\\\\docs\\\\chatbot\\\\'\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('.env\\my_api_key.env')) # read local .env file\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"SECRET_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very long string that needs to be wrapped to fit within 80 columns. It\n",
      "can contain spaces, punctuation, and even newlines.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, width=80):\n",
    "  \"\"\"\n",
    "  Prints a long string to the console, wrapped to fit within a specified width.\n",
    "\n",
    "  Args:\n",
    "      text: The long string to be wrapped.\n",
    "      width: The desired width for each line (default: 80 columns).\n",
    "  \"\"\"\n",
    "  wrapped_text = textwrap.wrap(text, width=width)\n",
    "  for line in wrapped_text:\n",
    "    print(line)\n",
    "\n",
    "# Example usage\n",
    "long_string = \"This is a very long string that needs to be wrapped to fit within 80 columns. It can contain spaces, punctuation, and even newlines.\"\n",
    "print_wrapped(long_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lumen_docs_processed_pdfs', 'lumen_docs_processed_videos', 'lumen_docs_processed_website', 'lumen_docs_processed_youtube']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_names(directory):\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "print(get_file_names(path_lumen_docs_chatbot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle_load(filename_pickle='lumen_website_docs_processed', path_pickle_dump=path_lumen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'where is lumen optometric?'\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "collection_name = 'lumen_website_docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only run cell below to add new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References Chroma db  \n",
    "https://docs.trychroma.com/guides  \n",
    "https://github.com/neo-con/chromadb-tutorial  \n",
    "https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # save to disk\n",
    "# db2 = Chroma.from_documents(\n",
    "#   documents=splits,\n",
    "#   embedding=embedding_function,\n",
    "#   collection_name=collection_name,\n",
    "#   persist_directory=persist_directory\n",
    "#   )\n",
    "# db2_ans = db2.similarity_search(query)\n",
    "# # print(docs[0].page_content)\n",
    "# print(f'db2_ans:{db2_ans}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "\n",
    "db3 = Chroma(\n",
    "  embedding_function=embedding_function,\n",
    "  collection_name=collection_name,\n",
    "  persist_directory=persist_directory\n",
    "  )\n",
    "db3_ans = db3.similarity_search(query)\n",
    "print(f'db3_ans:{db3_ans}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "  embedding_function=embedding_function,\n",
    "  collection_name=collection_name,\n",
    "  persist_directory=persist_directory\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load, chunk and index the contents of the blog.\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "# retriever = vectorstore.as_retriever()\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is lumen optometric's phone number?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is lumen optometric's address?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"What is lumen optometric's location?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is lumen optometric's location?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"do you take vision insurance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is lumen optometric's location?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is lumen optometric's address?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What is Ortho-k\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wrapped(rag_chain.invoke(\"What equipment do they have?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
