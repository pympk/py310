{"cells":[{"cell_type":"markdown","metadata":{"id":"8cF_oc_knFTn"},"source":["YouTube URLS to download"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":210,"status":"ok","timestamp":1724281960262,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"bPICKO6h9KU_"},"outputs":[],"source":["urls = [\"https://www.youtube.com/watch?v=w-cmMcMZoZ4\"]"]},{"cell_type":"markdown","metadata":{},"source":["Run below cell in Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26797,"status":"ok","timestamp":1724281851199,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"YqKXogB-gKlS","outputId":"76a26fcd-052c-439c-981e-eadf9d04dd28"},"outputs":[],"source":["# !pip install --quiet python-dotenv pytubefix\n","\n","# import os\n","# import sys\n","# from google.colab import drive\n","\n","# drive.mount('/content/drive')\n","\n","# path_utils =\"/content/drive/Othercomputers/My Computer/Files_win10/python/py310/\"\n","\n","# # Directory to save videos\n","# save_dir = '/content/drive/MyDrive/0_YouTube'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["current directory: c:\\Users\\ping\\Files_win10\\python\\py310\\Youtube\n","path_utils: c:\\Users\\ping\\Files_win10\\python\\py310\n","save_dir: C:\\Users\\ping\\Desktop\\youtube\n"]}],"source":["import os\n","import sys\n","\n","\n","current_dir = os.getcwd()\n","\n","# Parent directory where myUtils is located\n","path_utils = os.path.dirname(current_dir)\n","\n","# Save downloads to Desktop/youtube directory\n","save_dir = os.path.join(os.environ['USERPROFILE'], 'Desktop\\\\youtube')\n","\n","# Create save_dir if it does not exists\n","os.makedirs(save_dir, exist_ok=True)\n","\n","print(f'current directory: {current_dir}')\n","print(f'path_utils: {path_utils}')\n","print(f'save_dir: {save_dir}')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["sys.path.append(path_utils)\n","\n","from myUtils import get_file_names, find_strings_with_substring, keep_first_n_words, extract_caption_text"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1724281928503,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"GSDDID4Qqsnk","outputId":"0a374821-3d86-4ea6-80c6-85f29ca1d3ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["current directory: C:\\Users\\ping\\Desktop\\youtube\n"]}],"source":["# Change current directory to save_dir\n","os.chdir(save_dir)\n","print(f'current directory: {os.getcwd()}')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1724281934583,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"sHoukfXbobrM","outputId":"ce03b0c5-7ee1-4354-b9ac-baf197b429dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["files in current directory C:\\Users\\ping\\Desktop\\youtube:\n","Michelle_Obama_Full_Remarks.mp3\n","Michelle_Obama_Full_Remarks.mp4\n","Michelle_Obama_Full_Remarks.txt\n"]}],"source":["# Files in current directory\n","filenames = get_file_names(save_dir)\n","print(f'files in current directory {os.getcwd()}:')\n","for filename in filenames:\n","  print(filename)"]},{"cell_type":"markdown","metadata":{"id":"vV1aL6IYnT7-"},"source":["Download YouTube videos, captions, audios"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7844,"status":"ok","timestamp":1724282018333,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"5bfG4-mH7shY","outputId":"ac55fe0c-582f-46f2-8a8a-fdc348b0691c"},"outputs":[{"name":"stdout","output_type":"stream","text":["YouTube URL: https://www.youtube.com/watch?v=w-cmMcMZoZ4\n","YouTube title: AI and The Next Computing Platforms With Jensen Huang and Mark Zuckerberg\n","Filename: AI_and_The_Next\n","--------------------\n","Video saved as: AI_and_The_Next.mp4█████████████| 100.0%\n","--------------------\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\ping\\AppData\\Local\\Temp\\ipykernel_13616\\3856898318.py:25: DeprecationWarning: Call to deprecated function get_by_language_code (This object can be treated as a dictionary, i.e. captions['en']).\n","  caption = yt.captions.get_by_language_code('en')\n"]},{"name":"stdout","output_type":"stream","text":["Caption saved as: AI_and_The_Next.srt\n","--------------------\n","Audio saved as: AI_and_The_Next.mp3█████████████| 100.0%\n","========================================\n","\n"]}],"source":["from pytubefix import YouTube\n","from pytubefix.cli import on_progress\n","\n","for url in urls:\n","  # initialize filenames\n","  v_filename = \"\"\n","  c_filename = \"\"\n","  a_filename = \"\"\n","\n","  yt = YouTube(url, on_progress_callback = on_progress)\n","  filename = keep_first_n_words(yt.title, 4)\n","  print(f'YouTube URL: {url}')\n","  print(f'YouTube title: {yt.title}')\n","  print(f'Filename: {filename}')\n","  print('-'*20)\n","\n","  # download video\n","  ys = yt.streams.get_highest_resolution()\n","  v_filename = filename + '.mp4'\n","  ys.download(output_path=save_dir, filename=v_filename)\n","  print(f'Video saved as: {v_filename}')\n","  print('-'*20)\n","\n","  # download caption\n","  caption = yt.captions.get_by_language_code('en')\n","  if caption: # check if caption exists\n","    c_filename = filename + '.srt'\n","    caption.download(title=c_filename)\n","    print(f'Caption saved as: {c_filename}')\n","  else:\n","    print(f\"CAPTIONS NOT FOUND for {c_filename}\")\n","  print('-'*20)\n","\n","  # download audio\n","  ys = yt.streams.get_audio_only()\n","  a_filename = filename + '.mp3'\n","  ys.download(output_path=save_dir, filename=a_filename)\n","  print(f'Audio saved as: {a_filename}')\n","  print(f\"{'='*40}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"QAQ-2Al3_NoE"},"source":["Extracted caption text"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1724282037148,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"bTRzkVoVN0rn","outputId":"847c8bba-1851-4238-c678-d655daf292b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["srt files in current directory C:\\Users\\ping\\Desktop\\youtube:\n","['AI_and_The_Next (en).srt']\n"]}],"source":["# find srt files in current directory\n","filenames = get_file_names(save_dir)\n","srt_files = find_strings_with_substring(filenames, \"srt\", \"end\")\n","print(f'srt files in current directory {os.getcwd()}:')\n","print(srt_files)"]},{"cell_type":"markdown","metadata":{"id":"HiwxXboMjA3l"},"source":["Extract caption texts from .srt files and save as .txt files"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1384,"status":"ok","timestamp":1724282048670,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"uRDQUFI7Qz3X","outputId":"3495320f-707d-45aa-8b11-cde964164275"},"outputs":[{"name":"stdout","output_type":"stream","text":["path_srt_file: C:\\Users\\ping\\Desktop\\youtube/AI_and_The_Next (en).srt\n","caption_text: Ladies and gentlemen, I have a very special guest. But could I ask everybody to sit down? We're about to get started. My next, my next guest. I am so impressed by this person. Three reasons. First reason is there are only a handful of entrepreneurs, founders that started a company that literally touched the lives of billions of people around the world as part of the social fabric, invented services, and a state-of-the-art computing company. Two. Very few entrepreneurs, founders, founded the company and led it to over $1 trillion of value. And three, a college dropout. All three things simultaneously true. Ladies and gentlemen, please help me welcome Mark Zuckerberg. How's it going? Welcome. Mark, welcome to your first Siggraph. All right. Can you believe this? One of the pioneers of computing. A driver of modern computing. And I had to invite him to Siggraph. So, anyways, Mark, sit down. It's great to have you here. Welcome. Thanks for flying down. Yeah. No, this will be fun. I hear youâ€™ve been going for, like, five hours already or something. Well, yeah, sure. This is Siggraph. You know, there's 90% PhDs. And so the thing that's really great about Siggraph, as you know, this is this is the show of computer graphics, image processing, artificial intelligence and robotics combined. And some of the some of the companies that over the years have demonstrated and revealed amazing things here from Disney, Pixar, Adobe, Epic Games. And of course, you know, NVIDIA. We've done a lot of work here. This year we introduced 20 papers at the intersection of artificial intelligence and simulation. So we're using artificial intelligence to do, help simulation, be way larger scale, way faster. For example, differentiable physics. we're using simulation to create, simulation environments for synthetic data generation, for artificial intelligence. And so these two areas are really coming together. Weâ€™re really proud of the work that we've done here. At Meta, you guys have done amazing AI work. I mean, one of the things that, that I find amusing is, when the press writes about how Meta has jumped into AI this last couple of years, as if, you know, the work that the FAIR has done. remember, we all use PyTorch, that comes out of Meta, the work that you do in computer vision the work in language models, real-time translation. groundbreaking work. I guess my first question for you is, how do you see how the, the advances of generative AI at Meta today? And how do you apply it to either enhance your operations or introduce new capabilities that you're offering? Yeah. So a lot to unpack there. First of all, really happy to be here. you know, Meta has done a lot of work and, has been at Siggraph for, you know, eight years. So, I mean, it's a, you know, we're noobs compared to you guys. But, I know, I think it was back in in 2018. You're dressed right, but this is my hood. I just, you know, it's I mean, well, thank you for welcoming me to your hood. I think it was back in 2018. We showed the some of the early hand-tracking work for our VR and mixed reality headsets. You know, I think we've talked a bunch about the progress that we're making on codec avatars, the photorealistic avatars that we want to be able to drive from consumer headsets, which we're getting closer and closer to, so pretty excited about that. And also, a lot of the display systems work that we've done. So, some of the future prototypes and research for getting the mixed reality headsets to be able to be really thin with, like with just, pretty advanced optical stacks and display systems, the integrated system - I mean that's stuff that we've typically shown here first. So, excited to be here. You know, this year not just talking about the metaverse stuff, but also, all the AI pieces, which, as you said, I mean, we started FAIR, the AI research center. you know, back then it was Facebook. Now, Meta. Before we started Reality Labs. I mean, we've been at this for for a while. All the stuff around gen AI, it's an interesting revolution. And I think that it's going to end up making, I think all of the different products that we do, you know, different in interesting ways. I mean, I kind of go through - you can look at the big product lines that we have already, so things like the feed and recommendation systems and Instagram and Facebook and we've kind of been on this journey where that's gone from just being about connecting with your friends and, the ranking was always important because even when you were just, you know, following friends, you know, if someone did something really important, like your cousin had a baby or something, it's like, you want that at the top. You'd be pretty angry at us if we, you know, it was buried somewhere down in your feed. So the ranking was important. But now, over the last few years, it's gotten to a point where more of that stuff Is just different public content that's out there. The recommendation systems are super important because now instead of just a few hundred or thousand potential candidate posts from friends, there's millions of pieces of content and that turns into a really interesting recommendation problem. And with generative AI, I think we're going to quickly move into the zone where not only is is the majority of the content that you see today on Instagram, just recommended to you from the kind of stuff that's out there in the world that matches your interests, whether or not you follow the people. I think in the future, a lot of this stuff is going to be created with these tools, too. Some of that is going to be creators using the tools to create new content. Some of it, I think, eventually is going to be content that's either created on the fly for you, or kind of pulled together and synthesized through different things that are out there. So that that's just one example of how kind of the core part of what we're doing is just going to evolve. And it's been evolving for for 20 years already. Well very few people realize that one of the largest computing systems the world has ever conceived of is a recommender system. I mean, it's this whole yeah, it's this whole different path. Right? It's not quite the kind of gen AI hotness that people talk about, but I think it's all the transformer architectures. And it's a similar thing of just building up more and more general models Embedding, embedding unstructured data into features. Yeah. I mean, one of the big things that just drives quality improvements is, you know, it used to be that you'd have a different model for each type of content, right? So a recent example is, you know, we had, you know, one model for ranking and recommending reels and another model for ranking and recommending more long form videos. And then, you know, take some product work to basically make it so that the system can display, you know, anything in line. But, you know, the more you kind of just create more general recommendation models that can span everything, it just gets better and better. I mean, part of it, I think, is just like economics and liquidity of content and the broader of a pool that you can pull from, you're just not having these weird inefficiencies of pulling from different pools. But yeah, I mean, as the models get bigger and more general, that gets better and better. So I kind of dream of one day, you can almost imagine all of Facebook or Instagram being, you know, like a single AI model that is unified, all these different content types and systems together that actually have different objectives over different time frames. Right. Because some of it is just showing you, you know, what's the interesting content that you're going to be that, that you want to see today, but some of it is helping you build out your network over the long term. Right? People you may know or accounts you might want to follow. And these these multi-modal models tend to be, tend to be much better at recognizing patterns, weak signals and such. And so one of the things that people people, you know, it's so interesting that AI has been so deep in your company, you've been building GPU infrastructure, running these large recommender systems for a long time. Weâ€™re a little slow on it actually, getting to GPUs. Yeah, I was trying to be nice. I know. Well, you know too nice. I was trying to be nice. You know, youâ€™re my guest. When I was backstage before I came on here, you were talking about, like, owning your mistakes or something, right? So You don't have to volunteer it out of the blue. I think this one has been well tried. Yeah, it's like I got raked over the coals for it. As soon as you got into it, you got into it strong. Let's just put there you go, there you go. Now, the thing that's really cool about, about generative AI is these days when I use WhatsApp, I feel like I'm collaborating with WhatsApp. I love Imagine. I'm sitting here typing and it's generating the images as I'm going. I go back and I change my words. It's generating other images. Yeah. You know, and so the one that old Chinese guy, enjoying a glass of whiskey at sundown with three dogs: a Golden Retriever, a Goldendoodle and a Bernese Mountain dog. And it generates, you know, a pretty good-looking picture. Yeah. Yeah, we're getting there. And then now you could actually load my picture in there and itâ€™ll actually be me. Yeah. That's as of last week. Yeah. Yeah. Super excited about that. Now imagine me. Yeah. Now I'm spending a lot of time with my daughters imagining them as mermaids and things over the last, over the last week, it's been it's been a lot of fun. But yeah, I mean, that's that's the other half of it. I mean, a lot of the gen AI stuff is going to, on the one hand itâ€™s I think going to just be this big upgrade for all of the workflows and products that we've had for a long time. But on the other hand, there's going to be all these completely new things that can now get created. So Meta AI you know, the idea of having, you know, just an AI assistant that can help you with different tasks and, in our world is going to be, you know, very creatively oriented, like you're saying. But um. I mean, they're very general, so you don't need to just constrain it to that. It'll be able to answer any question. Over time, I think, you know, when we move from like the Llama 3 class of models to Llama 4 and beyond, it's, it's going to, I think, feel less like a chat bot where it's like you, you give it a prompt and it just responds. Then you give it a prompt and it responds, and it's just like back and forth. I think it's going to pretty quickly evolve to, you give it an intent and it actually can go away on multiple time frames. And I mean, it probably should acknowledge that you gave it an intent up front. But I mean, some of the stuff I think will end up, you know itâ€™ll spin up, you know, compute jobs that take, you know, weeks or months or something and then just come back to you and like, something happens in the world. And I think that that's going to be really powerful. Today's AI, as you know, is kind of turn-based, you say something, it says something back to you. But obviously when we think, when we're given a mission or we're giving a problem, you know, we'll contemplate multiple options. Or maybe we come up with a, you know, a tree of options, a decision tree, and we walk down the decision tree simulating in our mind, you know, what are the different outcomes of each decision that we could potentially make. And so we're doing planning. And so in the future AI's will kind of do the same. One of the things that that I was super excited about when you talked about your vision of creator AI, I just think that's a home run idea, frankly. Tell everybody about the creator AI and AI studio that's going to enable you to do that. Yeah, so we actually I mean, this is something that we're we've talked about it a bit, but we're rolling it out a lot wider today. You know, a lot of our vision is that I don't think that there's just going be like one AI model, right? I mean, this is something that some of the other companies in the industry, they're like, you know, it's like they're building like one central agent and yeah, we'll have the Meta AI assistant that you can use. But a lot of our vision is that we want to empower all the people who use our products to basically create agents for themselves. So whether that's, you know, all the many, many millions of creators that are on the platform or, you know, hundreds of millions of small businesses, we eventually want to just be able to pull in all your content and very quickly stand up a business agent and be able to interact with your customers and, you know, do sales and customer support and all that. So, the one that we're that we're just starting to roll out more now is, we call it AI Studio. And it basically is, a set of tools that eventually is going to make it so that every creator can build sort of an AI version of themselves, as sort of an agent or an assistant that their community can interact with. There's kind of a fundamental issue here where there's just not enough hours in the day. Right? Itâ€™s like if you're a creator, you want to engage more with your community, but you're constrained on time. And similarly, your community wants to engage with you, but it's tough. I mean, there's just limited time to do that. So the next best thing is allowing people to basically create these artifacts. Right? It's an agent, but it's you train it on your material to represent you in the way that you want. I think it's a very kind of creative endeavor, almost like a, like a piece of art or content that you're putting out there. No, it's to be very clear that it's not engaging with the creator themselves. But I think it'll be another interesting way, just like how creators put out content on, on these, social systems, to be able to have agents that do that. Similarly, I think that there's going to be a thing where people basically create their own agents for all different kinds of uses. Some will be sort of customized utility, things that they're trying to get done that they want to fine tune and and train an agent for, and some of them will be entertainment. And some of the things that people create are just funny, you know, and just kind of silly in different ways. Or kind of have a funny attitude about things that, you know, we probably couldn't we probably wouldn't build into Meta AI as an assistant, but I think people people are kind of pretty interested to see, and interact with. And then one of the interesting use cases that we're seeing is people kind of using these agents for support. This was one thing that was a little bit surprising to me is one of the top use cases for Meta AI already is people basically using it to role play difficult social situations that they're going to be in. So whether it's a professional situation, it's like, all right, I want to ask my manager, like, how do I get a promotion or raise? Or I'm having this fight with my friend, or I'm having this difficult situation with my girlfriend. Like, how can this conversation go? And basically having a like a completely judgment-free zone where you can basically role play that and see how the conversation will go and get feedback on it. But a lot of people, they don't just want to interact with the same agent, whether it's Meta AI or ChatGPT or whatever it is that everyone else is using, they want to kind of create their own thing. So that's roughly where we're going with AI studio. But it's all part of this bigger, I guess, view that we have, that there shouldn't just be one big AI that people interact with. We just think that the world will be better and more interesting if there's a diversity of these different things. I just think it's so cool that if you're an artist and you have a style, you could take your style, all of your body of work, you could fine tune one of your models. And now this becomes an AI model that you can come and you could prompt it. You could ask me to create something along the lines of the art style that I have, and you might even give me a piece of art as a drawing, a sketch, as an inspiration. And I can generate something for you. And you come to my bot for that, come to my AI for that. It could be, every single restaurant, every single website will probably in the future have these AIs. Yeah I mean, I kind of think that in the future, just like every business has, you know, an email address and a website and a social media account or several. I think in the future, every business is going to have an AI agent that interfaces with their customers. And some of these things, I think have been pretty hard to do historically. Like, if you think about any company, it's like you probably have customer support as just a separate organization from sales, and that's not really how you'd want it to work as CEO. It's just that, okay, they're kind of different skills. You're building up these- I'm your customer support just so you know. Yeah. Well, apparently I am. Whenever Mark needs something. I can't tell whether itâ€™s his chat bot or it's just Mark, butâ€¦ It just was my chat bot here, just asking here. Well, I guess that's kind of, when you're CEO, you have to do all this stuff. But, I mean, then when you build the abstraction in your organization, a lot of times, like the, you know, in general the organizations are separate because they're kind of optimized for different things. But I think, like the platonic ideal of this would be that it's kind of one thing, right? As a, you know, as a customer, you don't really care. You know, you don't want to have a different route when you're trying to buy something versus if you're having an issue with something that you bought, you just want to have a place that you can go and get your questions answered and be able to engage with the business in different ways. And I think that that applies for creators, too. I think thatâ€™s the kind of personal consumer side of this- And all that engagement with your customers, especially their complaints, is going to make your company better. Yeah. Totally. Right? The fact that is all engaging with this AI is going to capture the institutional knowledge and all of that can go into analytics which improves the AI and so on, so forth. Yeah, yeah. So the business version of this is- that I think has a little more integration and we're still in a pretty early alpha with that. But the AI Studio making it so that people can kind of create their UGC agents and different things, and getting started on this flywheel of having creators create them. I'm pretty excited about that. So can I, can I use AI Studio to fine tune with my images, my collection of images? Yeah, yeah, we're going to get there. And then I could, could I give it, load it with all the things that I've written, use it as my RAG? Yeah. Basically. Okay. And then every time I come back to it, it loads up its memory again, so it remembers where it left off last time. And we carry on our conversation as, though never nothing ever happened. Yeah and look, I mean, like any product, it'll get better over time. The tools for training, it will get better. It's not just about what you want it to say. I mean, I think generally creators and businesses have topics that they want to stay away from too. So just getting better at all this stuff, I think the platonic version of this is not just text, right? You almost want to just be able to, and this is a sort of an intersection with some of the codec avatar work that we're doing over time. You want to basically be able to have almost like a, a video chat with the agent. And I think we'll get there over time. I don't think that this stuff is that far off, but the flywheel is spinning really quickly, so it's exciting. There is a lot of new stuff to build. And I think even if the progress on the foundation models kind of stopped now, which I don't think it will, I think we'd have like five years of product innovation for the industry to basically figure out how to most effectively use all the stuff that's gotten built so far. But I actually just think the kind of foundation models and the progress on the fundamental research is accelerating. So, that it's, a pretty wild time. Your vision- It's all you know, you kind of made this happen. Why thank you. In the last conversation, I - Thank you. Yeah. You know, you know, you know, CEOs, we're delicate flowers. We need a lot of back- Yeah. We're pretty grizzled at this point. I think we're we're the two kind of longest standing founders in the industry, right? It's true. It's true. I just- And your hair has gotten gray. Mine has just gotten longer. Mine's gotten gray. Yours has gone curly, what's up? It was always curly. That's why I kept it short. Okay. You know, I just. If I'd known it was going to take so long to succeed, you would never would have started. No, I would have dropped out of college, just like you. Get a head start. Well, that's a there's a good difference between our personalities. You got a 12 year head start. That's pretty good. You know, you're doing pretty well. I'm gonna- I'm going to be able to carry on. Let me just put it that way. Yeah. So, so, the thing that I love about your vision that, everybody can have an AI that every business can have an AI In our company, I want every engineer and every software developer to have an AI. And, or many AIs. The thing that I love about your vision is you also believe that everybody and every company should be able to make their own AI. So you actually open-sourced, when you open-sourced Llama I thought that was great. Llama 2.1, by the way, I thought Llama 2 was probably the biggest event in AI last year. And the reason for that- I mean, I thought it was the H100, but, you know, it's, it's a chicken or the egg question. That's a chicken or the egg question. Yeah. Which came first? The H100. Well, Llama 2, it was, it was actually not the H100. Yeah, it was A100 yeah. Thank you. And so, but the reason why I said it was the biggest event was because when that came out, it activated every company, every enterprise and every industry. All of a sudden, every health care company was building AI. Every company was building AI, every large company, small companies, startups were building AIs. It made it possible for every researcher to be able to reengage AI again, because they have a starting point to do something with, and then now, 3.1 is out and the excitement, just so you know, you know, we work together to, to deploy, 3.1, we're taking it out to the world's enterprise. And the excitement is just off the charts. And, and I think it's going to enable all kinds of applications. But tell me about your your open-source philosophy. Where did that come from? And, you know, you open-sourced PyTorch. And that it is now the framework by which AI is done. And, now you've open-sourced Llama 3.1 or Llama there's a whole ecosystem built around it. And so I think it's terrific. But where did that all come from? Yeah. So there's a bunch of history on a lot of this. I mean, we've done a lot of open-source work over time. I think part of it, you know, just bluntly is, you know, we got started after some of the other tech companies, right, in building out stuff like the distributed computing infrastructure and the data centers. And, you know, because of that, by the time that we built that stuff, it wasn't a competitive advantage. We're like, all right, we might as well make this open and then we'll benefit from the ecosystem around that. So we had a bunch of projects like that. I think the biggest one was probably Open Compute where we took our server designs, the network designs, and eventually the data center designs and published all of that. And by having that become somewhat of an industry standard, all the supply chains basically got organized around it, which had this benefit of saving money for everyone. So by making it public, and open, we basically have saved billions of dollars from doing that. Well, Open Compute was also what made it possible for NVIDIA HGXs, that we designed for one data center, all of a sudden, works in every data center. Awesome. So that was an awesome experience. And then, you know, we've done it with a bunch of our infrastructure tools, things like React, PyTorch. So I'd say by the time that Llama came around, we were sort of positively predisposed towards doing this. For, for AI models specifically. I guess there's a few ways that I look at this. I mean, one is, you know it's been really fun building stuff over the last 20 years at the company. One of the things that that has been sort of the most difficult has been kind of having to navigate the fact that we ship our apps through our competitorâ€™s mobile platforms. So in the one hand, the mobile platforms have been this huge boon to the industry. That's been awesome. On the other hand, having to deliver your products through your competitors, is challenging, right? And I also, you know, I grew up in a time where, you know, the first version of Facebook was on the web and that was open. And then, as a transition to mobile, you know, the plus side of that was, you know, now everyone has a computer in their pocket. So that's great. The downside is, okay, we're a lot more restricted in what we can do. So, when you look at these generations of computing there's this big recency bias where everyone just looks at mobile and thinks, okay, because the closed ecosystem, because Apple basically won and set the the terms of that. And like yeah, I know that there's more Android phones out there technically, but like Apple basically has the whole market. and like all the profits. And basically Android is kind of following Apple in terms of the development of it. So I think Apple pretty clearly won this generation. But it's not always like that where if you go back a generation, you know, Apple was doing their kind of closed thing. But Microsoft, which as you know, it obviously wasn't like this perfectly open company, but, you know, compared to Apple with Windows running on all the different OEMs and different software, different hardware it was a much more open ecosystem and Windows was the leading ecosystem. It, basically in the kind of PC generation of things, the open ecosystem won. And I am kind of hopeful that in the next generation of computing, we're going to return to a zone where the open ecosystem wins and is the leading one again. There will always be a closed one and an open one. I think that there's reasons to do both. There are benefits to both. I'm not like a zealot on this. I mean, we do closed source stuff and not everything that we that we publish is open. But I think in general for the computing platforms that the whole industry is building on, there's a lot of value for that if the software especially is open. So that's really shaped my philosophy on this. And, for both AI with Llama and with the work that we're doing in AR and VR, where we are basically making the Horizon OS that we're building for mixed reality, an open operating system in the sense of, what Android or Windows was and basically making it so that we're going to be able to work with lots of different hardware companies to make all different kinds of devices. We basically just want to return the ecosystem to that level where that's going to be the open one. And I'm pretty optimistic that in the next generation, the open ones are going to win. For us specifically I just want to make sure that we have access to- I mean, this is sort of selfish, but, you know, after building this company for a while, one of my things for the next 10 or 15 years is like, I just want to make sure that we can build the fundamental technology that we're going to be building social experiences on, because there have just been too many things that I've tried to build and then have just been told, nah, you can't really build that by the platform provider, that like, we're going to go build all the way down and, and make sure that that- There goes our broadcast opportunity. Yeah. No, sorry. Sorry. There's a beep. Yeah. You know, Iâ€™ve been doing okay for, like, 20 minutes, but... get me talking about closed platforms and I get angry. Hey, look, it is great. I think it's a great world. Where there are people who are dedicated to build the best possible AIs, however they build it, and they offer it to the world, you know, as a service. And then. But if you want to build your own AI, you could still also build your own AI. So the ability to use an AI. You know, there's a lot of stuff, I prefer not to make this jacket myself. I prefer to have this jacket made for me. You know what I'm saying? Yeah. But so the fact that. So the fact that leather could be open source is not a useful concept for me, but I think the, the idea that you could, you could have great services, incredible services as well as open service. Open ability. Then we basically have the entire spectrum. But the thing that's, that you did with 3.1 that was really great was you have 4 or 5B, you have 70B, you have 8B you could, you could use it for synthetic data generation, use the larger models to essentially teach the smaller models. And although the larger models will be more general, it's less brittle, you could still build a smaller model that fits in, you know, whatever operating domain or operating costs that you would like to have. Meta guard, I think? Yeah Llama Guard. Yeah Llama Guard, Llama Guard for guard railing. Fantastic. And so now and the way that you built the model, it's built in a transparent way. You dedicated- You've got a world class safety team. World class ethics team. You could build it in such a way that everybody knows it's built properly. And so I really love that part of it. Yeah and I mean, just to finish the thought from before, before I got, I got sidetracked there for a detour. I do think there's this alignment where we're building it because we want the thing to exist, and we want to not get cut off from some closed model. Right? And, but this isn't just like a piece of software that you can build. It's, you know, you need an ecosystem around it. And so it's almost like it kind of almost wouldn't even work that well if we didn't open source it. Right? It's not we're not doing this because we're kind of altruistic people. Even though I think that this is going to be helpful for the ecosystem, and we're doing it because we think that this is going to make the thing that we're building the best by having a robust ecosystem. Well, look how many people contributed to PyTorch ecosystem. Yeah, totally. Mountains of engineering. Yeah. Right. Yeah. Yeah. I mean, NVIDIA alone, we probably have a couple of hundred people just dedicated to making PyTorch better and scalable and, you know, more performant and so on and so forth. Yeah and it's also just when something becomes something of an industry standard, other folks do work around it, right? So like all of the silicon in the systems will end up being optimized to run this thing really well, which will benefit everyone, but it will also work well with the system that we're building. And that's, I think, just one example of how this ends up being, just being really effective. So, yeah, I mean, I think that the open-source strategy is going to be, yeah, it's just going to be a good one as a business strategy. I think people still don't quite get it. We love it so much. We built an ecosystem around it. We build this thing Called AI Foundry. Yeah. Yeah, yeah. I mean, you guys have been awesome. Yeah. I mean, every time we're shipping something, you you guys are the first to release this and optimize it and make it work. And so I mean, I, I appreciate that. What can I say? We have good engineers you know and so. Well you always just jump on this stuff quickly too. You know, I'm a senior citizen, but I'm agile. You know, that's what CEOs have to do. And I recognize an important thing, I recognize an important thing. And I think that Llama is genuinely important. We built this concept called an AI factory, uh, AI Foundry around it so that we can help everybody build, take- you know, a lot of people, they have a desire to build AI. And it's very important for them to own the AI because once they put that into their flywheel, their data flywheel, that's how their company's institutional knowledge is encoded and embedded into an AI. So they can't afford to have the AI flywheel, the data flywheel that experience flywheel somewhere else. So and so open source allows them to do that. But they don't really know how to turn this whole thing into an AI and so we created this thing called AI Foundry. We provide the tooling, we provide the expertise, Llama technology, we have the ability to help them turn this whole thing, into an AI service. And, and then when we're done with that, they take it, they own it. The output of it is what we call a NIM. And this NIM, this this neuro micro NVIDIA Inference Microservice, they just download it, they take it, they run it anywhere they like, including on-prem. And we have a whole ecosystem of partners, from OEMs that can run the NIMs to, GSIs like Accenture that that we train and work with to create Llama-based NIMs and pipelines and and now we're off helping enterprises all over the world do this. I mean, it's really quite an exciting thing. It's really all triggered off of, the Llama open-sourcing. Yeah, I think especially the ability to help people distil their own models from the big model is going to be a really valuable new thing because there's this, just like we talked about on the product side, how at least I don't think that there's going to like one major AI agent that everyone talks to. At the same level, I don't think that there's going to necessarily be one model that everyone uses. We have a chip AI, chip design AI, we have a software coding AI, and our software coding AI understands USD because we code in USD for Omniverse stuff. We have software AI that understands Verilog, our Verilog. we have we have software AI that understands our bugs database and knows how to help us triage bugs and sends it to the right engineers. And so each one of these AIs are fine tuned off of Llama and, and so we fine tune them, we guardrail them, you know if we have an AI design, for, chip design, we're not interested in asking it about politics, you know, and religion and things like that. So we guardrail it. And so, so I think, I think every company will essentially have for every single function that they have, they will likely have AIs that are built for that. And they need help to do that. Yeah. I mean, I think it's one of the big questions is going to be in the future, to what extent are people just using the kind of the bigger, more sophisticated models versus just training their own models for the uses that they have? And at least I would bet that they're going to be just a vast proliferation of different models. We use the largest ones. And the reason for that is because our engineers, their time is so valuable. and so we get, right now we're getting 4 or 5B, optimized for performance. And as you know, 405B doesn't fit in any GPU, no matter how big. And so that's why the NVLink performance is so important. We have every one of our GPUs connected by this, non-blocking switch called NVLink switch. And in the HGX for example, there are two of those switches and we make it possible for all these, all these GPUs to work and, and run the 405Bs really performant. The reason why we do it is because the engineersâ€™ times are so valuable to us. You know, we want to use the best possible model, the fact that it's cost effective by a few pennies, who cares? And so we just want to make sure that the best quality of results is presented to them. Yeah. Well, I mean, the 405 I think is about half the cost to inference of the GPT 4o model. So I mean, at that level, it's already I mean, it's pretty good. But yeah, I mean I think people are doing stuff on devices or want smaller models. They're just going to distil it down. So that's like a whole different set of services. That AI is running, and let's pretend for a second that we're hiring that AI, that AI for chip design is probably $10 an hour. You're using, if you're using it constantly and you're sharing that AI across a whole bunch of engineers. So each engineer probably has an AI that's sitting with them. And, you know, it doesn't cost very much. And we pay the engineers a lot of money. And so to us, a few dollars an hour, amplifies the capabilities of somebody that's really valuable. Yeah, yeah. I mean, you don't need to convince me. If you haven't, if you haven't hired an AI, do it right away. That's all we're saying. And so, let's talk about, the next, the next wave. you know, one of the things that I really love about the work that you guys do, computer vision, one of the models that we use a lot internally, is Segment Everything, and, you know, that that we're now training AI models on video so that we can understand the world model. Our use case is for robotics and industrial digitalization and, connecting these AI models into Omniverse so that we can, we can, model and represent the physical world better, have robots that operate in these Omniverse worlds better. Your application, the Ray-Ban Meta glass, your vision for bringing AI into the virtual world, is really interesting. Tell us about that. Yeah. Well, okay, a lot to unpack in there. the Segment Anything model that you're talking about, we're actually presenting, I think the next version of that here at Siggraph. Segment Anything 2. And it now works, it's faster, it works with, oh, here we go. It works in video now as well. I think these are actually cattle from my ranch in Kauai. By the way, these are called Markâ€™s Cows Delicious Markâ€™s Cows. There you go. Next time we do- So, Mark, Mark came over to my house and we made Philly cheesesteak together. Next time you're bringing the cow. Iâ€™d say you did. I was more of a sous-chef. But, boy, that was really good. It was really good. That sous-chef comment. Okay, listen, And then at the end of the night though, you were like, hey, so you ate enough, right? And I was like, I don't know, I could eat another one. You're like, really? You know, usually when you say something to your guest. I was definitely like, yeah, we're making more, we're making more. Did you get enough to eat? Usually your guest says, oh yeah, I'm fine. Make me another cheesesteak Jensen. So just to let you know how OCD he is. So I turn around, I'm prepping the, the cheesesteak and I said, Mark, cut the tomatoes. And so Mark, I handed him a knife. Yeah, I'm a precision cutter. And so he cuts. He cuts the tomatoes. Every single one of them are perfectly to the exact millimeter. But the really interesting thing is, I was expecting all the tomatoes to be sliced and kind of stacked up, kind of like a deck of cards. And, but when I turned around, he said he needed another plate. And the reason for that was because all of the tomatoes he cut, none of them touched each other. Once he separates one slice of tomato from the other tomato, they shall not touch again. Yeah. Look, man, if you wanted them to touch, you needed to tell me that. Thatâ€™s why Iâ€™m just a sous-chef. Okay? That's why he needs an AI that doesn't judge. Yeah, it's like. So this is super cool. Okay, so it's recognizing the cows track. It's recognizing tracking the cows. Yeah, yeah. So it's, a lot of fun effects will be able to be made with this. And because it'll be open a lot of more serious applications across the industry, too. So, yeah, I mean, scientists use this stuff to, you know, study, like coral reefs and natural habitats and, and kind of evolution of landscapes and things like that. But, I mean, it's, being able to do this and video and having it be a zero shot and be able to kind of interact with it and tell it what you want to track is, it's pretty cool research. So, for example, the reason why we use it, for example, you have a warehouse and they've got a whole bunch of cameras and the warehouse AI is watching everything that's going on. And let's say you know, a stack of boxes fell, or somebody spilled water on the ground, or, you know, whatever accident is about to happen, the AI recognizes it, generates the text, sends it to somebody, and, you know, you know, help will come along the way. And so that's one way of using it, instead of recording everything. If there's an accident, instead of recording every nanosecond of video and then going back and retrieve that moment, it just records the important stuff because it knows what it's looking at. And so having a video understanding model, a video language model is really, really powerful for all of these interesting applications. Now what else what else are you guys going to work on beyond- talk to me about- Yes. There's all the smart glasses. Yeah. Right. So I think when we think about the next computing platform, you know, we kind of break it down into mixed reality, the headsets and the smart glasses. I think it's easier for people to wrap their head around that and wearing it because, you know, pretty much everyone who's wearing a pair of glasses today will end up that'll get upgraded to smart glasses. And that's like more than a billion people in the world. So that's going to be a pretty big thing. the VR MR headsets, I think some people find it interesting for gaming or different uses. Some don't yet. Yet my view is that they're going to be both in the world. I think the smart glasses are going to be sort of the mobile phone, kind of always on version of the next computing platform, and the mixed reality headsets are going to be more like your workstation or your game console, where, when you're sitting down for a more immersive session and you want access to more compute, I mean, look, I mean, the glasses are just very small form factor. There are going to be a lot of constraints on that. Just like you can't do the same level of computing on a phone. It came at exactly the time when all of these breakthroughs in generative AI happened. Yeah. So we basically for smart glasses, we've been we've been going at the problem from two different directions on the one hand, we've been building what we think is sort of the technology that you need for the kind of ideal holographic AR glasses and we're doing all the custom silicon work, all the custom display stack work, like all the stuff that you need to do to make that work in their glasses. Right? It's not a headset. It's not like a VR or MR headset. They look like glasses. But, they're still quite a bit far off from the glasses that you're wearing now. I mean, those are very thin, but, but even the Ray-Bans that we that we make, you couldn't quite fit all the tech that you need to into that yet for kind of full holographic AR, we're getting close. And over the next few years I think we'll basically get closer. It'll still be pretty expensive, but I think that will start to be a product. The other angle that we've come at this is let's start with good looking glasses. By partnering with the best glasses maker in the world, Essilor Luxottica. They basically make they have all the big brands that you use. You know, it's Ray-Ban or Oakley or Oliver Peoples or just like a handful of others. Yeah, it's kind of all Essilor Luxottica. The NVIDIA of glasses. I think that, you know, I think they would probably like that analogy, but, I mean, who wouldn't at this point? So we've been working with them on the Ray-Bans. We're on the second generation. And the goal there has been, okay, let's constrain the form factor to just something that looks great. And within that, let's put in as much technology as we can, understanding that we're not going to get to the kind of ideal of what we want to fit into a technically, but it'll, but at the end, it'll be like great looking glasses. And at this point we have we have camera sensors, so you can take photos and videos. You can actually livestream to Instagram. You can take video calls on WhatsApp and stream to the other person what you're seeing. You can, I mean, it has it has a microphone and speakers. I mean, the speaker is actually really, really good. Itâ€™s open ear so really a lot of people find it more comfortable than, than earbuds. you can listen to music and it's just like this private experience. That's pretty neat, people love that. You take phone calls on it. but then it just turned out that that sensor package was exactly what you needed to be able to talk to AI too. So that was sort of an accident. If you'd asked me five years ago, Were we going to get holographic AR before AI, I would have said, yeah, probably. Right I mean, it's just seems like kind of the graphics progression and the display progression on all the virtual and mixed reality stuff and building up the new display stack. We're just making continual progress towards that. That's right. And then this breakthrough happened with LLMs. And it turned out that we have sort of really high-quality AI now and getting better at a really fast rate before you have holographic AR. So it's sort of this inversion that, that I didn't really expect. I mean, we're we're fortunately well positioned because we were working on all these different products. But I think what you're going to end up with is, just a whole series of different potential glasses products at different price points with different levels of technology in them. So I kind of think, based on what we're seeing now with the Ray-Ban Metas, I would guess that display less AI glasses at like a $300 price point are going to be a really big product that, like tens of millions of people or hundreds of millions of people eventually are going to have. So you're going to have super interactive AI that you're talking to. Yeah, visual. You have visual language understanding that you just showed you have real time translation. You could talk to me in one language, I hear in another language. Then then the display is obviously going to be great too, but it's going to add a little bit of weight to the glasses and it's going to make them more expensive. So I think for there will be a lot of people who want the kind of full holographic display. But there are also going to be a lot of people for whom, you know, they want something that eventually is going to be like really thin glasses and- Well for industrial applications and for some work applications, we need that. I think, for consumer stuff too. You think so? Yeah. I mean, I think, you know, it's I was thinking about this a lot during the, you know, during Covid when everyone kind of went remote for a bit. It's like you're spending all this time on Zoom that's like, okay, this is like it's great that we have this, but, but in the future we're like, not that many years away from being able to have a virtual meeting where, like, you know, it's like, I'm not here physically. It's just my hologram. Yeah. And like, it just feels like we're there and we're physically present. We can work on something and collaborate on something together. But I think this is going to be especially important with AI. With that application I could live with, with a, a device that, that I'm not wearing all the time. Oh yeah. But I think we're going to get to the point where it actually is. Yeah Itâ€™ll be, I mean, within glasses there's like thinner frames and there's thicker frames and there's like all these styles. But so I don't, I think we're, we're a while away from having full holographic glasses in the form factor of your glasses, but I think having it in a pair of stylish, kind of chunkier framed glasses is not that far off. Sunglasses are face size these days. I could see that. Yeah. And you know, that's that's a very helpful style. Yeah, sure. that's very helpful. You know, it's like I'm trying to, you know, I'm trying to make my way into becoming a style influencer. So I can, like, influence this before, you know, before the glasses come to the market, but, you know? Well I can see you attempting it. How's your style influencing working out for you? You know, it's early.  Yeah? It's early. It's early. But, I don't know, I feel like if a big part of the future of the business is going to be building, kind of stylish glasses that people wear, this is something I should probably start paying a little more attention to. Thatâ€™s right. So, yeah, we're going to have to retire the version of me that wore the same thing every day. But I mean, that's the thing about glasses, too. I think it's, you know, it's unlike, you know, even the watch or phones, like, people really do not want to all look the same. Right? And it's like, so I do think that it's, you know, it's a, it's a platform that I think is going to lend itself, going back to the theme that we talked about before towards being an open ecosystem, because I think the diversity of form factors that people and styles that people are going to demand is going to be immense. It's not like everyone is not going to want to put like the one kind of pair of glasses that, you know, whoever else designs like, that's not I don't think that's going to fly for this. Yeah, I think that's right. Well, Mark, it's sort of incredible that we're living through a time where the entire computing stack is being reinvented, how we think about software. You know, what Andrej calls software one and software two. And now we're basically in software three now. The way we compute, from general purpose computing to these generative neural network processing way of doing computing. The capabilities, the applications we could develop now are unthinkable in the past. And, and this technology, generative AI, I don't remember another technology that that in such a fast rate, influenced consumers enterprise, industries and science. And to be able to, to cut across, cut across, all these different fields of science from, from climate tech to, biotech, to physical sciences, in every single field that we're encountering, generative AI is right in the middle of that, fundamental transition. And in addition to that, the things that you're talking about, generative AI is going to make a profound impact in society. You know, the products that we're making. And one of the things that I'm super excited about, and somebody asked me earlier, is there going to be a, you know, Jensen AI? Well, that's exactly the creative AI you were talking about. You know, where we just build our own AIs and I, I load it up with all of the things that I've written and I fine tune it with the way I answer questions and hopefully, over time, the accumulation of use and, you know, it becomes a really, really great assistant and companion, For a whole lot of people who just want to, you know, ask questions or, bounce ideas off of and, and it'll be the version of Jensen that as, as you were saying earlier, that's not judgmental. You're not afraid of being judged. And so you could come and interact with it all the time. But I just think, I think that those are really incredible things. And, you know, we write we write a lot of things all the time. And how incredible is it just to give it, you know, 3 or 4 topics. Now, these are the basic themes of what I want to write about and write in my voice and just use that as a starting point. So there's just so many things that we can do now. it's really terrific working with you. And, I know that, I know that, it's not easy building a company, and you pivoted yours from desktop to mobile to VR to AI, all these devices, it's really, really, really extraordinary to watch. And NVIDIA's pivoted many times ourselves, and I know exactly how hard it is doing that. And, you know, both of us have gotten kicked in our teeth a lot, plenty over the years. But that's what it takes to, to want to be a pioneer and, innovate. So it's really great watching you. Well. And likewise, I mean, it's like, it's I'm not sure if it's a pivot if you keep doing the thing you were doing before, but as well. But it's but you add to it. I mean there's more chapters to all, to all of this. And I think the same thing for, it's been fun watching... I mean, the journey that you guys have been on, I mean, just and you, we went through this period where everyone was like, nah, everything is going to kind of move to these devices and, you know, it's just going to get super kind of cheap compute. And you guys just kept on plugging away at this and it's like, no, like actually you're going to want these big systems that can parallelize. You went the other way. Yeah. No. We went and instead of building smaller and smaller devices, we made computers the size of warehouses. A little unfashionable. Super unfashionable. Yeah, yeah. But now, now it's cool. And instead of, you know, we started building a graphics chip, a GPU, and now when you, when, when you're deploying a GPU, you still call it Hopper H100, but so you guys know when, Zuck calls it H100 his data center of H100s, I think you're coming up on 600,000. Weâ€™re good customers. That's how you get the Jensen Q&A at Siggraph. Wow. Hang on. I was getting the Mark Zuckerberg Q&A. You were my guest. And I wanted to make sure that- You just called me one day you're like, hey, you know, in like a couple of weeks, we're doing this thing at Siggraph. I'm like, yeah, I don't think I'm doing anything that day. I'll fly to Denver. It sounds fun. Exactly. I'm not doing anything that afternoon, you just showed up. But the thing thatâ€™s just incredible. These systems that you guys build, they're giant systems. Incredibly hard to orchestrate, incredibly hard to run. And, you know, you said that, you got into the GPU, journey later than the most. but you're operating larger than just about anybody, and it's incredible to watch. And congratulations on everything that you've done. And, you are quite the style icon now. Check out this guy. Early stage, working on it. It's uh- Ladies and gentlemen, Mark Zuckerberg. Thank you. Hang on, hang on. Well, you know, you know, so it turns out the last time that we got together, after dinner, Mark and I were- Jersey swap. Jersey swap, and we took a picture and it turned into something viral, and, and. Now, I thought that he he has no trouble wearing my jacket. I don't know, is that my look? It should be. Is that right? Yeah, I actually, I, I made one for you. You did? Yeah. That one's Mark's. I mean, here, let's see. We got a box back here. It's black and leather and shearling. Oh! I didn't make this. I just ordered it online. Hang on a second. It's a little chilly in here. I think I'll try this on. I think this is- My goodness. I mean, it's a vibe you just need. Is this me? Get this guy a chain. Next time I see you Iâ€™m bringing you a gold chain. So fair is fair. So I let you know. I was telling everybody that Lori bought me a new jacket to celebrate this year's Siggraph. Siggraph is a big thing in our company. As you could imagine. RTX was launched here. amazing things were launched here. And this is a brand-new jacket. It's literally two hours old. Wow. And so I think we oughta jersey swap again. All right. Well- This oneâ€™s yours. I mean, this is worth more because it's used. Let's see. I don't know. I think I think Mark is pretty buff. He's like, the guy is pretty jacked. I'm in. You too, man. All right, all right, all right, everybody, thank you. Mark Zuckerberg have a great Siggraph.\n","--------------------\n","caption text saved as: AI_and_The_Next (en).txt\n","========================================\n","\n"]}],"source":["for file in srt_files:\n","  path_srt_file = save_dir + '/' + file\n","  caption_text = extract_caption_text(path_srt_file)\n","  print(f'path_srt_file: {path_srt_file}')\n","  print(f'caption_text: {caption_text}')\n","  print('-'*20)\n","\n","  # save caption_text as a .txt file\n","  txt_file_name = file.replace('.srt', '.txt')\n","  with open(save_dir + '/' + txt_file_name, 'w') as f:\n","    f.write(caption_text)\n","  print(f'caption text saved as: {txt_file_name}')\n","  print(f\"{'='*40}\\n\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1724282071169,"user":{"displayName":"ping yee","userId":"17916777163667289249"},"user_tz":420},"id":"NkcSaUYWurXv","outputId":"956528ff-0411-4d11-ce94-1322e1dafd50"},"outputs":[{"name":"stdout","output_type":"stream","text":["mp4 filenames: ['AI_and_The_Next.mp4', 'Michelle_Obama_Full_Remarks.mp4']\n","mp3 filenames: ['AI_and_The_Next.mp3', 'Michelle_Obama_Full_Remarks.mp3']\n","srt filenames: ['AI_and_The_Next (en).srt']\n","txt filenames: ['AI_and_The_Next (en).txt', 'Michelle_Obama_Full_Remarks.txt']\n"]}],"source":["# print directory filenames\n","filenames = get_file_names(save_dir)\n","mp4_filenames = find_strings_with_substring(filenames, \"mp4\", \"end\")\n","mp3_filenames = find_strings_with_substring(filenames, \"mp3\", \"end\")\n","srt_filenames = find_strings_with_substring(filenames, \"srt\", \"end\")\n","txt_filenames = find_strings_with_substring(filenames, \"txt\", \"end\")\n","\n","print(f'mp4 filenames: {mp4_filenames}')\n","print(f'mp3 filenames: {mp3_filenames}')\n","print(f'srt filenames: {srt_filenames}')\n","print(f'txt filenames: {txt_filenames}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43_h11ktBVxC"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNoLk0utYPKEZ106V9Eamy8","provenance":[{"file_id":"1kkAjy3QZyCTO-5IakQ0wX-hwFXpJncDP","timestamp":1724108615461},{"file_id":"1omHCshyvgAkl4xOFAnnEaubfxhjUOBbg","timestamp":1723249985497}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
