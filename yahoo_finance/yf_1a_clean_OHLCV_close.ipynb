{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook cleans the downloaded stock prices from Yahoo Finance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/pandas-groupby-a-simple-but-detailed-tutorial-314b8f37005d\n",
    "# https://towardsdatascience.com/accessing-data-in-a-multiindex-dataframe-in-pandas-569e8767201d\n",
    "# https://towardsdatascience.com/summarizing-data-with-pandas-crosstab-efc8b9abecf\n",
    "# https://towardsdatascience.com/how-to-flatten-multiindex-columns-and-rows-in-pandas-f5406c50e569\n",
    "# https://datascientyst.com/list-aggregation-functions-aggfunc-groupby-pandas/\n",
    "# https://stackoverflow.com/questions/25929319/how-to-iterate-over-pandas-multiindex-dataframe-using-index\n",
    "# https://stackoverflow.com/questions/24495695/pandas-get-unique-multiindex-level-values-by-label\n",
    "# https://stackoverflow.com/questions/55706391/pandas-crosstab-on-multiple-columns-then-groupby\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/pyplots/pyplot_text.html#sphx-glr-gallery-pyplots-pyplot-text-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Get the user's home directory Documents folder\n",
    "path_dir = os.path.join(os.path.expanduser('~'), 'Documents/')\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "print(f'path_dir: {path_dir}')\n",
    "print(f'path_data_dump: {path_data_dump}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Parent directory where myUtils is located\n",
    "path_utils = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(path_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from myUtils import pickle_load, pickle_dump\n",
    "\n",
    "# # path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "# path_dir = \"G:/My Drive/stocks/yfinance/\"\n",
    "# path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "# # filename_symbols = path_data_dump + 'vg_symbols_4chars_max.csv'  # symbols text file\n",
    "# filename_symbols = path_data_dump + 'my_symbols.csv'  # symbols text file\n",
    "\n",
    "# _filename_pickled_df_OHLCVA_downloaded = 'df_OHLCVA_downloaded '  # OHLCVA downloaded from Yahoo\n",
    "filename_pickled_df_adjOHLCV = 'df_adjOHLCV'  # adjusted OHLCV\n",
    "filename_pickled_df_symbols_close = \"df_symbols_close\"  # symbols' adjusted close\n",
    "filename_pickled_symbols_df_adjOHLCV =  'symbols_df_adjOHLCV'  # symbols in df_adjOHLCV\n",
    "filename_pickled_perf_rank_dict =  'perf_rank_dict'  # store symbols from performance rank results\n",
    "filename_pickled_r_all_ranks =  'r_all_ranks'  # list of top 100 most common symbols from performance rank results\n",
    "filename_pickled_df_a = 'df_OHLCV_clean'  # df adjusted OHLCV, dropped symbols with no vol and close\n",
    "filename_pickled_df_c = 'df_close_clean'  # df close, dropped symbols with no vol and close\n",
    "\n",
    "verbose = False  # True prints more output\n",
    "\n",
    "#################\n",
    "# look_back_days = -250 * 60  # subset df iloc days\n",
    "look_back_days = -250 * 6  # subset df iloc days, 6 years of data\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path to pickled df_symbols_close:  G:/My Drive/stocks/yfinance/VSCode_dump/df_symbols_close\n",
      "Full path to pickled df_adjOHLCV:  G:/My Drive/stocks/yfinance/VSCode_dump/df_adjOHLCV\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full path to pickled df_symbols_close:  {path_data_dump}{filename_pickled_df_symbols_close}\")\n",
    "df_close = pickle_load(path_data_dump, filename_pickled_df_symbols_close, verbose=verbose)\n",
    "print(f\"Full path to pickled df_adjOHLCV:  {path_data_dump}{filename_pickled_df_adjOHLCV}\")\n",
    "df_adjOHLCV = pickle_load(path_data_dump, filename_pickled_df_adjOHLCV, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbols with no volume:\n",
      "Index(['AMCR', 'BIIB', 'BKR', 'CCI', 'CHD', 'CNC', 'DOC', 'EVBG', 'FCFS',\n",
      "       'GEN', 'HOLI', 'IBKR', 'JJC', 'NSTG', 'SRPT', 'SSB', 'UCBI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/63826291/pandas-series-find-column-by-value\n",
    "df = df_adjOHLCV[look_back_days::]\n",
    "df_v = df.xs('Volume', level=1, axis=1)  # select only Volume columns\n",
    "rows, cols = np.where(df_v == 0)  # row index, column index where trading volumes are zero\n",
    "idx_no_volume = list(set(cols))\n",
    "idx_no_volume.sort()\n",
    "symbols_no_volume = df_v.columns[idx_no_volume]\n",
    "print(f'symbols with no volume:\\n{symbols_no_volume}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbols with same volume:\n",
      "Index(['AAON', 'ABM', 'ACIW', 'ACM', 'ACN', 'ALE', 'ALRM', 'AMCR', 'AMED',\n",
      "       'AMG',\n",
      "       ...\n",
      "       'VSAT', 'WDAY', 'WELL', 'WERN', 'WEX', 'WSFS', 'WTS', 'WTW', 'XEL',\n",
      "       'XENE'],\n",
      "      dtype='object', length=270)\n"
     ]
    }
   ],
   "source": [
    "df_dif = df_v - df_v.shift(periods=1)\n",
    "rows, cols = np.where(df_dif == 0)\n",
    "idx_same_volume = list(set(cols))\n",
    "idx_same_volume.sort()\n",
    "idx_same_volume\n",
    "symbols_same_volume = df_v.columns[idx_same_volume]\n",
    "print(f'symbols with same volume:\\n{symbols_same_volume}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df.xs('Close', level=1, axis=1)  # select only Close columns\n",
    "df_c = df_c.fillna(0).copy()  # convert NaNs to zeros\n",
    "rows, cols = np.where(df_c == 0)  # row index, column index where trading volumes are zero\n",
    "idx_no_close = list(set(cols))\n",
    "idx_no_close.sort()\n",
    "symbols_no_close = df_c.columns[idx_no_close]\n",
    "print(f'symbols with NaN close:\\n{symbols_no_close}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_bad_data = list(symbols_no_close) + list(symbols_no_volume) + list(symbols_same_volume) # combine symbols with no volume and no close\n",
    "unique_symbols_bad_data = sorted(list(set(symbols_bad_data)))  # unique symbols\n",
    "print(f'unique symbols with bad data, e.g. no volume, same volume and $0 close, includes duplicate symbols: {len(unique_symbols_bad_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get symbols of past model picks\n",
    "df_picks = pickle_load(path_data_dump, 'df_picks', verbose=verbose)\n",
    "df_picks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only columns with symbols of past picks\n",
    "df_picks.drop(columns=[\"date_end_df_train\", \"max_days_lookbacks\", \"days_lookbacks\"], inplace=True)\n",
    "df_picks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "\n",
    "for col in df_picks.columns:\n",
    "  # convert column values from string to list, e.g. '[]', '[]', '[\"A\", \"B\", ..]' ... to [], [], [\"A\", \"B\", ..], ...\n",
    "  l_series = df_picks[col].apply(ast.literal_eval)\n",
    "  # list_of_lists = [l_item for l_item in l_series if l_item]  # this doesn't works  \n",
    "  for l_item in l_series:\n",
    "      if l_item:  # \n",
    "        list_of_lists.append(l_item)  \n",
    "\n",
    "symbols_picks = [val for sublist in list_of_lists for val in sublist]\n",
    "print(f'symbol count from model picks: {len(symbols_picks)}')\n",
    "# list sorted unique symbols\n",
    "unique_symbols_picks = sorted(list(set(symbols_picks)))\n",
    "print(f'unique symbol count from model picks: {len(unique_symbols_picks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_items(list1, list2):\n",
    "  \"\"\"Finds the common items between two lists.\n",
    "\n",
    "  Args:\n",
    "    list1: The first list.\n",
    "    list2: The second list.\n",
    "\n",
    "  Returns:\n",
    "    A list of the common items between the two lists.\n",
    "  \"\"\"\n",
    "\n",
    "  common_items = []\n",
    "  for item in list1:\n",
    "    if item in list2:\n",
    "      common_items.append(item)\n",
    "  return common_items\n",
    "\n",
    "\n",
    "# code in if block runs only from command line, code will NOT be executed if imported as a module\n",
    "if __name__ == \"__main__\": \n",
    "  list1 = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "  list2 = [\"b\", \"c\", \"d\", \"f\", \"g\"]\n",
    "  common_items = find_common_items(list1, list2)\n",
    "  print(common_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols are in past picks but also have bad data \n",
    "common_symbols = find_common_items(unique_symbols_picks, unique_symbols_bad_data)\n",
    "print(common_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_items(list1, list2):\n",
    "  \"\"\"Subtracts items in list2 from items in list1.\n",
    "\n",
    "  Args:\n",
    "    list1: The first list.\n",
    "    list2: The second list.\n",
    "\n",
    "  Returns:\n",
    "    A list of the items in list1 that are not in list2.\n",
    "  \"\"\"\n",
    "\n",
    "  subtracted_items = []\n",
    "  for item in list1:\n",
    "    if item not in list2:\n",
    "      subtracted_items.append(item)\n",
    "  return subtracted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_drop = subtract_items(unique_symbols_bad_data, common_symbols)  # don't drop symbols in past picks\n",
    "symbols_drop .sort()\n",
    "print(f'len(unique_symbols_bad_data): {len(unique_symbols_bad_data)}')\n",
    "print(f'len(common_symbols): {len(common_symbols)}')\n",
    "print(f'len(symbols_drop): {len(symbols_drop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols_drop = list(symbols_no_close) + list(symbols_no_volume) + list(symbols_same_volume) # combine symbols with no volume and no close\n",
    "# print(f'combined symbols with no volume, same volume and no close, inculdes duplicate symbols: {len(symbols_drop)}')\n",
    "# symbols_drop = list(set(symbols_drop))  # drop duplicate symbols\n",
    "# symbols_drop .sort()\n",
    "\n",
    "\n",
    "df_a = df.drop(symbols_drop, axis=1, level=0)  # drop symbols from OHLCA df\n",
    "df_c = df_close.iloc[look_back_days::]\n",
    "df_c = df_c.drop(symbols_drop, axis=1)\n",
    "print(f'unique symbols dropped from df_a (adjOLHLV) and df_c (Close): {len(symbols_drop)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'symbols with no volume:   {len(symbols_no_volume):>5,}')\n",
    "print(f'symbols with same volume: {len(symbols_same_volume):>5,}')\n",
    "print(f'symbols with no close:    {len(symbols_no_close):>5,}\\n')\n",
    "print(f'symbols total before drop:                                        {len(df_close.columns):>5,}')\n",
    "print(f'unique symbols dropped from df OHLCVA (df_a) and df Close (df_c): {len(symbols_drop):>5,}\\n')\n",
    "print('                                          symbols     rows')\n",
    "print(f'df adjOHLCV (df_a) after dropped symbols:   {len(df_a.columns)/5:>5,.0f}    {len(df_a):>5,}')\n",
    "print(f'df Close (df_c) after dropped symbols:      {len(df_c.columns):>5,}    {len(df_c):>5,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(df_a, path_data_dump, filename_pickled_df_a)\n",
    "print(f'pickled df adjOHLCV after dropping symbols with no volume, same volume, and no close:\\n{path_data_dump}{filename_pickled_df_a}')\n",
    "pickle_dump(df_c, path_data_dump, filename_pickled_df_c)\n",
    "print(f'pickled df Close after dropping symbols with no volume, same volume, and no close:\\n{path_data_dump}{filename_pickled_df_c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myUtils import list_dump\n",
    "\n",
    "f_symbols_df_close_clean = 'symbols_df_close_clean.csv'  # symbols text file\n",
    "symbols_df_c = list(df_c)  # column names in df_c\n",
    "list_dump(symbols_df_c, path_data_dump, f_symbols_df_close_clean)# df_c.columns.to_csv(f_symbols_df_close_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a['ERIE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16adeade2e4c2fe5a5c8fae29fa6e6b24d9301998b78edb3420e7dd402ae68a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
